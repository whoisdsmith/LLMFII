# ChatGPT, GenerativeAI and LLMs Timeline 

This repository organizes a timeline of key events (products, services, papers, GitHub, blog posts and news) that occurred before and after the ChatGPT announcement. 

It's curating a variety of information in this timeline, with a particular focus on LLM and Generative AI. 

Maybe it's a scene from the hottest history, so I thought it would be important to keep those memories well, so I organized them.

## Statistics 

These diagrams were generated by ChatGPT's Code Interpreter.

<img src="statistics-1224-02.png"> 
<img src="statistics-1224-01.png">

## Contributing

Issues and Pull Requests are greatly appreciated. If you've never contributed to an open source project before I'm more than happy to walk you through how to create a pull request.

You can start by [opening an issue](https://github.com/hollobit/BCAC_timeline/issues/new) describing the problem that you're looking to resolve and we'll go from there.

## Emoji 

arXiv :x:, PDF :paperclip:, arxiv-vanity :orange_book:, paper page :house:, papers with code :eight_spoked_asterisk:, Github :octocat:

## License

This document is licensed under the [MIT license](https://opensource.org/licenses/mit-license.php) © Jonghong Jeon(전종홍)

## Timeline V2

### 2024

  * 05/17 - **OpenAI strikes Reddit deal to train its AI on your posts** <br>  ([News](https://www.theverge.com/2024/5/16/24158529/reddit-openai-chatgpt-api-access-advertising)), 
  * 05/17 - **OpenAI dissolves team focused on long-term AI risks, less than one year after announcing it** <br>  ([News](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html)), 
  * 05/17 - **International Scientific Report on the Safety of Advanced AI** <br>  ([Blog](https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai)), 
  * 05/16 - **TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction** <br>([:x:](https://arxiv.org/abs/2405.10315)), ([:book:](https://browse.arxiv.org/pdf/2405.10315.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.10315.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.10315)), ([:house:](https://huggingface.co/papers/2405.10315)), ([HTML](https://browse.arxiv.org/html/2405.10315v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.10315)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.10315v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.10315)), ([SS](https://api.semanticscholar.org/arXiv:2405.10315))
  * 05/16 - **Toon3D: Seeing Cartoons from a New Perspective** <br>([:x:](https://arxiv.org/abs/2405.10320)), ([:book:](https://browse.arxiv.org/pdf/2405.10320.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.10320.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.10320)), ([:house:](https://huggingface.co/papers/2405.10320)), ([HTML](https://browse.arxiv.org/html/2405.10320v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.10320)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.10320v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.10320)), ([SS](https://api.semanticscholar.org/arXiv:2405.10320))
  * 05/16 - **Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature** <br>  ([News](https://www.nature.com/articles/s44185-024-00043-9)), 
  * 05/16 - **Many-Shot In-Context Learning in Multimodal Foundation Models** <br>([:x:](https://arxiv.org/abs/2405.09798)), ([:book:](https://browse.arxiv.org/pdf/2405.09798.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.09798.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.09798)), ([:house:](https://huggingface.co/papers/2405.09798)), ([HTML](https://browse.arxiv.org/html/2405.09798v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.09798)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.09798v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.09798)), ([SS](https://api.semanticscholar.org/arXiv:2405.09798))
  * 05/16 - **How to Hit Pause on AI Before It’s Too Late** <br>  ([News](https://time.com/6978790/how-to-pause-artificial-intelligence/)), 
  * 05/16 - **Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection** <br>([:x:](https://arxiv.org/abs/2405.10300)), ([:book:](https://browse.arxiv.org/pdf/2405.10300.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.10300.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.10300)), ([:house:](https://huggingface.co/papers/2405.10300)), ([HTML](https://browse.arxiv.org/html/2405.10300v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.10300)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.10300v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.10300)), ([SS](https://api.semanticscholar.org/arXiv:2405.10300))
  * 05/16 - **GPT Store Mining and Analysis** <br>([:x:](https://arxiv.org/abs/2405.10210)), ([:book:](https://browse.arxiv.org/pdf/2405.10210.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.10210.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.10210)), ([:house:](https://huggingface.co/papers/2405.10210)), ([HTML](https://browse.arxiv.org/html/2405.10210v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.10210)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.10210v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.10210)), ([SS](https://api.semanticscholar.org/arXiv:2405.10210))
  * 05/16 - **Dual3D: Efficient and Consistent Text-to-3D Generation with Dual-mode Multi-view Latent Diffusion** <br>([:x:](https://arxiv.org/abs/2405.09874)), ([:book:](https://browse.arxiv.org/pdf/2405.09874.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.09874.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.09874)), ([:house:](https://huggingface.co/papers/2405.09874)), ([HTML](https://browse.arxiv.org/html/2405.09874v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.09874)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.09874v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.09874)), ([SS](https://api.semanticscholar.org/arXiv:2405.09874))
  * 05/16 - **Chameleon: Mixed-Modal Early-Fusion Foundation Models** <br>([:x:](https://arxiv.org/abs/2405.09818)), ([:book:](https://browse.arxiv.org/pdf/2405.09818.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.09818.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.09818)), ([:house:](https://huggingface.co/papers/2405.09818)), ([HTML](https://browse.arxiv.org/html/2405.09818v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.09818)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.09818v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.09818)), ([SS](https://api.semanticscholar.org/arXiv:2405.09818))
  * 05/16 - **CAT3D: Create Anything in 3D with Multi-View Diffusion Models** <br>([:x:](https://arxiv.org/abs/2405.10314)), ([:book:](https://browse.arxiv.org/pdf/2405.10314.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.10314.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.10314)), ([:house:](https://huggingface.co/papers/2405.10314)), ([HTML](https://browse.arxiv.org/html/2405.10314v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.10314)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.10314v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.10314)), ([SS](https://api.semanticscholar.org/arXiv:2405.10314))
  * 05/15 - **Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model** <br>([:x:](https://arxiv.org/abs/2405.09215)), ([:book:](https://browse.arxiv.org/pdf/2405.09215.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.09215.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.09215)), ([:house:](https://huggingface.co/papers/2405.09215)), ([HTML](https://browse.arxiv.org/html/2405.09215v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.09215)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.09215v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.09215)), ([SS](https://api.semanticscholar.org/arXiv:2405.09215))
  * 05/15 - **LoRA Learns Less and Forgets Less** <br>([:x:](https://arxiv.org/abs/2405.09673)), ([:book:](https://browse.arxiv.org/pdf/2405.09673.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.09673.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.09673)), ([:house:](https://huggingface.co/papers/2405.09673)), ([HTML](https://browse.arxiv.org/html/2405.09673v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.09673)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.09673v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.09673)), ([SS](https://api.semanticscholar.org/arXiv:2405.09673))
  * 05/15 - **Google’s invisible AI watermark will help identify generative text and video** <br>  ([News](https://www.theverge.com/2024/5/14/24155927/google-ai-synthid-watermark-text-video-io)), 
  * 05/15 - **Google I/O 2024: everything announced** <br>  ([Blog](https://www.theverge.com/24153841/google-io-2024-ai-gemini-android-chrome-photos)), 
  * 05/15 - **BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation** <br>([:x:](https://arxiv.org/abs/2405.09546)), ([:book:](https://browse.arxiv.org/pdf/2405.09546.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.09546.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.09546)), ([:house:](https://huggingface.co/papers/2405.09546)), ([HTML](https://browse.arxiv.org/html/2405.09546v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.09546)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.09546v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.09546)), ([SS](https://api.semanticscholar.org/arXiv:2405.09546))
  * 05/15 - **ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models** <br>([:x:](https://arxiv.org/abs/2405.09220)), ([:book:](https://browse.arxiv.org/pdf/2405.09220.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.09220.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.09220)), ([:house:](https://huggingface.co/papers/2405.09220)), ([HTML](https://browse.arxiv.org/html/2405.09220v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.09220)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.09220v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.09220)), ([SS](https://api.semanticscholar.org/arXiv:2405.09220))
  * 05/14 - **Understanding the performance gap between online and offline alignment algorithms** <br>([:x:](https://arxiv.org/abs/2405.08448)), ([:book:](https://browse.arxiv.org/pdf/2405.08448.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.08448.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.08448)), ([:house:](https://huggingface.co/papers/2405.08448)), ([HTML](https://browse.arxiv.org/html/2405.08448v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.08448)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.08448v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08448)), ([SS](https://api.semanticscholar.org/arXiv:2405.08448))
  * 05/14 - **SpeechVerse: A Large-scale Generalizable Audio Language Model** <br>([:x:](https://arxiv.org/abs/2405.08295)), ([:book:](https://browse.arxiv.org/pdf/2405.08295.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.08295.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.08295)), ([:house:](https://huggingface.co/papers/2405.08295)), ([HTML](https://browse.arxiv.org/html/2405.08295v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.08295)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.08295v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08295)), ([SS](https://api.semanticscholar.org/arXiv:2405.08295))
  * 05/14 - **SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models** <br>([:x:](https://arxiv.org/abs/2405.08317)), ([:book:](https://browse.arxiv.org/pdf/2405.08317.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.08317.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.08317)), ([:house:](https://huggingface.co/papers/2405.08317)), ([HTML](https://browse.arxiv.org/html/2405.08317v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.08317)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.08317v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08317)), ([SS](https://api.semanticscholar.org/arXiv:2405.08317))
  * 05/14 - **No Time to Waste: Squeeze Time into Channel for Mobile Video Understanding** <br>([:x:](https://arxiv.org/abs/2405.08344)), ([:book:](https://browse.arxiv.org/pdf/2405.08344.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.08344.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.08344)), ([:house:](https://huggingface.co/papers/2405.08344)), ([HTML](https://browse.arxiv.org/html/2405.08344v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.08344)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.08344v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08344)), ([SS](https://api.semanticscholar.org/arXiv:2405.08344))
  * 05/14 - **Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding** <br>([:x:](https://arxiv.org/abs/2405.08748)), ([:book:](https://browse.arxiv.org/pdf/2405.08748.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.08748.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.08748)), ([:house:](https://huggingface.co/papers/2405.08748)), ([HTML](https://browse.arxiv.org/html/2405.08748v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.08748)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.08748v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08748)), ([SS](https://api.semanticscholar.org/arXiv:2405.08748))
  * 05/14 - **Compositional Text-to-Image Generation with Dense Blob Representations** <br>([:x:](https://arxiv.org/abs/2405.08246)), ([:book:](https://browse.arxiv.org/pdf/2405.08246.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.08246.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.08246)), ([:house:](https://huggingface.co/papers/2405.08246)), ([HTML](https://browse.arxiv.org/html/2405.08246v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.08246)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.08246v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08246)), ([SS](https://api.semanticscholar.org/arXiv:2405.08246))
  * 05/14 - **Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory** <br>([:x:](https://arxiv.org/abs/2405.08707)), ([:book:](https://browse.arxiv.org/pdf/2405.08707.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.08707.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.08707)), ([:house:](https://huggingface.co/papers/2405.08707)), ([HTML](https://browse.arxiv.org/html/2405.08707v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.08707)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.08707v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08707)), ([SS](https://api.semanticscholar.org/arXiv:2405.08707))
  * 05/13 - **SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts** <br>([:x:](https://arxiv.org/abs/2405.07518)), ([:book:](https://browse.arxiv.org/pdf/2405.07518.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.07518.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.07518)), ([:house:](https://huggingface.co/papers/2405.07518)), ([HTML](https://browse.arxiv.org/html/2405.07518v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.07518)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.07518v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.07518)), ([SS](https://api.semanticscholar.org/arXiv:2405.07518))
  * 05/13 - **RLHF Workflow: From Reward Modeling to Online RLHF** <br>([:x:](https://arxiv.org/abs/2405.07863)), ([:book:](https://browse.arxiv.org/pdf/2405.07863.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.07863.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.07863)), ([:house:](https://huggingface.co/papers/2405.07863)), ([HTML](https://browse.arxiv.org/html/2405.07863v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.07863)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.07863v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.07863)), ([SS](https://api.semanticscholar.org/arXiv:2405.07863))
  * 05/13 - **Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots** <br>([:x:](https://arxiv.org/abs/2405.07990)), ([:book:](https://browse.arxiv.org/pdf/2405.07990.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.07990.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.07990)), ([:house:](https://huggingface.co/papers/2405.07990)), ([HTML](https://browse.arxiv.org/html/2405.07990v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.07990)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.07990v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.07990)), ([SS](https://api.semanticscholar.org/arXiv:2405.07990))
  * 05/13 - **OpenAI unveils newest AI model, GPT-4o** <br>  ([News](https://edition.cnn.com/2024/05/13/tech/openai-altman-new-ai-model-gpt-4o/index.html)), 
  * 05/13 - **MS MARCO Web Search: a Large-scale Information-rich Web Dataset with Millions of Real Click Labels** <br>([:x:](https://arxiv.org/abs/2405.07526)), ([:book:](https://browse.arxiv.org/pdf/2405.07526.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.07526.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.07526)), ([:house:](https://huggingface.co/papers/2405.07526)), ([HTML](https://browse.arxiv.org/html/2405.07526v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.07526)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.07526v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.07526)), ([SS](https://api.semanticscholar.org/arXiv:2405.07526))
  * 05/13 - **How Much Research Is Being Written by Large Language Models?** <br>  ([Blog](https://hai.stanford.edu/news/how-much-research-being-written-large-language-models)), 
  * 05/13 - **Hello GPT-4o** <br>  ([Blog](https://openai.com/index/hello-gpt-4o/)), 
  * 05/13 - **Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning** <br>([:x:](https://arxiv.org/abs/2405.08054)), ([:book:](https://browse.arxiv.org/pdf/2405.08054.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.08054.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.08054)), ([:house:](https://huggingface.co/papers/2405.08054)), ([HTML](https://browse.arxiv.org/html/2405.08054v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.08054)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.08054v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08054)), ([SS](https://api.semanticscholar.org/arXiv:2405.08054))
  * 05/11 - **Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training** <br>([:x:](https://arxiv.org/abs/2405.06932)), ([:book:](https://browse.arxiv.org/pdf/2405.06932.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.06932.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.06932)), ([:house:](https://huggingface.co/papers/2405.06932)), ([HTML](https://browse.arxiv.org/html/2405.06932v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.06932)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.06932v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.06932)), ([SS](https://api.semanticscholar.org/arXiv:2405.06932))
  * 05/11 - **LogoMotion: Visually Grounded Code Generation for Content-Aware Animation** <br>([:x:](https://arxiv.org/abs/2405.07065)), ([:book:](https://browse.arxiv.org/pdf/2405.07065.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.07065.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.07065)), ([:house:](https://huggingface.co/papers/2405.07065)), ([HTML](https://browse.arxiv.org/html/2405.07065v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.07065)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.07065v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.07065)), ([SS](https://api.semanticscholar.org/arXiv:2405.07065))
  * 05/10 - **INSPECT - An open-source framework for large language model evaluations** <br>  ([Blog](https://ukgovernmentbeis.github.io/inspect_ai/)), 
  * 05/10 - **AI Safety Institute releases new AI safety evaluations platform** <br>  ([News](https://www.gov.uk/government/news/ai-safety-institute-releases-new-ai-safety-evaluations-platform)), 
  * 05/07 - **SUTRA: Scalable Multilingual Language Model Architecture** <br>([:x:](https://arxiv.org/abs/2405.06694)), ([:book:](https://browse.arxiv.org/pdf/2405.06694.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.06694.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.06694)), ([:house:](https://huggingface.co/papers/2405.06694)), ([HTML](https://browse.arxiv.org/html/2405.06694v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.06694)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.06694v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.06694)), ([SS](https://api.semanticscholar.org/arXiv:2405.06694))
  * 05/07 - **Meta Releases Llama 3 Open-Source LLM** <br>  ([News](https://www.infoq.com/news/2024/05/meta-llama-3/)), 
  * 05/03 - **What matters when building vision-language models?** <br>([:x:](https://arxiv.org/abs/2405.02246)), ([:book:](https://browse.arxiv.org/pdf/2405.02246.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.02246.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.02246)), ([:house:](https://huggingface.co/papers/2405.02246)), ([HTML](https://browse.arxiv.org/html/2405.02246v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.02246)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.02246v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.02246)), ([SS](https://api.semanticscholar.org/arXiv:2405.02246))
  * 05/02 - **WildChat: 1M ChatGPT Interaction Logs in the Wild** <br>([:x:](https://arxiv.org/abs/2405.01470)), ([:book:](https://browse.arxiv.org/pdf/2405.01470.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.01470.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.01470)), ([:house:](https://huggingface.co/papers/2405.01470)), ([HTML](https://browse.arxiv.org/html/2405.01470v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.01470)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.01470v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.01470)), ([SS](https://api.semanticscholar.org/arXiv:2405.01470))
  * 05/02 - **StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation** <br>([:x:](https://arxiv.org/abs/2405.01434)), ([:book:](https://browse.arxiv.org/pdf/2405.01434.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.01434.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.01434)), ([:house:](https://huggingface.co/papers/2405.01434)), ([HTML](https://browse.arxiv.org/html/2405.01434v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.01434)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.01434v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.01434)), ([SS](https://api.semanticscholar.org/arXiv:2405.01434))
  * 05/02 - **Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models** <br>([:x:](https://arxiv.org/abs/2405.01535)), ([:book:](https://browse.arxiv.org/pdf/2405.01535.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.01535.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.01535)), ([:house:](https://huggingface.co/papers/2405.01535)), ([HTML](https://browse.arxiv.org/html/2405.01535v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.01535)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.01535v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.01535)), ([SS](https://api.semanticscholar.org/arXiv:2405.01535))
  * 05/02 - **NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment** <br>([:x:](https://arxiv.org/abs/2405.01481)), ([:book:](https://browse.arxiv.org/pdf/2405.01481.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.01481.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.01481)), ([:house:](https://huggingface.co/papers/2405.01481)), ([HTML](https://browse.arxiv.org/html/2405.01481v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.01481)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.01481v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.01481)), ([SS](https://api.semanticscholar.org/arXiv:2405.01481))
  * 05/02 - **LLM-AD: Large Language Model based Audio Description System** <br>([:x:](https://arxiv.org/abs/2405.00983)), ([:book:](https://browse.arxiv.org/pdf/2405.00983.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00983.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00983)), ([:house:](https://huggingface.co/papers/2405.00983)), ([HTML](https://browse.arxiv.org/html/2405.00983v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00983)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00983v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00983)), ([SS](https://api.semanticscholar.org/arXiv:2405.00983))
  * 05/02 - **FLAME: Factuality-Aware Alignment for Large Language Models** <br>([:x:](https://arxiv.org/abs/2405.01525)), ([:book:](https://browse.arxiv.org/pdf/2405.01525.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.01525.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.01525)), ([:house:](https://huggingface.co/papers/2405.01525)), ([HTML](https://browse.arxiv.org/html/2405.01525v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.01525)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.01525v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.01525)), ([SS](https://api.semanticscholar.org/arXiv:2405.01525))
  * 05/02 - **Customizing Text-to-Image Models with a Single Image Pair** <br>([:x:](https://arxiv.org/abs/2405.01536)), ([:book:](https://browse.arxiv.org/pdf/2405.01536.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.01536.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.01536)), ([:house:](https://huggingface.co/papers/2405.01536)), ([HTML](https://browse.arxiv.org/html/2405.01536v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.01536)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.01536v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.01536)), ([SS](https://api.semanticscholar.org/arXiv:2405.01536))
  * 05/01 - **Spectrally Pruned Gaussian Fields with Neural Compensation** <br>([:x:](https://arxiv.org/abs/2405.00676)), ([:book:](https://browse.arxiv.org/pdf/2405.00676.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00676.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00676)), ([:house:](https://huggingface.co/papers/2405.00676)), ([HTML](https://browse.arxiv.org/html/2405.00676v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00676)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00676v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00676)), ([SS](https://api.semanticscholar.org/arXiv:2405.00676))
  * 05/01 - **Self-Play Preference Optimization for Language Model Alignment** <br>([:x:](https://arxiv.org/abs/2405.00675)), ([:book:](https://browse.arxiv.org/pdf/2405.00675.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00675.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00675)), ([:house:](https://huggingface.co/papers/2405.00675)), ([HTML](https://browse.arxiv.org/html/2405.00675v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00675)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00675v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00675)), ([SS](https://api.semanticscholar.org/arXiv:2405.00675))
  * 05/01 - **Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model Editing with Llama-3** <br>([:x:](https://arxiv.org/abs/2405.00664)), ([:book:](https://browse.arxiv.org/pdf/2405.00664.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00664.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00664)), ([:house:](https://huggingface.co/papers/2405.00664)), ([HTML](https://browse.arxiv.org/html/2405.00664v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00664)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00664v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00664)), ([SS](https://api.semanticscholar.org/arXiv:2405.00664))
  * 05/01 - **Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge** <br>([:x:](https://arxiv.org/abs/2405.00263)), ([:book:](https://browse.arxiv.org/pdf/2405.00263.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00263.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00263)), ([:house:](https://huggingface.co/papers/2405.00263)), ([HTML](https://browse.arxiv.org/html/2405.00263v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00263)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00263v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00263)), ([SS](https://api.semanticscholar.org/arXiv:2405.00263))
  * 05/01 - **A Careful Examination of Large Language Model Performance on Grade School Arithmetic** <br>([:x:](https://arxiv.org/abs/2405.00332)), ([:book:](https://browse.arxiv.org/pdf/2405.00332.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00332.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00332)), ([:house:](https://huggingface.co/papers/2405.00332)), ([HTML](https://browse.arxiv.org/html/2405.00332v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00332)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00332v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00332)), ([SS](https://api.semanticscholar.org/arXiv:2405.00332))
  * 04/30 - **Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation** <br>([:x:](https://arxiv.org/abs/2404.19752)), ([:book:](https://browse.arxiv.org/pdf/2404.19752.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19752.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19752)), ([:house:](https://huggingface.co/papers/2404.19752)), ([HTML](https://browse.arxiv.org/html/2404.19752v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19752)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19752v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19752)), ([SS](https://api.semanticscholar.org/arXiv:2404.19752))
  * 04/30 - **STT: Stateful Tracking with Transformers for Autonomous Driving** <br>([:x:](https://arxiv.org/abs/2405.00236)), ([:book:](https://browse.arxiv.org/pdf/2405.00236.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00236.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00236)), ([:house:](https://huggingface.co/papers/2405.00236)), ([HTML](https://browse.arxiv.org/html/2405.00236v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00236)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00236v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00236)), ([SS](https://api.semanticscholar.org/arXiv:2405.00236))
  * 04/30 - **SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound** <br>([:x:](https://arxiv.org/abs/2405.00233)), ([:book:](https://browse.arxiv.org/pdf/2405.00233.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00233.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00233)), ([:house:](https://huggingface.co/papers/2405.00233)), ([HTML](https://browse.arxiv.org/html/2405.00233v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00233)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00233v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00233)), ([SS](https://api.semanticscholar.org/arXiv:2405.00233))
  * 04/30 - **Octopus v4: Graph of language models** <br>([:x:](https://arxiv.org/abs/2404.19296)), ([:book:](https://browse.arxiv.org/pdf/2404.19296.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19296.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19296)), ([:house:](https://huggingface.co/papers/2404.19296)), ([HTML](https://browse.arxiv.org/html/2404.19296v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19296)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19296v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19296)), ([SS](https://api.semanticscholar.org/arXiv:2404.19296))
  * 04/30 - **MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model** <br>([:x:](https://arxiv.org/abs/2404.19759)), ([:book:](https://browse.arxiv.org/pdf/2404.19759.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19759.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19759)), ([:house:](https://huggingface.co/papers/2404.19759)), ([HTML](https://browse.arxiv.org/html/2404.19759v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19759)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19759v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19759)), ([SS](https://api.semanticscholar.org/arXiv:2404.19759))
  * 04/30 - **MicroDreamer: Zero-shot 3D Generation in sim20 Seconds by Score-based Iterative Reconstruction** <br>([:x:](https://arxiv.org/abs/2404.19525)), ([:book:](https://browse.arxiv.org/pdf/2404.19525.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19525.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19525)), ([:house:](https://huggingface.co/papers/2404.19525)), ([HTML](https://browse.arxiv.org/html/2404.19525v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19525)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19525v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19525)), ([SS](https://api.semanticscholar.org/arXiv:2404.19525))
  * 04/30 - **Lightplane: Highly-Scalable Components for Neural 3D Fields** <br>([:x:](https://arxiv.org/abs/2404.19760)), ([:book:](https://browse.arxiv.org/pdf/2404.19760.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19760.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19760)), ([:house:](https://huggingface.co/papers/2404.19760)), ([HTML](https://browse.arxiv.org/html/2404.19760v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19760)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19760v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19760)), ([SS](https://api.semanticscholar.org/arXiv:2404.19760))
  * 04/30 - **KAN: Kolmogorov-Arnold Networks** <br>([:x:](https://arxiv.org/abs/2404.19756)), ([:book:](https://browse.arxiv.org/pdf/2404.19756.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19756.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19756)), ([:house:](https://huggingface.co/papers/2404.19756)), ([HTML](https://browse.arxiv.org/html/2404.19756v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19756)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19756v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19756)), ([SS](https://api.semanticscholar.org/arXiv:2404.19756))
  * 04/30 - **Iterative Reasoning Preference Optimization** <br>([:x:](https://arxiv.org/abs/2404.19733)), ([:book:](https://browse.arxiv.org/pdf/2404.19733.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19733.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19733)), ([:house:](https://huggingface.co/papers/2404.19733)), ([HTML](https://browse.arxiv.org/html/2404.19733v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19733)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19733v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19733)), ([SS](https://api.semanticscholar.org/arXiv:2404.19733))
  * 04/30 - **Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting** <br>([:x:](https://arxiv.org/abs/2404.19758)), ([:book:](https://browse.arxiv.org/pdf/2404.19758.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19758.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19758)), ([:house:](https://huggingface.co/papers/2404.19758)), ([HTML](https://browse.arxiv.org/html/2404.19758v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19758)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19758v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19758)), ([SS](https://api.semanticscholar.org/arXiv:2404.19758))
  * 04/30 - **InstantFamily: Masked Attention for Zero-shot Multi-ID Image Generation** <br>([:x:](https://arxiv.org/abs/2404.19427)), ([:book:](https://browse.arxiv.org/pdf/2404.19427.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19427.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19427)), ([:house:](https://huggingface.co/papers/2404.19427)), ([HTML](https://browse.arxiv.org/html/2404.19427v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19427)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19427v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19427)), ([SS](https://api.semanticscholar.org/arXiv:2404.19427))
  * 04/30 - **GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting** <br>([:x:](https://arxiv.org/abs/2404.19702)), ([:book:](https://browse.arxiv.org/pdf/2404.19702.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19702.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19702)), ([:house:](https://huggingface.co/papers/2404.19702)), ([HTML](https://browse.arxiv.org/html/2404.19702v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19702)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19702v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19702)), ([SS](https://api.semanticscholar.org/arXiv:2404.19702))
  * 04/30 - **Extending Llama-3's Context Ten-Fold Overnight** <br>([:x:](https://arxiv.org/abs/2404.19553)), ([:book:](https://browse.arxiv.org/pdf/2404.19553.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19553.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19553)), ([:house:](https://huggingface.co/papers/2404.19553)), ([HTML](https://browse.arxiv.org/html/2404.19553v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19553)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19553v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19553)), ([SS](https://api.semanticscholar.org/arXiv:2404.19553))
  * 04/30 - **DOCCI: Descriptions of Connected and Contrasting Images** <br>([:x:](https://arxiv.org/abs/2404.19753)), ([:book:](https://browse.arxiv.org/pdf/2404.19753.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19753.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19753)), ([:house:](https://huggingface.co/papers/2404.19753)), ([HTML](https://browse.arxiv.org/html/2404.19753v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19753)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19753v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19753)), ([SS](https://api.semanticscholar.org/arXiv:2404.19753))
  * 04/30 - **Better & Faster Large Language Models via Multi-token Prediction** <br>([:x:](https://arxiv.org/abs/2404.19737)), ([:book:](https://browse.arxiv.org/pdf/2404.19737.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19737.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19737)), ([:house:](https://huggingface.co/papers/2404.19737)), ([HTML](https://browse.arxiv.org/html/2404.19737v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19737)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19737v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19737)), ([SS](https://api.semanticscholar.org/arXiv:2404.19737))
  * 04/29 - **Stylus: Automatic Adapter Selection for Diffusion Models** <br>([:x:](https://arxiv.org/abs/2404.18928)), ([:book:](https://browse.arxiv.org/pdf/2404.18928.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.18928.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.18928)), ([:house:](https://huggingface.co/papers/2404.18928)), ([HTML](https://browse.arxiv.org/html/2404.18928v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.18928)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.18928v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.18928)), ([SS](https://api.semanticscholar.org/arXiv:2404.18928))
  * 04/29 - **SAGS: Structure-Aware 3D Gaussian Splatting** <br>([:x:](https://arxiv.org/abs/2404.19149)), ([:book:](https://browse.arxiv.org/pdf/2404.19149.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.19149.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.19149)), ([:house:](https://huggingface.co/papers/2404.19149)), ([HTML](https://browse.arxiv.org/html/2404.19149v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.19149)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.19149v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.19149)), ([SS](https://api.semanticscholar.org/arXiv:2404.19149))
  * 04/29 - **Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models** <br>([:x:](https://arxiv.org/abs/2404.18796)), ([:book:](https://browse.arxiv.org/pdf/2404.18796.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.18796.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.18796)), ([:house:](https://huggingface.co/papers/2404.18796)), ([HTML](https://browse.arxiv.org/html/2404.18796v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.18796)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.18796v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.18796)), ([SS](https://api.semanticscholar.org/arXiv:2404.18796))
  * 04/29 - **NIST  AI RMF Generative AI Profile** <br>  ([News](https://www.nist.gov/news-events/news/2024/04/department-commerce-announces-new-actions-implement-president-bidens)), 
  * 04/29 - **LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report** <br>([:x:](https://arxiv.org/abs/2405.00732)), ([:book:](https://browse.arxiv.org/pdf/2405.00732.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00732.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00732)), ([:house:](https://huggingface.co/papers/2405.00732)), ([HTML](https://browse.arxiv.org/html/2405.00732v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00732)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00732v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00732)), ([SS](https://api.semanticscholar.org/arXiv:2405.00732))
  * 04/29 - **Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting** <br>([:x:](https://arxiv.org/abs/2404.18911)), ([:book:](https://browse.arxiv.org/pdf/2404.18911.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.18911.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.18911)), ([:house:](https://huggingface.co/papers/2404.18911)), ([HTML](https://browse.arxiv.org/html/2404.18911v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.18911)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.18911v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.18911)), ([SS](https://api.semanticscholar.org/arXiv:2404.18911))
  * 04/29 - **Capabilities of Gemini Models in Medicine** <br>([:x:](https://arxiv.org/abs/2404.18416)), ([:book:](https://browse.arxiv.org/pdf/2404.18416.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.18416.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.18416)), ([:house:](https://huggingface.co/papers/2404.18416)), ([HTML](https://browse.arxiv.org/html/2404.18416v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.18416)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.18416v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.18416)), ([SS](https://api.semanticscholar.org/arXiv:2404.18416))
  * 04/28 - **Paint by Inpaint: Learning to Add Image Objects by Removing Them First** <br>([:x:](https://arxiv.org/abs/2404.18212)), ([:book:](https://browse.arxiv.org/pdf/2404.18212.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.18212.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.18212)), ([:house:](https://huggingface.co/papers/2404.18212)), ([HTML](https://browse.arxiv.org/html/2404.18212v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.18212)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.18212v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.18212)), ([SS](https://api.semanticscholar.org/arXiv:2404.18212))
  * 04/28 - **LEGENT: Open Platform for Embodied Agents** <br>([:x:](https://arxiv.org/abs/2404.18243)), ([:book:](https://browse.arxiv.org/pdf/2404.18243.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.18243.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.18243)), ([:house:](https://huggingface.co/papers/2404.18243)), ([HTML](https://browse.arxiv.org/html/2404.18243v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.18243)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.18243v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.18243)), ([SS](https://api.semanticscholar.org/arXiv:2404.18243))
  * 04/27 - **Ag2Manip: Learning Novel Manipulation Skills with Agent-Agnostic Visual and Action Representations** <br>([:x:](https://arxiv.org/abs/2404.17521)), ([:book:](https://browse.arxiv.org/pdf/2404.17521.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.17521.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.17521)), ([:house:](https://huggingface.co/papers/2404.17521)), ([HTML](https://browse.arxiv.org/html/2404.17521v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.17521)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.17521v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.17521)), ([SS](https://api.semanticscholar.org/arXiv:2404.17521))
  * 04/26 - **MaPa: Text-driven Photorealistic Material Painting for 3D Shapes** <br>([:x:](https://arxiv.org/abs/2404.17569)), ([:book:](https://browse.arxiv.org/pdf/2404.17569.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.17569.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.17569)), ([:house:](https://huggingface.co/papers/2404.17569)), ([HTML](https://browse.arxiv.org/html/2404.17569v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.17569)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.17569v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.17569)), ([SS](https://api.semanticscholar.org/arXiv:2404.17569))
  * 04/26 - **BlenderAlchemy: Editing 3D Graphics with Vision-Language Models** <br>([:x:](https://arxiv.org/abs/2404.17672)), ([:book:](https://browse.arxiv.org/pdf/2404.17672.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.17672.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.17672)), ([:house:](https://huggingface.co/papers/2404.17672)), ([HTML](https://browse.arxiv.org/html/2404.17672v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.17672)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.17672v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.17672)), ([SS](https://api.semanticscholar.org/arXiv:2404.17672))
  * 04/25 - **Tele-FLM Technical Report** <br>([:x:](https://arxiv.org/abs/2404.16645)), ([:book:](https://browse.arxiv.org/pdf/2404.16645.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16645.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16645)), ([:house:](https://huggingface.co/papers/2404.16645)), ([HTML](https://browse.arxiv.org/html/2404.16645v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16645)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16645v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16645)), ([SS](https://api.semanticscholar.org/arXiv:2404.16645))
  * 04/25 - **SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension** <br>([:x:](https://arxiv.org/abs/2404.16790)), ([:book:](https://browse.arxiv.org/pdf/2404.16790.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16790.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16790)), ([:house:](https://huggingface.co/papers/2404.16790)), ([HTML](https://browse.arxiv.org/html/2404.16790v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16790)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16790v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16790)), ([SS](https://api.semanticscholar.org/arXiv:2404.16790))
  * 04/25 - **Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and Human Ratings** <br>([:x:](https://arxiv.org/abs/2404.16820)), ([:book:](https://browse.arxiv.org/pdf/2404.16820.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16820.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16820)), ([:house:](https://huggingface.co/papers/2404.16820)), ([HTML](https://browse.arxiv.org/html/2404.16820v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16820)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16820v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16820)), ([SS](https://api.semanticscholar.org/arXiv:2404.16820))
  * 04/25 - **PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning** <br>([:x:](https://arxiv.org/abs/2404.16994)), ([:book:](https://browse.arxiv.org/pdf/2404.16994.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16994.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16994)), ([:house:](https://huggingface.co/papers/2404.16994)), ([HTML](https://browse.arxiv.org/html/2404.16994v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16994)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16994v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16994)), ([SS](https://api.semanticscholar.org/arXiv:2404.16994))
  * 04/25 - **Make Your LLM Fully Utilize the Context** <br>([:x:](https://arxiv.org/abs/2404.16811)), ([:book:](https://browse.arxiv.org/pdf/2404.16811.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16811.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16811)), ([:house:](https://huggingface.co/papers/2404.16811)), ([HTML](https://browse.arxiv.org/html/2404.16811v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16811)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16811v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16811)), ([SS](https://api.semanticscholar.org/arXiv:2404.16811))
  * 04/25 - **List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs** <br>([:x:](https://arxiv.org/abs/2404.16375)), ([:book:](https://browse.arxiv.org/pdf/2404.16375.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16375.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16375)), ([:house:](https://huggingface.co/papers/2404.16375)), ([HTML](https://browse.arxiv.org/html/2404.16375v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16375)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16375v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16375)), ([SS](https://api.semanticscholar.org/arXiv:2404.16375))
  * 04/25 - **Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding** <br>([:x:](https://arxiv.org/abs/2404.16710)), ([:book:](https://browse.arxiv.org/pdf/2404.16710.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16710.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16710)), ([:house:](https://huggingface.co/papers/2404.16710)), ([HTML](https://browse.arxiv.org/html/2404.16710v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16710)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16710v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16710)), ([SS](https://api.semanticscholar.org/arXiv:2404.16710))
  * 04/25 - **Interactive3D: Create What You Want by Interactive 3D Generation** <br>([:x:](https://arxiv.org/abs/2404.16510)), ([:book:](https://browse.arxiv.org/pdf/2404.16510.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16510.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16510)), ([:house:](https://huggingface.co/papers/2404.16510)), ([HTML](https://browse.arxiv.org/html/2404.16510v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16510)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16510v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16510)), ([SS](https://api.semanticscholar.org/arXiv:2404.16510))
  * 04/25 - **How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites** <br>([:x:](https://arxiv.org/abs/2404.16821)), ([:book:](https://browse.arxiv.org/pdf/2404.16821.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16821.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16821)), ([:house:](https://huggingface.co/papers/2404.16821)), ([HTML](https://browse.arxiv.org/html/2404.16821v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16821)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16821v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16821)), ([SS](https://api.semanticscholar.org/arXiv:2404.16821))
  * 04/25 - **ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving** <br>([:x:](https://arxiv.org/abs/2404.16771)), ([:book:](https://browse.arxiv.org/pdf/2404.16771.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16771.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16771)), ([:house:](https://huggingface.co/papers/2404.16771)), ([HTML](https://browse.arxiv.org/html/2404.16771v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16771)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16771v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16771)), ([SS](https://api.semanticscholar.org/arXiv:2404.16771))
  * 04/24 - **XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference** <br>([:x:](https://arxiv.org/abs/2404.15420)), ([:book:](https://browse.arxiv.org/pdf/2404.15420.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.15420.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.15420)), ([:house:](https://huggingface.co/papers/2404.15420)), ([HTML](https://browse.arxiv.org/html/2404.15420v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.15420)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.15420v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.15420)), ([SS](https://api.semanticscholar.org/arXiv:2404.15420))
  * 04/24 - **The Ethics of Advanced AI Assistants** <br>([:x:](https://arxiv.org/abs/2404.16244)), ([:book:](https://browse.arxiv.org/pdf/2404.16244.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16244.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16244)), ([:house:](https://huggingface.co/papers/2404.16244)), ([HTML](https://browse.arxiv.org/html/2404.16244v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16244)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16244v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16244)), ([SS](https://api.semanticscholar.org/arXiv:2404.16244))
  * 04/24 - **PuLID: Pure and Lightning ID Customization via Contrastive Alignment** <br>([:x:](https://arxiv.org/abs/2404.16022)), ([:book:](https://browse.arxiv.org/pdf/2404.16022.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16022.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16022)), ([:house:](https://huggingface.co/papers/2404.16022)), ([HTML](https://browse.arxiv.org/html/2404.16022v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16022)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16022v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16022)), ([SS](https://api.semanticscholar.org/arXiv:2404.16022))
  * 04/24 - **NeRF-XL: Scaling NeRFs with Multiple GPUs** <br>([:x:](https://arxiv.org/abs/2404.16221)), ([:book:](https://browse.arxiv.org/pdf/2404.16221.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16221.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16221)), ([:house:](https://huggingface.co/papers/2404.16221)), ([HTML](https://browse.arxiv.org/html/2404.16221v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16221)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16221v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16221)), ([SS](https://api.semanticscholar.org/arXiv:2404.16221))
  * 04/24 - **MotionMaster: Training-free Camera Motion Transfer For Video Generation** <br>([:x:](https://arxiv.org/abs/2404.15789)), ([:book:](https://browse.arxiv.org/pdf/2404.15789.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.15789.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.15789)), ([:house:](https://huggingface.co/papers/2404.15789)), ([HTML](https://browse.arxiv.org/html/2404.15789v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.15789)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.15789v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.15789)), ([SS](https://api.semanticscholar.org/arXiv:2404.15789))
  * 04/24 - **MoDE: CLIP Data Experts via Clustering** <br>([:x:](https://arxiv.org/abs/2404.16030)), ([:book:](https://browse.arxiv.org/pdf/2404.16030.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16030.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16030)), ([:house:](https://huggingface.co/papers/2404.16030)), ([HTML](https://browse.arxiv.org/html/2404.16030v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16030)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16030v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16030)), ([SS](https://api.semanticscholar.org/arXiv:2404.16030))
  * 04/24 - **MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI** <br>([:x:](https://arxiv.org/abs/2404.16006)), ([:book:](https://browse.arxiv.org/pdf/2404.16006.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16006.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16006)), ([:house:](https://huggingface.co/papers/2404.16006)), ([HTML](https://browse.arxiv.org/html/2404.16006v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16006)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16006v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16006)), ([SS](https://api.semanticscholar.org/arXiv:2404.16006))
  * 04/24 - **MaGGIe: Masked Guided Gradual Human Instance Matting** <br>([:x:](https://arxiv.org/abs/2404.16035)), ([:book:](https://browse.arxiv.org/pdf/2404.16035.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16035.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16035)), ([:house:](https://huggingface.co/papers/2404.16035)), ([HTML](https://browse.arxiv.org/html/2404.16035v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16035)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16035v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16035)), ([SS](https://api.semanticscholar.org/arXiv:2404.16035))
  * 04/24 - **ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with Reward Feedback Learning** <br>([:x:](https://arxiv.org/abs/2404.15449)), ([:book:](https://browse.arxiv.org/pdf/2404.15449.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.15449.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.15449)), ([:house:](https://huggingface.co/papers/2404.15449)), ([HTML](https://browse.arxiv.org/html/2404.15449v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.15449)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.15449v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.15449)), ([SS](https://api.semanticscholar.org/arXiv:2404.15449))
  * 04/24 - **Editable Image Elements for Controllable Synthesis** <br>([:x:](https://arxiv.org/abs/2404.16029)), ([:book:](https://browse.arxiv.org/pdf/2404.16029.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16029.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16029)), ([:house:](https://huggingface.co/papers/2404.16029)), ([HTML](https://browse.arxiv.org/html/2404.16029v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16029)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16029v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16029)), ([SS](https://api.semanticscholar.org/arXiv:2404.16029))
  * 04/24 - **CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data** <br>([:x:](https://arxiv.org/abs/2404.15653)), ([:book:](https://browse.arxiv.org/pdf/2404.15653.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.15653.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.15653)), ([:house:](https://huggingface.co/papers/2404.15653)), ([HTML](https://browse.arxiv.org/html/2404.15653v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.15653)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.15653v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.15653)), ([SS](https://api.semanticscholar.org/arXiv:2404.15653))
  * 04/24 - **BASS: Batched Attention-optimized Speculative Sampling** <br>([:x:](https://arxiv.org/abs/2404.15778)), ([:book:](https://browse.arxiv.org/pdf/2404.15778.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.15778.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.15778)), ([:house:](https://huggingface.co/papers/2404.15778)), ([HTML](https://browse.arxiv.org/html/2404.15778v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.15778)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.15778v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.15778)), ([SS](https://api.semanticscholar.org/arXiv:2404.15778))
  * 04/23 - **Transformers Can Represent n-gram Language Models** <br>([:x:](https://arxiv.org/abs/2404.14994)), ([:book:](https://browse.arxiv.org/pdf/2404.14994.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14994.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14994)), ([:house:](https://huggingface.co/papers/2404.14994)), ([HTML](https://browse.arxiv.org/html/2404.14994v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14994)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14994v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14994)), ([SS](https://api.semanticscholar.org/arXiv:2404.14994))
  * 04/23 - **Pegasus-v1 Technical Report** <br>([:x:](https://arxiv.org/abs/2404.14687)), ([:book:](https://browse.arxiv.org/pdf/2404.14687.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14687.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14687)), ([:house:](https://huggingface.co/papers/2404.14687)), ([HTML](https://browse.arxiv.org/html/2404.14687v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14687)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14687v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14687)), ([SS](https://api.semanticscholar.org/arXiv:2404.14687))
  * 04/23 - **Multi-Head Mixture-of-Experts** <br>([:x:](https://arxiv.org/abs/2404.15045)), ([:book:](https://browse.arxiv.org/pdf/2404.15045.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.15045.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.15045)), ([:house:](https://huggingface.co/papers/2404.15045)), ([HTML](https://browse.arxiv.org/html/2404.15045v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.15045)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.15045v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.15045)), ([SS](https://api.semanticscholar.org/arXiv:2404.15045))
  * 04/23 - **FlashSpeech: Efficient Zero-Shot Speech Synthesis** <br>([:x:](https://arxiv.org/abs/2404.14700)), ([:book:](https://browse.arxiv.org/pdf/2404.14700.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14700.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14700)), ([:house:](https://huggingface.co/papers/2404.14700)), ([HTML](https://browse.arxiv.org/html/2404.14700v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14700)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14700v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14700)), ([SS](https://api.semanticscholar.org/arXiv:2404.14700))
  * 04/22 - **SnapKV: LLM Knows What You are Looking for Before Generation** <br>([:x:](https://arxiv.org/abs/2404.14469)), ([:book:](https://browse.arxiv.org/pdf/2404.14469.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14469.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14469)), ([:house:](https://huggingface.co/papers/2404.14469)), ([HTML](https://browse.arxiv.org/html/2404.14469v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14469)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14469v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14469)), ([SS](https://api.semanticscholar.org/arXiv:2404.14469))
  * 04/22 - **SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation** <br>([:x:](https://arxiv.org/abs/2404.14396)), ([:book:](https://browse.arxiv.org/pdf/2404.14396.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14396.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14396)), ([:house:](https://huggingface.co/papers/2404.14396)), ([HTML](https://browse.arxiv.org/html/2404.14396v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14396)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14396v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14396)), ([SS](https://api.semanticscholar.org/arXiv:2404.14396))
  * 04/22 - **Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer** <br>([:x:](https://arxiv.org/abs/2404.14351)), ([:book:](https://browse.arxiv.org/pdf/2404.14351.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14351.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14351)), ([:house:](https://huggingface.co/papers/2404.14351)), ([HTML](https://browse.arxiv.org/html/2404.14351v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14351)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14351v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14351)), ([SS](https://api.semanticscholar.org/arXiv:2404.14351))
  * 04/22 - **Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone** <br>([:x:](https://arxiv.org/abs/2404.14219)), ([:book:](https://browse.arxiv.org/pdf/2404.14219.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14219.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14219)), ([:house:](https://huggingface.co/papers/2404.14219)), ([HTML](https://browse.arxiv.org/html/2404.14219v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14219)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14219v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14219)), ([SS](https://api.semanticscholar.org/arXiv:2404.14219))
  * 04/22 - **OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework** <br>([:x:](https://arxiv.org/abs/2404.14619)), ([:book:](https://browse.arxiv.org/pdf/2404.14619.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14619.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14619)), ([:house:](https://huggingface.co/papers/2404.14619)), ([HTML](https://browse.arxiv.org/html/2404.14619v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14619)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14619v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14619)), ([SS](https://api.semanticscholar.org/arXiv:2404.14619))
  * 04/22 - **MultiBooth: Towards Generating All Your Concepts in an Image from Text** <br>([:x:](https://arxiv.org/abs/2404.14239)), ([:book:](https://browse.arxiv.org/pdf/2404.14239.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14239.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14239)), ([:house:](https://huggingface.co/papers/2404.14239)), ([HTML](https://browse.arxiv.org/html/2404.14239v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14239)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14239v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14239)), ([SS](https://api.semanticscholar.org/arXiv:2404.14239))
  * 04/22 - **Learning H-Infinity Locomotion Control** <br>([:x:](https://arxiv.org/abs/2404.14405)), ([:book:](https://browse.arxiv.org/pdf/2404.14405.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14405.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14405)), ([:house:](https://huggingface.co/papers/2404.14405)), ([HTML](https://browse.arxiv.org/html/2404.14405v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14405)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14405v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14405)), ([SS](https://api.semanticscholar.org/arXiv:2404.14405))
  * 04/22 - **How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study** <br>([:x:](https://arxiv.org/abs/2404.14047)), ([:book:](https://browse.arxiv.org/pdf/2404.14047.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14047.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14047)), ([:house:](https://huggingface.co/papers/2404.14047)), ([HTML](https://browse.arxiv.org/html/2404.14047v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14047)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14047v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14047)), ([SS](https://api.semanticscholar.org/arXiv:2404.14047))
  * 04/22 - **Align Your Steps: Optimizing Sampling Schedules in Diffusion Models** <br>([:x:](https://arxiv.org/abs/2404.14507)), ([:book:](https://browse.arxiv.org/pdf/2404.14507.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14507.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14507)), ([:house:](https://huggingface.co/papers/2404.14507)), ([HTML](https://browse.arxiv.org/html/2404.14507v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14507)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14507v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14507)), ([SS](https://api.semanticscholar.org/arXiv:2404.14507))
  * 04/22 - **A Multimodal Automated Interpretability Agent** <br>([:x:](https://arxiv.org/abs/2404.14394)), ([:book:](https://browse.arxiv.org/pdf/2404.14394.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.14394.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.14394)), ([:house:](https://huggingface.co/papers/2404.14394)), ([HTML](https://browse.arxiv.org/html/2404.14394v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.14394)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.14394v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14394)), ([SS](https://api.semanticscholar.org/arXiv:2404.14394))
  * 04/21 - **Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis** <br>([:x:](https://arxiv.org/abs/2404.13686)), ([:book:](https://browse.arxiv.org/pdf/2404.13686.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.13686.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.13686)), ([:house:](https://huggingface.co/papers/2404.13686)), ([HTML](https://browse.arxiv.org/html/2404.13686v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.13686)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.13686v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.13686)), ([SS](https://api.semanticscholar.org/arXiv:2404.13686))
  * 04/21 - **AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs** <br>([:x:](https://arxiv.org/abs/2404.16873)), ([:book:](https://browse.arxiv.org/pdf/2404.16873.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.16873.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.16873)), ([:house:](https://huggingface.co/papers/2404.16873)), ([HTML](https://browse.arxiv.org/html/2404.16873v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.16873)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.16873v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.16873)), ([SS](https://api.semanticscholar.org/arXiv:2404.16873))
  * 04/20 - **Music Consistency Models** <br>([:x:](https://arxiv.org/abs/2404.13358)), ([:book:](https://browse.arxiv.org/pdf/2404.13358.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.13358.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.13358)), ([:house:](https://huggingface.co/papers/2404.13358)), ([HTML](https://browse.arxiv.org/html/2404.13358v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.13358)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.13358v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.13358)), ([SS](https://api.semanticscholar.org/arXiv:2404.13358))
  * 04/19 - **The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions** <br>([:x:](https://arxiv.org/abs/2404.13208)), ([:book:](https://browse.arxiv.org/pdf/2404.13208.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.13208.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.13208)), ([:house:](https://huggingface.co/papers/2404.13208)), ([HTML](https://browse.arxiv.org/html/2404.13208v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.13208)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.13208v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.13208)), ([SS](https://api.semanticscholar.org/arXiv:2404.13208))
  * 04/19 - **TextSquare: Scaling up Text-Centric Visual Instruction Tuning** <br>([:x:](https://arxiv.org/abs/2404.12803)), ([:book:](https://browse.arxiv.org/pdf/2404.12803.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12803.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12803)), ([:house:](https://huggingface.co/papers/2404.12803)), ([HTML](https://browse.arxiv.org/html/2404.12803v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12803)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12803v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12803)), ([SS](https://api.semanticscholar.org/arXiv:2404.12803))
  * 04/19 - **PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation** <br>([:x:](https://arxiv.org/abs/2404.13026)), ([:book:](https://browse.arxiv.org/pdf/2404.13026.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.13026.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.13026)), ([:house:](https://huggingface.co/papers/2404.13026)), ([HTML](https://browse.arxiv.org/html/2404.13026v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.13026)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.13026v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.13026)), ([SS](https://api.semanticscholar.org/arXiv:2404.13026))
  * 04/19 - **LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency** <br>([:x:](https://arxiv.org/abs/2404.12872)), ([:book:](https://browse.arxiv.org/pdf/2404.12872.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12872.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12872)), ([:house:](https://huggingface.co/papers/2404.12872)), ([HTML](https://browse.arxiv.org/html/2404.12872v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12872)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12872v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12872)), ([SS](https://api.semanticscholar.org/arXiv:2404.12872))
  * 04/19 - **How Real Is Real? A Human Evaluation Framework for Unrestricted Adversarial Examples** <br>([:x:](https://arxiv.org/abs/2404.12653)), ([:book:](https://browse.arxiv.org/pdf/2404.12653.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12653.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12653)), ([:house:](https://huggingface.co/papers/2404.12653)), ([HTML](https://browse.arxiv.org/html/2404.12653v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12653)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12653v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12653)), ([SS](https://api.semanticscholar.org/arXiv:2404.12653))
  * 04/19 - **How Far Can We Go with Practical Function-Level Program Repair?** <br>([:x:](https://arxiv.org/abs/2404.12833)), ([:book:](https://browse.arxiv.org/pdf/2404.12833.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12833.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12833)), ([:house:](https://huggingface.co/papers/2404.12833)), ([HTML](https://browse.arxiv.org/html/2404.12833v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12833)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12833v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12833)), ([SS](https://api.semanticscholar.org/arXiv:2404.12833))
  * 04/19 - **Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.13013)), ([:book:](https://browse.arxiv.org/pdf/2404.13013.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.13013.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.13013)), ([:house:](https://huggingface.co/papers/2404.13013)), ([HTML](https://browse.arxiv.org/html/2404.13013v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.13013)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.13013v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.13013)), ([SS](https://api.semanticscholar.org/arXiv:2404.13013))
  * 04/19 - **Does Gaussian Splatting need SFM Initialization?** <br>([:x:](https://arxiv.org/abs/2404.12547)), ([:book:](https://browse.arxiv.org/pdf/2404.12547.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12547.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12547)), ([:house:](https://huggingface.co/papers/2404.12547)), ([HTML](https://browse.arxiv.org/html/2404.12547v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12547)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12547v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12547)), ([SS](https://api.semanticscholar.org/arXiv:2404.12547))
  * 04/19 - **AutoCrawler: A Progressive Understanding Web Agent for Web Crawler Generation** <br>([:x:](https://arxiv.org/abs/2404.12753)), ([:book:](https://browse.arxiv.org/pdf/2404.12753.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12753.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12753)), ([:house:](https://huggingface.co/papers/2404.12753)), ([HTML](https://browse.arxiv.org/html/2404.12753v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12753)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12753v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12753)), ([SS](https://api.semanticscholar.org/arXiv:2404.12753))
  * 04/18 - **TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding** <br>([:x:](https://arxiv.org/abs/2404.11912)), ([:book:](https://browse.arxiv.org/pdf/2404.11912.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.11912.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.11912)), ([:house:](https://huggingface.co/papers/2404.11912)), ([HTML](https://browse.arxiv.org/html/2404.11912v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.11912)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.11912v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.11912)), ([SS](https://api.semanticscholar.org/arXiv:2404.11912))
  * 04/18 - **Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing** <br>([:x:](https://arxiv.org/abs/2404.12253)), ([:book:](https://browse.arxiv.org/pdf/2404.12253.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12253.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12253)), ([:house:](https://huggingface.co/papers/2404.12253)), ([HTML](https://browse.arxiv.org/html/2404.12253v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12253)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12253v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12253)), ([SS](https://api.semanticscholar.org/arXiv:2404.12253))
  * 04/18 - **Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment** <br>([:x:](https://arxiv.org/abs/2404.12318)), ([:book:](https://browse.arxiv.org/pdf/2404.12318.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12318.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12318)), ([:house:](https://huggingface.co/papers/2404.12318)), ([HTML](https://browse.arxiv.org/html/2404.12318v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12318)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12318v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12318)), ([SS](https://api.semanticscholar.org/arXiv:2404.12318))
  * 04/18 - **Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models** <br>([:x:](https://arxiv.org/abs/2404.12387)), ([:book:](https://browse.arxiv.org/pdf/2404.12387.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12387.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12387)), ([:house:](https://huggingface.co/papers/2404.12387)), ([HTML](https://browse.arxiv.org/html/2404.12387v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12387)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12387v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12387)), ([SS](https://api.semanticscholar.org/arXiv:2404.12387))
  * 04/18 - **OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data** <br>([:x:](https://arxiv.org/abs/2404.12195)), ([:book:](https://browse.arxiv.org/pdf/2404.12195.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12195.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12195)), ([:house:](https://huggingface.co/papers/2404.12195)), ([HTML](https://browse.arxiv.org/html/2404.12195v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12195)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12195v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12195)), ([SS](https://api.semanticscholar.org/arXiv:2404.12195))
  * 04/18 - **MeshLRM: Large Reconstruction Model for High-Quality Mesh** <br>([:x:](https://arxiv.org/abs/2404.12385)), ([:book:](https://browse.arxiv.org/pdf/2404.12385.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12385.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12385)), ([:house:](https://huggingface.co/papers/2404.12385)), ([HTML](https://browse.arxiv.org/html/2404.12385v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12385)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12385v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12385)), ([SS](https://api.semanticscholar.org/arXiv:2404.12385))
  * 04/18 - **Introducing v0.5 of the AI Safety Benchmark from MLCommons** <br>([:x:](https://arxiv.org/abs/2404.12241)), ([:book:](https://browse.arxiv.org/pdf/2404.12241.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12241.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12241)), ([:house:](https://huggingface.co/papers/2404.12241)), ([HTML](https://browse.arxiv.org/html/2404.12241v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12241)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12241v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12241)), ([SS](https://api.semanticscholar.org/arXiv:2404.12241))
  * 04/18 - **Introducing Meta Llama 3: The most capable openly available LLM to date** <br>  ([Blog](https://ai.meta.com/blog/meta-llama-3/)), 
  * 04/18 - **EdgeFusion: On-Device Text-to-Image Generation** <br>([:x:](https://arxiv.org/abs/2404.11925)), ([:book:](https://browse.arxiv.org/pdf/2404.11925.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.11925.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.11925)), ([:house:](https://huggingface.co/papers/2404.11925)), ([HTML](https://browse.arxiv.org/html/2404.11925v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.11925)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.11925v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.11925)), ([SS](https://api.semanticscholar.org/arXiv:2404.11925))
  * 04/18 - **BLINK: Multimodal Large Language Models Can See but Not Perceive** <br>([:x:](https://arxiv.org/abs/2404.12390)), ([:book:](https://browse.arxiv.org/pdf/2404.12390.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12390.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12390)), ([:house:](https://huggingface.co/papers/2404.12390)), ([HTML](https://browse.arxiv.org/html/2404.12390v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12390)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12390v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12390)), ([SS](https://api.semanticscholar.org/arXiv:2404.12390))
  * 04/18 - **AniClipart: Clipart Animation with Text-to-Video Priors** <br>([:x:](https://arxiv.org/abs/2404.12347)), ([:book:](https://browse.arxiv.org/pdf/2404.12347.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.12347.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.12347)), ([:house:](https://huggingface.co/papers/2404.12347)), ([HTML](https://browse.arxiv.org/html/2404.12347v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.12347)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.12347v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.12347)), ([SS](https://api.semanticscholar.org/arXiv:2404.12347))
  * 04/17 - **MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation** <br>([:x:](https://arxiv.org/abs/2404.11565)), ([:book:](https://browse.arxiv.org/pdf/2404.11565.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.11565.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.11565)), ([:house:](https://huggingface.co/papers/2404.11565)), ([HTML](https://browse.arxiv.org/html/2404.11565v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.11565)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.11565v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.11565)), ([SS](https://api.semanticscholar.org/arXiv:2404.11565))
  * 04/17 - **FlowMind: Automatic Workflow Generation with LLMs** <br>([:x:](https://arxiv.org/abs/2404.13050)), ([:book:](https://browse.arxiv.org/pdf/2404.13050.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.13050.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.13050)), ([:house:](https://huggingface.co/papers/2404.13050)), ([HTML](https://browse.arxiv.org/html/2404.13050v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.13050)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.13050v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.13050)), ([SS](https://api.semanticscholar.org/arXiv:2404.13050))
  * 04/17 - **Dynamic Typography: Bringing Words to Life** <br>([:x:](https://arxiv.org/abs/2404.11614)), ([:book:](https://browse.arxiv.org/pdf/2404.11614.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.11614.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.11614)), ([:house:](https://huggingface.co/papers/2404.11614)), ([HTML](https://browse.arxiv.org/html/2404.11614v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.11614)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.11614v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.11614)), ([SS](https://api.semanticscholar.org/arXiv:2404.11614))
  * 04/17 - **Stable Diffusion 3 API Now Available** <br>  ([twitter](https://twitter.com/StabilityAI/status/1780599024707596508)),  ([Blog](https://stability.ai/news/stable-diffusion-3-api?utm_source=twitter&utm_medium=website&utm_campaign=blog)),  ([Demo](https://platform.stability.ai/docs/api-reference#tag/Generate/paths/~1v2beta~1stable-image~1generate~1sd3/post)), 
  * 04/16 - **VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time** <br>([:x:](https://arxiv.org/abs/2404.10667)), ([:book:](https://browse.arxiv.org/pdf/2404.10667.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.10667.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.10667)), ([:house:](https://huggingface.co/papers/2404.10667)), ([HTML](https://browse.arxiv.org/html/2404.10667v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.10667)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.10667v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.10667)), ([SS](https://api.semanticscholar.org/arXiv:2404.10667)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vasa-1-lifelike-audio-driven-talking-faces))
  * 04/16 - **U.S. Commerce Secretary Gina Raimondo Announces Expansion of U.S. AI Safety Institute Leadership Team** <br>  ([News](https://www.commerce.gov/news/press-releases/2024/04/us-commerce-secretary-gina-raimondo-announces-expansion-us-ai-safety)), 
  * 04/16 - **Long-form music generation with latent diffusion** <br>([:x:](https://arxiv.org/abs/2404.10301)), ([:book:](https://browse.arxiv.org/pdf/2404.10301.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.10301.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.10301)), ([:house:](https://huggingface.co/papers/2404.10301)), ([HTML](https://browse.arxiv.org/html/2404.10301v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.10301)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.10301v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.10301)), ([SS](https://api.semanticscholar.org/arXiv:2404.10301))
  * 04/15 - **LLM Evaluators Recognize and Favor Their Own Generations** <br>([:x:](https://arxiv.org/abs/2404.13076)), ([:book:](https://browse.arxiv.org/pdf/2404.13076.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.13076.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.13076)), ([:house:](https://huggingface.co/papers/2404.13076)), ([HTML](https://browse.arxiv.org/html/2404.13076v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.13076)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.13076v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.13076)), ([SS](https://api.semanticscholar.org/arXiv:2404.13076))
  * 04/15 - **Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video** <br>([:x:](https://arxiv.org/abs/2404.09833)), ([:book:](https://browse.arxiv.org/pdf/2404.09833.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.09833.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.09833)), ([:house:](https://huggingface.co/papers/2404.09833)), ([HTML](https://browse.arxiv.org/html/2404.09833v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.09833)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.09833v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.09833)), ([SS](https://api.semanticscholar.org/arXiv:2404.09833)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video2game-real-time-interactive-realistic))
  * 04/15 - **Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization** <br>([:x:](https://arxiv.org/abs/2404.09956)), ([:book:](https://browse.arxiv.org/pdf/2404.09956.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.09956.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.09956)), ([:house:](https://huggingface.co/papers/2404.09956)), ([HTML](https://browse.arxiv.org/html/2404.09956v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.09956)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.09956v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.09956)), ([SS](https://api.semanticscholar.org/arXiv:2404.09956)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tango-2-aligning-diffusion-based-text-to)), ([:octocat:](https://github.com/declare-lab/tango)![GitHub Repo stars](https://img.shields.io/github/stars/declare-lab/tango?style=social))
  * 04/15 - **Taming Latent Diffusion Model for Neural Radiance Field Inpainting** <br>([:x:](https://arxiv.org/abs/2404.09995)), ([:book:](https://browse.arxiv.org/pdf/2404.09995.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.09995.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.09995)), ([:house:](https://huggingface.co/papers/2404.09995)), ([HTML](https://browse.arxiv.org/html/2404.09995v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.09995)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.09995v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.09995)), ([SS](https://api.semanticscholar.org/arXiv:2404.09995)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/taming-latent-diffusion-model-for-neural))
  * 04/15 - **Opus can operate as a Turing machine** <br>  ([twitter](https://twitter.com/ctjlewis/status/1779740038852690393)), 
  * 04/15 - **MathGPT: Leveraging Llama 2 to create a platform for highly personalized learning** <br> 
  * 04/15 - **HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing** <br>([:x:](https://arxiv.org/abs/2404.09990)), ([:book:](https://browse.arxiv.org/pdf/2404.09990.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.09990.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.09990)), ([:house:](https://huggingface.co/papers/2404.09990)), ([HTML](https://browse.arxiv.org/html/2404.09990v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.09990)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.09990v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.09990)), ([SS](https://api.semanticscholar.org/arXiv:2404.09990)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hq-edit-a-high-quality-dataset-for))
  * 04/15 - **Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model** <br>([:x:](https://arxiv.org/abs/2404.09967)), ([:book:](https://browse.arxiv.org/pdf/2404.09967.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.09967.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.09967)), ([:house:](https://huggingface.co/papers/2404.09967)), ([HTML](https://browse.arxiv.org/html/2404.09967v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.09967)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.09967v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.09967)), ([SS](https://api.semanticscholar.org/arXiv:2404.09967)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ctrl-adapter-an-efficient-and-versatile))
  * 04/15 - **Compression Represents Intelligence Linearly** <br>([:x:](https://arxiv.org/abs/2404.09937)), ([:book:](https://browse.arxiv.org/pdf/2404.09937.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.09937.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.09937)), ([:house:](https://huggingface.co/papers/2404.09937)), ([HTML](https://browse.arxiv.org/html/2404.09937v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.09937)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.09937v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.09937)), ([SS](https://api.semanticscholar.org/arXiv:2404.09937)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/compression-represents-intelligence-linearly))
  * 04/15 - **CompGS: Efficient 3D Scene Representation via Compressed Gaussian Splatting** <br>([:x:](https://arxiv.org/abs/2404.09458)), ([:book:](https://browse.arxiv.org/pdf/2404.09458.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.09458.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.09458)), ([:house:](https://huggingface.co/papers/2404.09458)), ([HTML](https://browse.arxiv.org/html/2404.09458v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.09458)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.09458v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.09458)), ([SS](https://api.semanticscholar.org/arXiv:2404.09458)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/compgs-efficient-3d-scene-representation-via))
  * 04/14 - **TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.09204)), ([:book:](https://browse.arxiv.org/pdf/2404.09204.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.09204.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.09204)), ([:house:](https://huggingface.co/papers/2404.09204)), ([HTML](https://browse.arxiv.org/html/2404.09204v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.09204)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.09204v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.09204)), ([SS](https://api.semanticscholar.org/arXiv:2404.09204)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/texthawk-exploring-efficient-fine-grained)), ([:octocat:](https://github.com/yuyq96/texthawk)![GitHub Repo stars](https://img.shields.io/github/stars/yuyq96/texthawk?style=social))
  * 04/13 - **Cathie Wood Muscles Into ChatGPT Boom With New OpenAI Stake** <br>  ([News](https://finance.yahoo.com/news/cathie-wood-ark-investment-management-232619722.html)),
  * 04/12 - **Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies** <br>([:x:](https://arxiv.org/abs/2404.08197)), ([:book:](https://browse.arxiv.org/pdf/2404.08197.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.08197.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.08197)), ([:house:](https://huggingface.co/papers/2404.08197)), ([HTML](https://browse.arxiv.org/html/2404.08197v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.08197)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.08197v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.08197)), ([SS](https://api.semanticscholar.org/arXiv:2404.08197)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaling-down-clip-a-comprehensive-analysis-of))
  * 04/12 - **Probing the 3D Awareness of Visual Foundation Models** <br>([:x:](https://arxiv.org/abs/2404.08636)), ([:book:](https://browse.arxiv.org/pdf/2404.08636.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.08636.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.08636)), ([:house:](https://huggingface.co/papers/2404.08636)), ([HTML](https://browse.arxiv.org/html/2404.08636v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.08636)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.08636v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.08636)), ([SS](https://api.semanticscholar.org/arXiv:2404.08636)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/probing-the-3d-awareness-of-visual-foundation)), ([:octocat:](https://github.com/mbanani/probe3d)![GitHub Repo stars](https://img.shields.io/github/stars/mbanani/probe3d?style=social))
  * 04/12 - **Pre-training Small Base LMs with Fewer Tokens** <br>([:x:](https://arxiv.org/abs/2404.08634)), ([:book:](https://browse.arxiv.org/pdf/2404.08634.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.08634.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.08634)), ([:house:](https://huggingface.co/papers/2404.08634)), ([HTML](https://browse.arxiv.org/html/2404.08634v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.08634)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.08634v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.08634)), ([SS](https://api.semanticscholar.org/arXiv:2404.08634)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pre-training-small-base-lms-with-fewer-tokens)), ([:octocat:](https://github.com/Lightning-AI/lit-gpt)![GitHub Repo stars](https://img.shields.io/github/stars/Lightning-AI/lit-gpt?style=social))
  * 04/12 - **On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation** <br>([:x:](https://arxiv.org/abs/2404.08540)), ([:book:](https://browse.arxiv.org/pdf/2404.08540.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.08540.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.08540)), ([:house:](https://huggingface.co/papers/2404.08540)), ([HTML](https://browse.arxiv.org/html/2404.08540v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.08540)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.08540v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.08540)), ([SS](https://api.semanticscholar.org/arXiv:2404.08540)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-the-robustness-of-language-guidance-for)), ([:octocat:](https://github.com/agneet42/robustness_depth_lang)![GitHub Repo stars](https://img.shields.io/github/stars/agneet42/robustness_depth_lang?style=social))
  * 04/12 - **MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based Monocular Guidance** <br>([:x:](https://arxiv.org/abs/2404.08252)), ([:book:](https://browse.arxiv.org/pdf/2404.08252.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.08252.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.08252)), ([:house:](https://huggingface.co/papers/2404.08252)), ([HTML](https://browse.arxiv.org/html/2404.08252v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.08252)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.08252v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.08252)), ([SS](https://api.semanticscholar.org/arXiv:2404.08252)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/monopatchnerf-improving-neural-radiance))
  * 04/12 - **Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length** <br>([:x:](https://arxiv.org/abs/2404.08801)), ([:book:](https://browse.arxiv.org/pdf/2404.08801.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.08801.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.08801)), ([:house:](https://huggingface.co/papers/2404.08801)), ([HTML](https://browse.arxiv.org/html/2404.08801v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.08801)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.08801v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.08801)), ([SS](https://api.semanticscholar.org/arXiv:2404.08801)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/megalodon-efficient-llm-pretraining-and)), ([:octocat:](https://github.com/xuezhemax/megalodon)![GitHub Repo stars](https://img.shields.io/github/stars/xuezhemax/megalodon?style=social))
  * 04/12 - **Is ChatGPT Transforming Academics' Writing Style?** <br>([:x:](https://arxiv.org/abs/2404.08627)), ([:book:](https://browse.arxiv.org/pdf/2404.08627.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.08627.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.08627)), ([:house:](https://huggingface.co/papers/2404.08627)), ([HTML](https://browse.arxiv.org/html/2404.08627v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.08627)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.08627v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.08627)), ([SS](https://api.semanticscholar.org/arXiv:2404.08627)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/is-chatgpt-transforming-academics-writing))
  * 04/12 - **COCONut: Modernizing COCO Segmentation** <br>([:x:](https://arxiv.org/abs/2404.08639)), ([:book:](https://browse.arxiv.org/pdf/2404.08639.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.08639.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.08639)), ([:house:](https://huggingface.co/papers/2404.08639)), ([HTML](https://browse.arxiv.org/html/2404.08639v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.08639)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.08639v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.08639)), ([SS](https://api.semanticscholar.org/arXiv:2404.08639)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/coconut-modernizing-coco-segmentation))
  * 04/12 - **AI Chip Trims Energy Budget Back by 99+ Percent** <br>  ([News](https://spectrum.ieee.org/optical-neural-network)), 
  * 04/12 - **AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control Guarantees** <br>([:x:](https://arxiv.org/abs/2404.08417)), ([:book:](https://browse.arxiv.org/pdf/2404.08417.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.08417.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.08417)), ([:house:](https://huggingface.co/papers/2404.08417)), ([HTML](https://browse.arxiv.org/html/2404.08417v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.08417)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.08417v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.08417)), ([SS](https://api.semanticscholar.org/arXiv:2404.08417)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/adapterswap-continuous-training-of-llms-with))
  * 04/12 - **Grok-1.5 Vision Preview** <br>  ([Demo](https://x.ai/blog/grok-1.5v)),  
  * 04/12 - **The good, the bad, and the Humane Pin** <br>  ([News](https://www.theverge.com/24128273/humane-ai-pin-review-taylor-swift-tiktok-openai-vergecast)), 
  * 04/12 - **Paid ChatGPT users can now access GPT-4 Turbo** <br>  ([twitter](https://twitter.com/OpenAI/status/1778574613813006610)),  ([News](https://www.engadget.com/paid-chatgpt-users-can-now-access-gpt-4-turbo-123031501.html)), , ([:octocat:](https://github.com/openai/simple-evals)![GitHub Repo stars](https://img.shields.io/github/stars/openai/simple-evals?style=social))
  * 04/11 - **The Necessity of AI Audit Standards Boards** <br>([:x:](https://arxiv.org/abs/2404.13060)), ([:book:](https://browse.arxiv.org/pdf/2404.13060.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.13060.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.13060)), ([:house:](https://huggingface.co/papers/2404.13060)), ([HTML](https://browse.arxiv.org/html/2404.13060v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.13060)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.13060v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.13060)), ([SS](https://api.semanticscholar.org/arXiv:2404.13060))
  * 04/11 - **Remembering Transformer for Continual Learning** <br>([:x:](https://arxiv.org/abs/2404.07518)), ([:book:](https://browse.arxiv.org/pdf/2404.07518.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07518.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07518)), ([:house:](https://huggingface.co/papers/2404.07518)), ([HTML](https://browse.arxiv.org/html/2404.07518v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07518)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07518v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07518)), ([SS](https://api.semanticscholar.org/arXiv:2404.07518)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/remembering-transformer-for-continual))
  * 04/11 - **Amazon adds Andrew Ng, a leading voice in artificial intelligence, to its board of directors** <br>  ([News](https://abcnews.go.com/Technology/wireStory/amazon-adds-andrew-ng-leading-voice-artificial-intelligence-109131975)), 
  * 04/11 - **Adobe Is Buying Videos for $3 Per Minute to Build AI Model** <br>  ([News](https://www.bloomberg.com/news/articles/2024-04-10/adobe-is-buying-video-clips-for-3-per-minute-to-build-ai-model)), 
  * 04/11 - **UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs** <br>([:x:](https://arxiv.org/abs/2404.07584)), ([:book:](https://browse.arxiv.org/pdf/2404.07584.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07584.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07584)), ([:house:](https://huggingface.co/papers/2404.07584)), ([HTML](https://browse.arxiv.org/html/2404.07584v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07584)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07584v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07584)), ([SS](https://api.semanticscholar.org/arXiv:2404.07584)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ultraeval-a-lightweight-platform-for-flexible)), ([:octocat:](https://github.com/openbmb/ultraeval)![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/ultraeval?style=social))
  * 04/11 - **Transferable and Principled Efficiency for Open-Vocabulary Segmentation** <br>([:x:](https://arxiv.org/abs/2404.07448)), ([:book:](https://browse.arxiv.org/pdf/2404.07448.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07448.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07448)), ([:house:](https://huggingface.co/papers/2404.07448)), ([HTML](https://browse.arxiv.org/html/2404.07448v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07448)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07448v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07448)), ([SS](https://api.semanticscholar.org/arXiv:2404.07448)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/transferable-and-principled-efficiency-for)), ([:octocat:](https://github.com/xujxyang/opentrans)![GitHub Repo stars](https://img.shields.io/github/stars/xujxyang/opentrans?style=social))
  * 04/11 - **SWE-agent** <br>  ([twitter](https://twitter.com/jyangballin/status/1775114444370051582)),  ([Demo](https://swe-agent.com/)), , ([:octocat:](https://github.com/princeton-nlp/SWE-agent)![GitHub Repo stars](https://img.shields.io/github/stars/princeton-nlp/SWE-agent?style=social))
  * 04/11 - **Sparse Laneformer** <br>([:x:](https://arxiv.org/abs/2404.07821)), ([:book:](https://browse.arxiv.org/pdf/2404.07821.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07821.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07821)), ([:house:](https://huggingface.co/papers/2404.07821)), ([HTML](https://browse.arxiv.org/html/2404.07821v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07821)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07821v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07821)), ([SS](https://api.semanticscholar.org/arXiv:2404.07821)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sparse-laneformer))
  * 04/11 - **Rho-1: Not All Tokens Are What You Need** <br>([:x:](https://arxiv.org/abs/2404.07965)), ([:book:](https://browse.arxiv.org/pdf/2404.07965.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07965.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07965)), ([:house:](https://huggingface.co/papers/2404.07965)), ([HTML](https://browse.arxiv.org/html/2404.07965v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07965)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07965v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07965)), ([SS](https://api.semanticscholar.org/arXiv:2404.07965)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rho-1-not-all-tokens-are-what-you-need)), ([:octocat:](https://github.com/microsoft/rho)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/rho?style=social))
  * 04/11 - **ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.07738)), ([:book:](https://browse.arxiv.org/pdf/2404.07738.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07738.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07738)), ([:house:](https://huggingface.co/papers/2404.07738)), ([HTML](https://browse.arxiv.org/html/2404.07738v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07738)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07738v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07738)), ([SS](https://api.semanticscholar.org/arXiv:2404.07738)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/researchagent-iterative-research-idea))
  * 04/11 - **RecurrentGemma: Moving Past Transformers for Efficient Open Language Models** <br>([:x:](https://arxiv.org/abs/2404.07839)), ([:book:](https://browse.arxiv.org/pdf/2404.07839.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07839.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07839)), ([:house:](https://huggingface.co/papers/2404.07839)), ([HTML](https://browse.arxiv.org/html/2404.07839v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07839)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07839v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07839)), ([SS](https://api.semanticscholar.org/arXiv:2404.07839)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/recurrentgemma-moving-past-transformers-for))
  * 04/11 - **OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments** <br>([:x:](https://arxiv.org/abs/2404.07972)), ([:book:](https://browse.arxiv.org/pdf/2404.07972.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07972.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07972)), ([:house:](https://huggingface.co/papers/2404.07972)), ([HTML](https://browse.arxiv.org/html/2404.07972v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07972)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07972v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07972)), ([SS](https://api.semanticscholar.org/arXiv:2404.07972)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/osworld-benchmarking-multimodal-agents-for))
  * 04/11 - **LLoCO: Learning Long Contexts Offline** <br>([:x:](https://arxiv.org/abs/2404.07979)), ([:book:](https://browse.arxiv.org/pdf/2404.07979.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07979.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07979)), ([:house:](https://huggingface.co/papers/2404.07979)), ([HTML](https://browse.arxiv.org/html/2404.07979v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07979)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07979v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07979)), ([SS](https://api.semanticscholar.org/arXiv:2404.07979)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lloco-learning-long-contexts-offline))
  * 04/11 - **Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation** <br>([:x:](https://arxiv.org/abs/2404.07926)), ([:book:](https://browse.arxiv.org/pdf/2404.07926.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07926.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07926)), ([:house:](https://huggingface.co/papers/2404.07926)), ([HTML](https://browse.arxiv.org/html/2404.07926v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07926)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07926v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07926)), ([SS](https://api.semanticscholar.org/arXiv:2404.07926)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/leveraging-large-language-models-llms-to-1))
  * 04/11 - **JetMoE: Reaching Llama2 Performance with 0.1M Dollars** <br>([:x:](https://arxiv.org/abs/2404.07413)), ([:book:](https://browse.arxiv.org/pdf/2404.07413.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07413.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07413)), ([:house:](https://huggingface.co/papers/2404.07413)), ([HTML](https://browse.arxiv.org/html/2404.07413v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07413)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07413v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07413)), ([SS](https://api.semanticscholar.org/arXiv:2404.07413)) ([Project](https://research.myshell.ai/jetmoe)),  ([twitter](https://twitter.com/omarsar0/status/1775971009469768104)), , ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/jetmoe-reaching-llama2-performance-with-0-1m)), ([:octocat:](https://github.com/myshell-ai/jetmoe)![GitHub Repo stars](https://img.shields.io/github/stars/myshell-ai/jetmoe?style=social))
  * 04/11 - **HGRN2: Gated Linear RNNs with State Expansion** <br>([:x:](https://arxiv.org/abs/2404.07904)), ([:book:](https://browse.arxiv.org/pdf/2404.07904.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07904.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07904)), ([:house:](https://huggingface.co/papers/2404.07904)), ([HTML](https://browse.arxiv.org/html/2404.07904v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07904)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07904v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07904)), ([SS](https://api.semanticscholar.org/arXiv:2404.07904)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hgrn2-gated-linear-rnns-with-state-expansion)), ([:octocat:](https://github.com/opennlplab/hgrn2)![GitHub Repo stars](https://img.shields.io/github/stars/opennlplab/hgrn2?style=social))
  * 04/11 - **From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples** <br>([:x:](https://arxiv.org/abs/2404.07544)), ([:book:](https://browse.arxiv.org/pdf/2404.07544.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07544.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07544)), ([:house:](https://huggingface.co/papers/2404.07544)), ([HTML](https://browse.arxiv.org/html/2404.07544v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07544)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07544v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07544)), ([SS](https://api.semanticscholar.org/arXiv:2404.07544)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/from-words-to-numbers-your-large-language))
  * 04/11 - **Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.07973)), ([:book:](https://browse.arxiv.org/pdf/2404.07973.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07973.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07973)), ([:house:](https://huggingface.co/papers/2404.07973)), ([HTML](https://browse.arxiv.org/html/2404.07973v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07973)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07973v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07973)), ([SS](https://api.semanticscholar.org/arXiv:2404.07973)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ferret-v2-an-improved-baseline-for-referring))
  * 04/11 - **ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback** <br>([:x:](https://arxiv.org/abs/2404.07987)), ([:book:](https://browse.arxiv.org/pdf/2404.07987.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07987.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07987)), ([:house:](https://huggingface.co/papers/2404.07987)), ([HTML](https://browse.arxiv.org/html/2404.07987v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07987)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07987v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07987)), ([SS](https://api.semanticscholar.org/arXiv:2404.07987)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/controlnet-improving-conditional-controls))
  * 04/11 - **Context-aware Video Anomaly Detection in Long-Term Datasets** <br>([:x:](https://arxiv.org/abs/2404.07887)), ([:book:](https://browse.arxiv.org/pdf/2404.07887.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07887.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07887)), ([:house:](https://huggingface.co/papers/2404.07887)), ([HTML](https://browse.arxiv.org/html/2404.07887v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07887)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07887v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07887)), ([SS](https://api.semanticscholar.org/arXiv:2404.07887)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/context-aware-video-anomaly-detection-in-long))
  * 04/11 - **ChatGPT-3.5, Claude 3 kick pixelated butt in Street Fighter III tournament for LLMs** <br>  ([News](https://www.theregister.com/2024/04/11/chatgpt_claude_street_fighter_3/)), 
  * 04/11 - **ChatGPT Can Predict the Future when it Tells Stories Set in the Future About the Past** <br>([:x:](https://arxiv.org/abs/2404.07396)), ([:book:](https://browse.arxiv.org/pdf/2404.07396.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07396.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07396)), ([:house:](https://huggingface.co/papers/2404.07396)), ([HTML](https://browse.arxiv.org/html/2404.07396v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07396)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07396v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07396)), ([SS](https://api.semanticscholar.org/arXiv:2404.07396)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatgpt-can-predict-the-future-when-it-tells))
  * 04/11 - **Best Practices and Lessons Learned on Synthetic Data for Language Models** <br>([:x:](https://arxiv.org/abs/2404.07503)), ([:book:](https://browse.arxiv.org/pdf/2404.07503.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07503.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07503)), ([:house:](https://huggingface.co/papers/2404.07503)), ([HTML](https://browse.arxiv.org/html/2404.07503v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07503)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07503v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07503)), ([SS](https://api.semanticscholar.org/arXiv:2404.07503)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/best-practices-and-lessons-learned-on))
  * 04/11 - **Benchmark LLMs by fighting in Street Fighter 3** <br>  ([Demo](https://huggingface.co/spaces/junior-labs/llm-colosseum)), , ([:octocat:](https://github.com/OpenGenerativeAI/llm-colosseum)![GitHub Repo stars](https://img.shields.io/github/stars/OpenGenerativeAI/llm-colosseum?style=social))
  * 04/11 - **Audio Dialogues: Dialogues dataset for audio and music understanding** <br>([:x:](https://arxiv.org/abs/2404.07616)), ([:book:](https://browse.arxiv.org/pdf/2404.07616.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07616.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07616)), ([:house:](https://huggingface.co/papers/2404.07616)), ([HTML](https://browse.arxiv.org/html/2404.07616v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07616)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07616v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07616)), ([SS](https://api.semanticscholar.org/arXiv:2404.07616)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/audio-dialogues-dialogues-dataset-for-audio))
  * 04/11 - **Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models** <br>([:x:](https://arxiv.org/abs/2404.07724)), ([:book:](https://browse.arxiv.org/pdf/2404.07724.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07724.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07724)), ([:house:](https://huggingface.co/papers/2404.07724)), ([HTML](https://browse.arxiv.org/html/2404.07724v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07724)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07724v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07724)), ([SS](https://api.semanticscholar.org/arXiv:2404.07724)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/applying-guidance-in-a-limited-interval))
  * 04/11 - **AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs** <br>([:x:](https://arxiv.org/abs/2404.07921)), ([:book:](https://browse.arxiv.org/pdf/2404.07921.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07921.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07921)), ([:house:](https://huggingface.co/papers/2404.07921)), ([HTML](https://browse.arxiv.org/html/2404.07921v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07921)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07921v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07921)), ([SS](https://api.semanticscholar.org/arXiv:2404.07921)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/amplegcg-learning-a-universal-and)), ([:octocat:](https://github.com/osu-nlp-group/amplegcg)![GitHub Repo stars](https://img.shields.io/github/stars/osu-nlp-group/amplegcg?style=social))
  * 04/10 - **LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models** <br>([:x:](https://arxiv.org/abs/2404.07004)), ([:book:](https://browse.arxiv.org/pdf/2404.07004.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07004.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07004)), ([:house:](https://huggingface.co/papers/2404.07004)), ([HTML](https://browse.arxiv.org/html/2404.07004v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07004)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07004v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07004)), ([SS](https://api.semanticscholar.org/arXiv:2404.07004)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lm-transparency-tool-interactive-tool-for))
  * 04/10 - **Gemini 1.5 Pro now understands audio** <br>  ([twitter](https://twitter.com/liambolling/status/1777758743637483562)), 
  * 04/10 - **Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?** <br>([:x:](https://arxiv.org/abs/2404.07066)), ([:book:](https://browse.arxiv.org/pdf/2404.07066.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07066.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07066)), ([:house:](https://huggingface.co/papers/2404.07066)), ([HTML](https://browse.arxiv.org/html/2404.07066v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07066)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07066v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07066)), ([SS](https://api.semanticscholar.org/arXiv:2404.07066)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-concept-depth-how-large-language)), ([:octocat:](https://github.com/luckfort/cd)![GitHub Repo stars](https://img.shields.io/github/stars/luckfort/cd?style=social))
  * 04/10 - **Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior** <br>([:x:](https://arxiv.org/abs/2404.06780)), ([:book:](https://browse.arxiv.org/pdf/2404.06780.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06780.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06780)), ([:house:](https://huggingface.co/papers/2404.06780)), ([HTML](https://browse.arxiv.org/html/2404.06780v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06780)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06780v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06780)), ([SS](https://api.semanticscholar.org/arXiv:2404.06780)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/urban-architect-steerable-3d-urban-scene))
  * 04/10 - **RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion** <br>([:x:](https://arxiv.org/abs/2404.07199)), ([:book:](https://browse.arxiv.org/pdf/2404.07199.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07199.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07199)), ([:house:](https://huggingface.co/papers/2404.07199)), ([HTML](https://browse.arxiv.org/html/2404.07199v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07199)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07199v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07199)), ([SS](https://api.semanticscholar.org/arXiv:2404.07199)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/realmdreamer-text-driven-3d-scene-generation))
  * 04/10 - **OpenAI and Meta are on the verge of releasing AI models capable of reasoning like humans, report says** <br>  ([News](https://www.businessinsider.com/openai-meta-agi-ai-models-reasoning-race-2024-4)), 
  * 04/10 - **MetaCheckGPT -- A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models** <br>([:x:](https://arxiv.org/abs/2404.06948)), ([:book:](https://browse.arxiv.org/pdf/2404.06948.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06948.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06948)), ([:house:](https://huggingface.co/papers/2404.06948)), ([HTML](https://browse.arxiv.org/html/2404.06948v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06948)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06948v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06948)), ([SS](https://api.semanticscholar.org/arXiv:2404.06948)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/metacheckgpt-a-multi-task-hallucination))
  * 04/10 - **Meta confirms that its Llama 3 open source LLM is coming in the next month** <br>  ([News](https://techcrunch.com/2024/04/09/meta-confirms-that-its-llama-3-open-source-llm-is-coming-in-the-next-month/)), 
  * 04/10 - **Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention** <br>([:x:](https://arxiv.org/abs/2404.07143)), ([:book:](https://browse.arxiv.org/pdf/2404.07143.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07143.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07143)), ([:house:](https://huggingface.co/papers/2404.07143)), ([HTML](https://browse.arxiv.org/html/2404.07143v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07143)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07143v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07143)), ([SS](https://api.semanticscholar.org/arXiv:2404.07143)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/leave-no-context-behind-efficient-infinite))
  * 04/10 - **Incremental XAI: Memorable Understanding of AI with Incremental Explanations** <br>([:x:](https://arxiv.org/abs/2404.06733)), ([:book:](https://browse.arxiv.org/pdf/2404.06733.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06733.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06733)), ([:house:](https://huggingface.co/papers/2404.06733)), ([HTML](https://browse.arxiv.org/html/2404.06733v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06733)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06733v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06733)), ([SS](https://api.semanticscholar.org/arXiv:2404.06733)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/incremental-xai-memorable-understanding-of-ai))
  * 04/10 - **DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting** <br>([:x:](https://arxiv.org/abs/2404.06903)), ([:book:](https://browse.arxiv.org/pdf/2404.06903.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06903.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06903)), ([:house:](https://huggingface.co/papers/2404.06903)), ([HTML](https://browse.arxiv.org/html/2404.06903v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06903)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06903v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06903)), ([SS](https://api.semanticscholar.org/arXiv:2404.06903)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamscene360-unconstrained-text-to-3d-scene))
  * 04/10 - **Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge** <br>([:x:](https://arxiv.org/abs/2404.06833)), ([:book:](https://browse.arxiv.org/pdf/2404.06833.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06833.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06833)), ([:house:](https://huggingface.co/papers/2404.06833)), ([HTML](https://browse.arxiv.org/html/2404.06833v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06833)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06833v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06833)), ([SS](https://api.semanticscholar.org/arXiv:2404.06833)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/does-mapo-tofu-contain-coffee-probing-llms))
  * 04/10 - **BRAVE: Broadening the visual encoding of vision-language models** <br>([:x:](https://arxiv.org/abs/2404.07204)), ([:book:](https://browse.arxiv.org/pdf/2404.07204.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.07204.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.07204)), ([:house:](https://huggingface.co/papers/2404.07204)), ([HTML](https://browse.arxiv.org/html/2404.07204v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.07204)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.07204v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.07204)), ([SS](https://api.semanticscholar.org/arXiv:2404.07204)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/brave-broadening-the-visual-encoding-of))
  * 04/10 - **AI startup Mistral launches a 281GB AI model to rival OpenAI, Meta, and Google** <br>  ([News](https://www.zdnet.com/article/ai-startup-mistral-launches-a-281gb-ai-model-to-rival-openai-meta-and-google/)), 
  * 04/10 - **Agent-driven Generative Semantic Communication for Remote Surveillance** <br>([:x:](https://arxiv.org/abs/2404.06997)), ([:book:](https://browse.arxiv.org/pdf/2404.06997.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06997.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06997)), ([:house:](https://huggingface.co/papers/2404.06997)), ([HTML](https://browse.arxiv.org/html/2404.06997v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06997)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06997v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06997)), ([SS](https://api.semanticscholar.org/arXiv:2404.06997)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agent-driven-generative-semantic))
  * 04/10 - **Adapting LLaMA Decoder to Vision Transformer** <br>([:x:](https://arxiv.org/abs/2404.06773)), ([:book:](https://browse.arxiv.org/pdf/2404.06773.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06773.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06773)), ([:house:](https://huggingface.co/papers/2404.06773)), ([HTML](https://browse.arxiv.org/html/2404.06773v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06773)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06773v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06773)), ([SS](https://api.semanticscholar.org/arXiv:2404.06773)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/adapting-llama-decoder-to-vision-transformer))
  * 04/10 - **A Survey on the Integration of Generative AI for Critical Thinking in Mobile Networks** <br>([:x:](https://arxiv.org/abs/2404.06946)), ([:book:](https://browse.arxiv.org/pdf/2404.06946.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06946.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06946)), ([:house:](https://huggingface.co/papers/2404.06946)), ([HTML](https://browse.arxiv.org/html/2404.06946v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06946)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06946v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06946)), ([SS](https://api.semanticscholar.org/arXiv:2404.06946)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-the-integration-of-generative-ai))
  * 04/09 - **Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak** <br>([:x:](https://arxiv.org/abs/2404.06407)), ([:book:](https://browse.arxiv.org/pdf/2404.06407.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06407.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06407)), ([:house:](https://huggingface.co/papers/2404.06407)), ([HTML](https://browse.arxiv.org/html/2404.06407v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06407)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06407v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06407)), ([SS](https://api.semanticscholar.org/arXiv:2404.06407)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/take-a-look-at-it-rethinking-how-to-evaluate)), ([:octocat:](https://github.com/controllability/jailbreak-evaluation)![GitHub Repo stars](https://img.shields.io/github/stars/controllability/jailbreak-evaluation?style=social))
  * 04/09 - **RULER: What's the Real Context Size of Your Long-Context Language Models?** <br>([:x:](https://arxiv.org/abs/2404.06654)), ([:book:](https://browse.arxiv.org/pdf/2404.06654.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06654.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06654)), ([:house:](https://huggingface.co/papers/2404.06654)), ([HTML](https://browse.arxiv.org/html/2404.06654v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06654)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06654v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06654)), ([SS](https://api.semanticscholar.org/arXiv:2404.06654)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ruler-what-s-the-real-context-size-of-your)), ([:octocat:](https://github.com/hsiehjackson/ruler)![GitHub Repo stars](https://img.shields.io/github/stars/hsiehjackson/ruler?style=social))
  * 04/09 - **Revising Densification in Gaussian Splatting** <br>([:x:](https://arxiv.org/abs/2404.06109)), ([:book:](https://browse.arxiv.org/pdf/2404.06109.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06109.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06109)), ([:house:](https://huggingface.co/papers/2404.06109)), ([HTML](https://browse.arxiv.org/html/2404.06109v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06109)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06109v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06109)), ([SS](https://api.semanticscholar.org/arXiv:2404.06109)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/revising-densification-in-gaussian-splatting))
  * 04/09 - **Reconstructing Hand-Held Objects in 3D** <br>([:x:](https://arxiv.org/abs/2404.06507)), ([:book:](https://browse.arxiv.org/pdf/2404.06507.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06507.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06507)), ([:house:](https://huggingface.co/papers/2404.06507)), ([HTML](https://browse.arxiv.org/html/2404.06507v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06507)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06507v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06507)), ([SS](https://api.semanticscholar.org/arXiv:2404.06507)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reconstructing-hand-held-objects-in-3d))
  * 04/09 - **RAR-b: Reasoning as Retrieval Benchmark** <br>([:x:](https://arxiv.org/abs/2404.06347)), ([:book:](https://browse.arxiv.org/pdf/2404.06347.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06347.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06347)), ([:house:](https://huggingface.co/papers/2404.06347)), ([HTML](https://browse.arxiv.org/html/2404.06347v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06347)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06347v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06347)), ([SS](https://api.semanticscholar.org/arXiv:2404.06347)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rar-b-reasoning-as-retrieval-benchmark))
  * 04/09 - **Privacy Preserving Prompt Engineering: A Survey** <br>([:x:](https://arxiv.org/abs/2404.06001)), ([:book:](https://browse.arxiv.org/pdf/2404.06001.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06001.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06001)), ([:house:](https://huggingface.co/papers/2404.06001)), ([HTML](https://browse.arxiv.org/html/2404.06001v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06001)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06001v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06001)), ([SS](https://api.semanticscholar.org/arXiv:2404.06001)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/privacy-preserving-prompt-engineering-a))
  * 04/09 - **On Evaluating the Efficiency of Source Code Generated by LLMs** <br>([:x:](https://arxiv.org/abs/2404.06041)), ([:book:](https://browse.arxiv.org/pdf/2404.06041.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06041.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06041)), ([:house:](https://huggingface.co/papers/2404.06041)), ([HTML](https://browse.arxiv.org/html/2404.06041v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06041)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06041v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06041)), ([SS](https://api.semanticscholar.org/arXiv:2404.06041))
  * 04/09 - **OmniFusion Technical Report** <br>([:x:](https://arxiv.org/abs/2404.06212)), ([:book:](https://browse.arxiv.org/pdf/2404.06212.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06212.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06212)), ([:house:](https://huggingface.co/papers/2404.06212)), ([HTML](https://browse.arxiv.org/html/2404.06212v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06212)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06212v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06212)), ([SS](https://api.semanticscholar.org/arXiv:2404.06212)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/omnifusion-technical-report)), ([:octocat:](https://github.com/airi-institute/omnifusion)![GitHub Repo stars](https://img.shields.io/github/stars/airi-institute/omnifusion?style=social))
  * 04/09 - **MuPT: A Generative Symbolic Music Pretrained Transformer** <br>([:x:](https://arxiv.org/abs/2404.06393)), ([:book:](https://browse.arxiv.org/pdf/2404.06393.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06393.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06393)), ([:house:](https://huggingface.co/papers/2404.06393)), ([HTML](https://browse.arxiv.org/html/2404.06393v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06393)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06393v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06393)), ([SS](https://api.semanticscholar.org/arXiv:2404.06393)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mupt-a-generative-symbolic-music-pretrained))
  * 04/09 - **MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies** <br>([:x:](https://arxiv.org/abs/2404.06395)), ([:book:](https://browse.arxiv.org/pdf/2404.06395.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06395.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06395)), ([:house:](https://huggingface.co/papers/2404.06395)), ([HTML](https://browse.arxiv.org/html/2404.06395v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06395)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06395v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06395)), ([SS](https://api.semanticscholar.org/arXiv:2404.06395)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/minicpm-unveiling-the-potential-of-small)), ([:octocat:](https://github.com/openbmb/minicpm)![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/minicpm?style=social))
  * 04/09 - **Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion** <br>([:x:](https://arxiv.org/abs/2404.06429)), ([:book:](https://browse.arxiv.org/pdf/2404.06429.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06429.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06429)), ([:house:](https://huggingface.co/papers/2404.06429)), ([HTML](https://browse.arxiv.org/html/2404.06429v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06429)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06429v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06429)), ([SS](https://api.semanticscholar.org/arXiv:2404.06429)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magic-boost-boost-3d-generation-with-mutli))
  * 04/09 - **LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders** <br>([:x:](https://arxiv.org/abs/2404.05961)), ([:book:](https://browse.arxiv.org/pdf/2404.05961.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05961.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05961)), ([:house:](https://huggingface.co/papers/2404.05961)), ([HTML](https://browse.arxiv.org/html/2404.05961v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05961)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05961v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05961)), ([SS](https://api.semanticscholar.org/arXiv:2404.05961)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm2vec-large-language-models-are-secretly)), ([:octocat:](https://github.com/mcgill-nlp/llm2vec)![GitHub Repo stars](https://img.shields.io/github/stars/mcgill-nlp/llm2vec?style=social))
  * 04/09 - **InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD** <br>([:x:](https://arxiv.org/abs/2404.06512)), ([:book:](https://browse.arxiv.org/pdf/2404.06512.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06512.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06512)), ([:house:](https://huggingface.co/papers/2404.06512)), ([HTML](https://browse.arxiv.org/html/2404.06512v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06512)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06512v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06512)), ([SS](https://api.semanticscholar.org/arXiv:2404.06512)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/internlm-xcomposer2-4khd-a-pioneering-large)), ([:octocat:](https://github.com/internlm/internlm-xcomposer)![GitHub Repo stars](https://img.shields.io/github/stars/internlm/internlm-xcomposer?style=social))
  * 04/09 - **Hash3D: Training-free Acceleration for 3D Generation** <br>([:x:](https://arxiv.org/abs/2404.06091)), ([:book:](https://browse.arxiv.org/pdf/2404.06091.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06091.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06091)), ([:house:](https://huggingface.co/papers/2404.06091)), ([HTML](https://browse.arxiv.org/html/2404.06091v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06091)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06091v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06091)), ([SS](https://api.semanticscholar.org/arXiv:2404.06091)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hash3d-training-free-acceleration-for-3d)), ([:octocat:](https://github.com/Adamdad/hash3D)![GitHub Repo stars](https://img.shields.io/github/stars/Adamdad/hash3D?style=social))
  * 04/09 - **Google unveils open source projects for generative AI** <br>  ([News](https://www.infoworld.com/article/3715123/google-unveils-open-source-projects-for-generative-ai.html)), 
  * 04/09 - **Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.06209)), ([:book:](https://browse.arxiv.org/pdf/2404.06209.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.06209.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.06209)), ([:house:](https://huggingface.co/papers/2404.06209)), ([HTML](https://browse.arxiv.org/html/2404.06209v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.06209)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.06209v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.06209)), ([SS](https://api.semanticscholar.org/arXiv:2404.06209)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/elephants-never-forget-memorization-and)), ([:octocat:](https://github.com/interpretml/llm-tabular-memorization-checker)![GitHub Repo stars](https://img.shields.io/github/stars/interpretml/llm-tabular-memorization-checker?style=social))
  * 04/09 - **Apple just unveiled new Ferret-UI LLM — this AI can read your iPhone screen** <br>  ([News](https://www.tomsguide.com/ai/apple-just-unveiled-new-ferret-ui-llm-this-ai-can-read-your-iphone-screen)), 
  * 04/09 - **AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts** <br>([:x:](https://arxiv.org/abs/2404.05993)), ([:book:](https://browse.arxiv.org/pdf/2404.05993.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05993.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05993)), ([:house:](https://huggingface.co/papers/2404.05993)), ([HTML](https://browse.arxiv.org/html/2404.05993v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05993)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05993v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05993)), ([SS](https://api.semanticscholar.org/arXiv:2404.05993)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aegis-online-adaptive-ai-content-safety))
  * 04/08 - **YaART: Yet Another ART Rendering Technology** <br>([:x:](https://arxiv.org/abs/2404.05666)), ([:book:](https://browse.arxiv.org/pdf/2404.05666.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05666.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05666)), ([:house:](https://huggingface.co/papers/2404.05666)), ([HTML](https://browse.arxiv.org/html/2404.05666v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05666)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05666v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05666)), ([SS](https://api.semanticscholar.org/arXiv:2404.05666)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/yaart-yet-another-art-rendering-technology))
  * 04/08 - **WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents** <br>([:x:](https://arxiv.org/abs/2404.05902)), ([:book:](https://browse.arxiv.org/pdf/2404.05902.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05902.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05902)), ([:house:](https://huggingface.co/papers/2404.05902)), ([HTML](https://browse.arxiv.org/html/2404.05902v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05902)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05902v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05902)), ([SS](https://api.semanticscholar.org/arXiv:2404.05902)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wilbur-adaptive-in-context-learning-for))
  * 04/08 - **UniFL: Improve Stable Diffusion via Unified Feedback Learning** <br>([:x:](https://arxiv.org/abs/2404.05595)), ([:book:](https://browse.arxiv.org/pdf/2404.05595.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05595.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05595)), ([:house:](https://huggingface.co/papers/2404.05595)), ([HTML](https://browse.arxiv.org/html/2404.05595v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05595)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05595v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05595)), ([SS](https://api.semanticscholar.org/arXiv:2404.05595)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unifl-improve-stable-diffusion-via-unified))
  * 04/08 - **Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security** <br>([:x:](https://arxiv.org/abs/2404.05264)), ([:book:](https://browse.arxiv.org/pdf/2404.05264.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05264.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05264)), ([:house:](https://huggingface.co/papers/2404.05264)), ([HTML](https://browse.arxiv.org/html/2404.05264v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05264)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05264v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05264)), ([SS](https://api.semanticscholar.org/arXiv:2404.05264)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unbridled-icarus-a-survey-of-the-potential))
  * 04/08 - **The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.05904)), ([:book:](https://browse.arxiv.org/pdf/2404.05904.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05904.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05904)), ([:house:](https://huggingface.co/papers/2404.05904)), ([HTML](https://browse.arxiv.org/html/2404.05904v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05904)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05904v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05904)), ([SS](https://api.semanticscholar.org/arXiv:2404.05904)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-hallucinations-leaderboard-an-open-effort))
  * 04/08 - **The Fact Selection Problem in LLM-Based Program Repair** <br>([:x:](https://arxiv.org/abs/2404.05520)), ([:book:](https://browse.arxiv.org/pdf/2404.05520.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05520.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05520)), ([:house:](https://huggingface.co/papers/2404.05520)), ([HTML](https://browse.arxiv.org/html/2404.05520v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05520)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05520v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05520)), ([SS](https://api.semanticscholar.org/arXiv:2404.05520)), ([:eight_spoked_asterisk:](https://cs.paperswithcode.com/paper/the-fact-selection-problem-in-llm-based)), ([:octocat:](https://github.com/pyrepair/maniple)![GitHub Repo stars](https://img.shields.io/github/stars/pyrepair/maniple?style=social))
  * 04/08 - **SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing** <br>([:x:](https://arxiv.org/abs/2404.05717)), ([:book:](https://browse.arxiv.org/pdf/2404.05717.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05717.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05717)), ([:house:](https://huggingface.co/papers/2404.05717)), ([HTML](https://browse.arxiv.org/html/2404.05717v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05717)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05717v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05717)), ([SS](https://api.semanticscholar.org/arXiv:2404.05717)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/swapanything-enabling-arbitrary-object))
  * 04/08 - **SambaLingo: Teaching Large Language Models New Languages** <br>([:x:](https://arxiv.org/abs/2404.05829)), ([:book:](https://browse.arxiv.org/pdf/2404.05829.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05829.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05829)), ([:house:](https://huggingface.co/papers/2404.05829)), ([HTML](https://browse.arxiv.org/html/2404.05829v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05829)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05829v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05829)), ([SS](https://api.semanticscholar.org/arXiv:2404.05829)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sambalingo-teaching-large-language-models-new))
  * 04/08 - **Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning** <br>([:x:](https://arxiv.org/abs/2404.05868)), ([:book:](https://browse.arxiv.org/pdf/2404.05868.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05868.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05868)), ([:house:](https://huggingface.co/papers/2404.05868)), ([HTML](https://browse.arxiv.org/html/2404.05868v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05868)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05868v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05868)), ([SS](https://api.semanticscholar.org/arXiv:2404.05868)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/negative-preference-optimization-from))
  * 04/08 - **Naver debuts multilingual HyperCLOVA X LLM it will use to build sovereign AI for Asia** <br>  ([News](https://www.theregister.com/2024/04/08/naver_cloud_hyperclova_llm_sovereign_ai/)), 
  * 04/08 - **MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation** <br>([:x:](https://arxiv.org/abs/2404.05674)), ([:book:](https://browse.arxiv.org/pdf/2404.05674.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05674.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05674)), ([:house:](https://huggingface.co/papers/2404.05674)), ([HTML](https://browse.arxiv.org/html/2404.05674v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05674)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05674v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05674)), ([SS](https://api.semanticscholar.org/arXiv:2404.05674)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/moma-multimodal-llm-adapter-for-fast))
  * 04/08 - **MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question Answering** <br>([:x:](https://arxiv.org/abs/2404.05590)), ([:book:](https://browse.arxiv.org/pdf/2404.05590.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05590.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05590)), ([:house:](https://huggingface.co/papers/2404.05590)), ([HTML](https://browse.arxiv.org/html/2404.05590v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05590)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05590v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05590)), ([SS](https://api.semanticscholar.org/arXiv:2404.05590)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/medexpqa-multilingual-benchmarking-of-large))
  * 04/08 - **MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding** <br>([:x:](https://arxiv.org/abs/2404.05726)), ([:book:](https://browse.arxiv.org/pdf/2404.05726.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05726.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05726)), ([:house:](https://huggingface.co/papers/2404.05726)), ([HTML](https://browse.arxiv.org/html/2404.05726v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05726)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05726v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05726)), ([SS](https://api.semanticscholar.org/arXiv:2404.05726)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ma-lmm-memory-augmented-large-multimodal))
  * 04/08 - **LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding** <br>([:x:](https://arxiv.org/abs/2404.05225)), ([:book:](https://browse.arxiv.org/pdf/2404.05225.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05225.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05225)), ([:house:](https://huggingface.co/papers/2404.05225)), ([HTML](https://browse.arxiv.org/html/2404.05225v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05225)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05225v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05225)), ([SS](https://api.semanticscholar.org/arXiv:2404.05225)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/layoutllm-layout-instruction-tuning-with)), ([:octocat:](https://github.com/alibabaresearch/advancedliteratemachinery)![GitHub Repo stars](https://img.shields.io/github/stars/alibabaresearch/advancedliteratemachinery?style=social))
  * 04/08 - **Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs** <br>([:x:](https://arxiv.org/abs/2404.05719)), ([:book:](https://browse.arxiv.org/pdf/2404.05719.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05719.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05719)), ([:house:](https://huggingface.co/papers/2404.05719)), ([HTML](https://browse.arxiv.org/html/2404.05719v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05719)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05719v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05719)), ([SS](https://api.semanticscholar.org/arXiv:2404.05719)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ferret-ui-grounded-mobile-ui-understanding))
  * 04/08 - **Evaluating Interventional Reasoning Capabilities of Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.05545)), ([:book:](https://browse.arxiv.org/pdf/2404.05545.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05545.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05545)), ([:house:](https://huggingface.co/papers/2404.05545)), ([HTML](https://browse.arxiv.org/html/2404.05545v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05545)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05545v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05545)), ([SS](https://api.semanticscholar.org/arXiv:2404.05545)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-interventional-reasoning))
  * 04/08 - **Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence** <br>([:x:](https://arxiv.org/abs/2404.05892)), ([:book:](https://browse.arxiv.org/pdf/2404.05892.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05892.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05892)), ([:house:](https://huggingface.co/papers/2404.05892)), ([HTML](https://browse.arxiv.org/html/2404.05892v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05892)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05892v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05892)), ([SS](https://api.semanticscholar.org/arXiv:2404.05892)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/eagle-and-finch-rwkv-with-matrix-valued)), ([:octocat:](https://github.com/rwkv/rwkv-infctx-trainer)![GitHub Repo stars](https://img.shields.io/github/stars/rwkv/rwkv-infctx-trainer?style=social))
  * 04/08 - **CodecLM: Aligning Language Models with Tailored Synthetic Data** <br>([:x:](https://arxiv.org/abs/2404.05875)), ([:book:](https://browse.arxiv.org/pdf/2404.05875.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05875.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05875)), ([:house:](https://huggingface.co/papers/2404.05875)), ([HTML](https://browse.arxiv.org/html/2404.05875v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05875)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05875v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05875)), ([SS](https://api.semanticscholar.org/arXiv:2404.05875)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/codeclm-aligning-language-models-with))
  * 04/08 - **AutoCodeRover: Autonomous Program Improvement** <br>([:x:](https://arxiv.org/abs/2404.05427)), ([:book:](https://browse.arxiv.org/pdf/2404.05427.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05427.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05427)), ([:house:](https://huggingface.co/papers/2404.05427)), ([HTML](https://browse.arxiv.org/html/2404.05427v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05427)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05427v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05427)), ([SS](https://api.semanticscholar.org/arXiv:2404.05427)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/autocoderover-autonomous-program-improvement)), ([:octocat:](https://github.com/nus-apr/auto-code-rover)![GitHub Repo stars](https://img.shields.io/github/stars/nus-apr/auto-code-rover?style=social))
  * 04/07 - **TimeGPT in Load Forecasting: A Large Time Series Model Perspective** <br>([:x:](https://arxiv.org/abs/2404.04885)), ([:book:](https://browse.arxiv.org/pdf/2404.04885.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04885.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04885)), ([:house:](https://huggingface.co/papers/2404.04885)), ([HTML](https://browse.arxiv.org/html/2404.04885v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04885)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04885v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04885)), ([SS](https://api.semanticscholar.org/arXiv:2404.04885)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/timegpt-in-load-forecasting-a-large-time))
  * 04/07 - **OpenAI transcribed over a million hours of YouTube videos to train GPT-4** <br>  ([News](https://www.theverge.com/2024/4/6/24122915/openai-youtube-transcripts-gpt-4-training-data-google)), 
  * 04/07 - **MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators** <br>([:x:](https://arxiv.org/abs/2404.05014)), ([:book:](https://browse.arxiv.org/pdf/2404.05014.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.05014.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.05014)), ([:house:](https://huggingface.co/papers/2404.05014)), ([HTML](https://browse.arxiv.org/html/2404.05014v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.05014)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.05014v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.05014)), ([SS](https://api.semanticscholar.org/arXiv:2404.05014)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magictime-time-lapse-video-generation-models)), ([:octocat:](https://github.com/pku-yuangroup/magictime)![GitHub Repo stars](https://img.shields.io/github/stars/pku-yuangroup/magictime?style=social))
  * 04/07 - **ByteEdit: Boost, Comply and Accelerate Generative Image Editing** <br>([:x:](https://arxiv.org/abs/2404.04860)), ([:book:](https://browse.arxiv.org/pdf/2404.04860.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04860.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04860)), ([:house:](https://huggingface.co/papers/2404.04860)), ([HTML](https://browse.arxiv.org/html/2404.04860v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04860)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04860v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04860)), ([SS](https://api.semanticscholar.org/arXiv:2404.04860)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/byteedit-boost-comply-and-accelerate))
  * 04/06 - **Majority Voting of Doctors Improves Appropriateness of AI Reliance in Pathology** <br>([:x:](https://arxiv.org/abs/2404.04485)), ([:book:](https://browse.arxiv.org/pdf/2404.04485.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04485.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04485)), ([:house:](https://huggingface.co/papers/2404.04485)), ([HTML](https://browse.arxiv.org/html/2404.04485v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04485)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04485v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04485)), ([SS](https://api.semanticscholar.org/arXiv:2404.04485))
  * 04/06 - **Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models** <br>([:x:](https://arxiv.org/abs/2404.04478)), ([:book:](https://browse.arxiv.org/pdf/2404.04478.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04478.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04478)), ([:house:](https://huggingface.co/papers/2404.04478)), ([HTML](https://browse.arxiv.org/html/2404.04478v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04478)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04478v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04478)), ([SS](https://api.semanticscholar.org/arXiv:2404.04478)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusion-rwkv-scaling-rwkv-like)), ([:octocat:](https://github.com/feizc/diffusion-rwkv)![GitHub Repo stars](https://img.shields.io/github/stars/feizc/diffusion-rwkv?style=social))
  * 04/06 - **DATENeRF: Depth-Aware Text-based Editing of NeRFs** <br>([:x:](https://arxiv.org/abs/2404.04526)), ([:book:](https://browse.arxiv.org/pdf/2404.04526.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04526.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04526)), ([:house:](https://huggingface.co/papers/2404.04526)), ([HTML](https://browse.arxiv.org/html/2404.04526v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04526)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04526v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04526)), ([SS](https://api.semanticscholar.org/arXiv:2404.04526)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/datenerf-depth-aware-text-based-editing-of))
  * 04/06 - **BeyondScene: Higher-Resolution Human-Centric Scene Generation With Pretrained Diffusion** <br>([:x:](https://arxiv.org/abs/2404.04544)), ([:book:](https://browse.arxiv.org/pdf/2404.04544.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04544.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04544)), ([:house:](https://huggingface.co/papers/2404.04544)), ([HTML](https://browse.arxiv.org/html/2404.04544v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04544)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04544v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04544)), ([SS](https://api.semanticscholar.org/arXiv:2404.04544)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/beyondscene-higher-resolution-human-centric))
  * 04/06 - **Aligning Diffusion Models by Optimizing Human Utility** <br>([:x:](https://arxiv.org/abs/2404.04465)), ([:book:](https://browse.arxiv.org/pdf/2404.04465.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04465.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04465)), ([:house:](https://huggingface.co/papers/2404.04465)), ([HTML](https://browse.arxiv.org/html/2404.04465v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04465)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04465v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04465)), ([SS](https://api.semanticscholar.org/arXiv:2404.04465)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aligning-diffusion-models-by-optimizing-human))
  * 04/06 - **The Case for Developing a Foundation Model for Planning-like Tasks from Scratch** <br>([:x:](https://arxiv.org/abs/2404.04540)), ([:book:](https://browse.arxiv.org/pdf/2404.04540.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04540.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04540)), ([:house:](https://huggingface.co/papers/2404.04540)), ([HTML](https://browse.arxiv.org/html/2404.04540v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04540)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04540v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04540)), ([SS](https://api.semanticscholar.org/arXiv:2404.04540)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-case-for-developing-a-foundation-model))
  * 04/05 - **Increased LLM Vulnerabilities from Fine-tuning and Quantization** <br>([:x:](https://arxiv.org/abs/2404.04392)), ([:book:](https://browse.arxiv.org/pdf/2404.04392.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04392.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04392)), ([:house:](https://huggingface.co/papers/2404.04392)), ([HTML](https://browse.arxiv.org/html/2404.04392v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04392)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04392v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04392)), ([SS](https://api.semanticscholar.org/arXiv:2404.04392)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/increased-llm-vulnerabilities-from-fine))
  * 04/05 - **SpatialTracker: Tracking Any 2D Pixels in 3D Space** <br>([:x:](https://arxiv.org/abs/2404.04319)), ([:book:](https://browse.arxiv.org/pdf/2404.04319.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04319.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04319)), ([:house:](https://huggingface.co/papers/2404.04319)), ([HTML](https://browse.arxiv.org/html/2404.04319v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04319)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04319v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04319)), ([SS](https://api.semanticscholar.org/arXiv:2404.04319)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/spatialtracker-tracking-any-2d-pixels-in-3d))
  * 04/05 - **Social Skill Training with Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.04204)), ([:book:](https://browse.arxiv.org/pdf/2404.04204.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04204.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04204)), ([:house:](https://huggingface.co/papers/2404.04204)), ([HTML](https://browse.arxiv.org/html/2404.04204v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04204)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04204v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04204)), ([SS](https://api.semanticscholar.org/arXiv:2404.04204)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/social-skill-training-with-large-language))
  * 04/05 - **Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation** <br>([:x:](https://arxiv.org/abs/2404.04256)), ([:book:](https://browse.arxiv.org/pdf/2404.04256.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04256.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04256)), ([:house:](https://huggingface.co/papers/2404.04256)), ([HTML](https://browse.arxiv.org/html/2404.04256v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04256)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04256v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04256)), ([SS](https://api.semanticscholar.org/arXiv:2404.04256)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sigma-siamese-mamba-network-for-multi-modal)), ([:octocat:](https://github.com/zifuwan/sigma)![GitHub Repo stars](https://img.shields.io/github/stars/zifuwan/sigma?style=social))
  * 04/05 - **Robust Gaussian Splatting** <br>([:x:](https://arxiv.org/abs/2404.04211)), ([:book:](https://browse.arxiv.org/pdf/2404.04211.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04211.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04211)), ([:house:](https://huggingface.co/papers/2404.04211)), ([HTML](https://browse.arxiv.org/html/2404.04211v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04211)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04211v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04211)), ([SS](https://api.semanticscholar.org/arXiv:2404.04211)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/robust-gaussian-splatting))
  * 04/05 - **PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations** <br>([:x:](https://arxiv.org/abs/2404.04421)), ([:book:](https://browse.arxiv.org/pdf/2404.04421.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04421.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04421)), ([:house:](https://huggingface.co/papers/2404.04421)), ([HTML](https://browse.arxiv.org/html/2404.04421v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04421)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04421v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04421)), ([SS](https://api.semanticscholar.org/arXiv:2404.04421)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/physavatar-learning-the-physics-of-dressed-3d))
  * 04/05 - **Koala: Key frame-conditioned long video-LLM** <br>([:x:](https://arxiv.org/abs/2404.04346)), ([:book:](https://browse.arxiv.org/pdf/2404.04346.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04346.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04346)), ([:house:](https://huggingface.co/papers/2404.04346)), ([HTML](https://browse.arxiv.org/html/2404.04346v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04346)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04346v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04346)), ([SS](https://api.semanticscholar.org/arXiv:2404.04346)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/koala-key-frame-conditioned-long-video-llm))
  * 04/05 - **CLUE: A Clinical Language Understanding Evaluation for LLMs** <br>([:x:](https://arxiv.org/abs/2404.04067)), ([:book:](https://browse.arxiv.org/pdf/2404.04067.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04067.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04067)), ([:house:](https://huggingface.co/papers/2404.04067)), ([HTML](https://browse.arxiv.org/html/2404.04067v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04067)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04067v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04067)), ([SS](https://api.semanticscholar.org/arXiv:2404.04067)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clue-a-clinical-language-understanding))
  * 04/05 - **Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model** <br>([:x:](https://arxiv.org/abs/2404.04167)), ([:book:](https://browse.arxiv.org/pdf/2404.04167.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04167.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04167)), ([:house:](https://huggingface.co/papers/2404.04167)), ([HTML](https://browse.arxiv.org/html/2404.04167v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04167)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04167v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04167)), ([SS](https://api.semanticscholar.org/arXiv:2404.04167)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chinese-tiny-llm-pretraining-a-chinese))
  * 04/05 - **Assisting humans in complex comparisons: automated information comparison at scale** <br>([:x:](https://arxiv.org/abs/2404.04351)), ([:book:](https://browse.arxiv.org/pdf/2404.04351.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04351.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04351)), ([:house:](https://huggingface.co/papers/2404.04351)), ([HTML](https://browse.arxiv.org/html/2404.04351v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04351)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04351v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04351)), ([SS](https://api.semanticscholar.org/arXiv:2404.04351)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/assisting-humans-in-complex-comparisons))
  * 04/04 - **Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity** <br>([:x:](https://arxiv.org/abs/2404.03570)), ([:book:](https://browse.arxiv.org/pdf/2404.03570.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03570.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03570)), ([:house:](https://huggingface.co/papers/2404.03570)), ([HTML](https://browse.arxiv.org/html/2404.03570v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03570)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03570v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03570)), ([SS](https://api.semanticscholar.org/arXiv:2404.03570))
  * 04/04 - **Language Model Evolution: An Iterated Learning Perspective** <br>([:x:](https://arxiv.org/abs/2404.04286)), ([:book:](https://browse.arxiv.org/pdf/2404.04286.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04286.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04286)), ([:house:](https://huggingface.co/papers/2404.04286)), ([HTML](https://browse.arxiv.org/html/2404.04286v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04286)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04286v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04286)), ([SS](https://api.semanticscholar.org/arXiv:2404.04286)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-model-evolution-an-iterated-learning))
  * 04/04 - **Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.03622)), ([:book:](https://browse.arxiv.org/pdf/2404.03622.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03622.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03622)), ([:house:](https://huggingface.co/papers/2404.03622)), ([HTML](https://browse.arxiv.org/html/2404.03622v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03622)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03622v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03622)), ([SS](https://api.semanticscholar.org/arXiv:2404.03622)) ([twitter](https://twitter.com/omarsar0/status/1776082343813403063)), 
  * 04/04 - **No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance** <br>([:x:](https://arxiv.org/abs/2404.04125)), ([:book:](https://browse.arxiv.org/pdf/2404.04125.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04125.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04125)), ([:house:](https://huggingface.co/papers/2404.04125)), ([HTML](https://browse.arxiv.org/html/2404.04125v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04125)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04125v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04125)), ([SS](https://api.semanticscholar.org/arXiv:2404.04125)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/no-zero-shot-without-exponential-data)), ([:octocat:](https://github.com/bethgelab/frequency_determines_performance)![GitHub Repo stars](https://img.shields.io/github/stars/bethgelab/frequency_determines_performance?style=social))
  * 04/04 - **Evaluating LLMs at Detecting Errors in LLM Responses** <br>([:x:](https://arxiv.org/abs/2404.03602)), ([:book:](https://browse.arxiv.org/pdf/2404.03602.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03602.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03602)), ([:house:](https://huggingface.co/papers/2404.03602)), ([HTML](https://browse.arxiv.org/html/2404.03602v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03602)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03602v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03602)), ([SS](https://api.semanticscholar.org/arXiv:2404.03602)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-llms-at-detecting-errors-in-llm)), ([:octocat:](https://github.com/psunlpgroup/realmistake)![GitHub Repo stars](https://img.shields.io/github/stars/psunlpgroup/realmistake?style=social))
  * 04/04 - **Evaluating Generative Language Models in Information Extraction as Subjective Question Correction** <br>([:x:](https://arxiv.org/abs/2404.03532)), ([:book:](https://browse.arxiv.org/pdf/2404.03532.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03532.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03532)), ([:house:](https://huggingface.co/papers/2404.03532)), ([HTML](https://browse.arxiv.org/html/2404.03532v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03532)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03532v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03532)), ([SS](https://api.semanticscholar.org/arXiv:2404.03532)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-generative-language-models-in)), ([:octocat:](https://github.com/thu-keg/sqc-score)![GitHub Repo stars](https://img.shields.io/github/stars/thu-keg/sqc-score?style=social))
  * 04/04 - **Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences** <br>([:x:](https://arxiv.org/abs/2404.03715)), ([:book:](https://browse.arxiv.org/pdf/2404.03715.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03715.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03715)), ([:house:](https://huggingface.co/papers/2404.03715)), ([HTML](https://browse.arxiv.org/html/2404.03715v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03715)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03715v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03715)), ([SS](https://api.semanticscholar.org/arXiv:2404.03715)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/direct-nash-optimization-teaching-language))
  * 04/04 - **CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering** <br>([:x:](https://arxiv.org/abs/2404.04302)), ([:book:](https://browse.arxiv.org/pdf/2404.04302.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.04302.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.04302)), ([:house:](https://huggingface.co/papers/2404.04302)), ([HTML](https://browse.arxiv.org/html/2404.04302v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.04302)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.04302v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04302)), ([SS](https://api.semanticscholar.org/arXiv:2404.04302)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cbr-rag-case-based-reasoning-for-retrieval))
  * 04/04 - **Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra** <br>([:x:](https://arxiv.org/abs/2404.03647)), ([:book:](https://browse.arxiv.org/pdf/2404.03647.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03647.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03647)), ([:house:](https://huggingface.co/papers/2404.03647)), ([HTML](https://browse.arxiv.org/html/2404.03647v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03647)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03647v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03647)), ([SS](https://api.semanticscholar.org/arXiv:2404.03647)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/capabilities-of-large-language-models-in))
  * 04/04 - **CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues** <br>([:x:](https://arxiv.org/abs/2404.03820)), ([:book:](https://browse.arxiv.org/pdf/2404.03820.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03820.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03820)), ([:house:](https://huggingface.co/papers/2404.03820)), ([HTML](https://browse.arxiv.org/html/2404.03820v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03820)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03820v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03820)), ([SS](https://api.semanticscholar.org/arXiv:2404.03820)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/canttalkaboutthis-aligning-language-models-to))
  * 04/04 - **AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent** <br>([:x:](https://arxiv.org/abs/2404.03648)), ([:book:](https://browse.arxiv.org/pdf/2404.03648.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03648.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03648)), ([:house:](https://huggingface.co/papers/2404.03648)), ([HTML](https://browse.arxiv.org/html/2404.03648v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03648)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03648v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03648)), ([SS](https://api.semanticscholar.org/arXiv:2404.03648)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/autowebglm-bootstrap-and-reinforce-a-large)), ([:octocat:](https://github.com/thudm/autowebglm)![GitHub Repo stars](https://img.shields.io/github/stars/thudm/autowebglm?style=social))
  * 04/04 - **Training LLMs over Neurally Compressed Text** <br>([:x:](https://arxiv.org/abs/2404.03626)), ([:book:](https://browse.arxiv.org/pdf/2404.03626.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03626.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03626)), ([:house:](https://huggingface.co/papers/2404.03626)), ([HTML](https://browse.arxiv.org/html/2404.03626v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03626)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03626v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03626)), ([SS](https://api.semanticscholar.org/arXiv:2404.03626)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/training-llms-over-neurally-compressed-text))	
  * 04/04 - **ReFT: Representation Finetuning for Language Models** <br>([:x:](https://arxiv.org/abs/2404.03592)), ([:book:](https://browse.arxiv.org/pdf/2404.03592.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03592.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03592)), ([:house:](https://huggingface.co/papers/2404.03592)), ([HTML](https://browse.arxiv.org/html/2404.03592v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03592)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03592v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03592)), ([SS](https://api.semanticscholar.org/arXiv:2404.03592)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reft-representation-finetuning-for-language)), ([:octocat:](https://github.com/stanfordnlp/pyreft)![GitHub Repo stars](https://img.shields.io/github/stars/stanfordnlp/pyreft?style=social))	
  * 04/04 - **Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?** <br>([:x:](https://arxiv.org/abs/2404.03411)), ([:book:](https://browse.arxiv.org/pdf/2404.03411.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03411.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03411)), ([:house:](https://huggingface.co/papers/2404.03411)), ([HTML](https://browse.arxiv.org/html/2404.03411v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03411)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03411v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03411)), ([SS](https://api.semanticscholar.org/arXiv:2404.03411)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/red-teaming-gpt-4v-are-gpt-4v-safe-against))	
  * 04/04 - **RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis** <br>([:x:](https://arxiv.org/abs/2404.03204)), ([:book:](https://browse.arxiv.org/pdf/2404.03204.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03204.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03204)), ([:house:](https://huggingface.co/papers/2404.03204)), ([HTML](https://browse.arxiv.org/html/2404.03204v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03204)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03204v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03204)), ([SS](https://api.semanticscholar.org/arXiv:2404.03204)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rall-e-robust-codec-language-modeling-with))	
  * 04/04 - **PointInfinity: Resolution-Invariant Point Diffusion Models** <br>([:x:](https://arxiv.org/abs/2404.03566)), ([:book:](https://browse.arxiv.org/pdf/2404.03566.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03566.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03566)), ([:house:](https://huggingface.co/papers/2404.03566)), ([HTML](https://browse.arxiv.org/html/2404.03566v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03566)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03566v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03566)), ([SS](https://api.semanticscholar.org/arXiv:2404.03566)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pointinfinity-resolution-invariant-point))	
  * 04/04 - **MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens** <br>([:x:](https://arxiv.org/abs/2404.03413)), ([:book:](https://browse.arxiv.org/pdf/2404.03413.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03413.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03413)), ([:house:](https://huggingface.co/papers/2404.03413)), ([HTML](https://browse.arxiv.org/html/2404.03413v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03413)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03413v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03413)), ([SS](https://api.semanticscholar.org/arXiv:2404.03413)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/minigpt4-video-advancing-multimodal-llms-for))	
  * 04/04 - **CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching** <br>([:x:](https://arxiv.org/abs/2404.03653)), ([:book:](https://browse.arxiv.org/pdf/2404.03653.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03653.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03653)), ([:house:](https://huggingface.co/papers/2404.03653)), ([HTML](https://browse.arxiv.org/html/2404.03653v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03653)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03653v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03653)), ([SS](https://api.semanticscholar.org/arXiv:2404.03653)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/comat-aligning-text-to-image-diffusion-model))	
  * 04/04 - **CodeEditorBench: Evaluating Code Editing Capability of Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.03543)), ([:book:](https://browse.arxiv.org/pdf/2404.03543.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03543.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03543)), ([:house:](https://huggingface.co/papers/2404.03543)), ([HTML](https://browse.arxiv.org/html/2404.03543v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03543)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03543v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03543)), ([SS](https://api.semanticscholar.org/arXiv:2404.03543)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/codeeditorbench-evaluating-code-editing))	
  * 04/04 - **AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent** <br>([:x:](https://arxiv.org/abs/2404.03648)), ([:book:](https://browse.arxiv.org/pdf/2404.03648.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03648.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03648)), ([:house:](https://huggingface.co/papers/2404.03648)), ([HTML](https://browse.arxiv.org/html/2404.03648v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03648)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03648v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03648)), ([SS](https://api.semanticscholar.org/arXiv:2404.03648)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/autowebglm-bootstrap-and-reinforce-a-large)), ([:octocat:](https://github.com/thudm/autowebglm)![GitHub Repo stars](https://img.shields.io/github/stars/thudm/autowebglm?style=social))	
  * 04/03 - **Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction** <br>([:x:](https://arxiv.org/abs/2404.02905)), ([:book:](https://browse.arxiv.org/pdf/2404.02905.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02905.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02905)), ([:house:](https://huggingface.co/papers/2404.02905)), ([HTML](https://browse.arxiv.org/html/2404.02905v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02905)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02905v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02905)), ([SS](https://api.semanticscholar.org/arXiv:2404.02905)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image)), ([:octocat:](https://github.com/FoundationVision/VAR)![GitHub Repo stars](https://img.shields.io/github/stars/FoundationVision/VAR?style=social))	
  * 04/03 - **On the Scalability of Diffusion-based Text-to-Image Generation** <br>([:x:](https://arxiv.org/abs/2404.02883)), ([:book:](https://browse.arxiv.org/pdf/2404.02883.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02883.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02883)), ([:house:](https://huggingface.co/papers/2404.02883)), ([HTML](https://browse.arxiv.org/html/2404.02883v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02883)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02883v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02883)), ([SS](https://api.semanticscholar.org/arXiv:2404.02883)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-the-scalability-of-diffusion-based-text-to))	
  * 04/03 - **Many-shot jailbreaking** <br>([:x:](https://www.anthropic.com/research/many-shot-jailbreaking)) 	
  * 04/03 - **LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models** <br>([:x:](https://arxiv.org/abs/2404.03118)), ([:book:](https://browse.arxiv.org/pdf/2404.03118.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03118.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03118)), ([:house:](https://huggingface.co/papers/2404.03118)), ([HTML](https://browse.arxiv.org/html/2404.03118v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03118)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03118v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03118)), ([SS](https://api.semanticscholar.org/arXiv:2404.03118)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lvlm-intrepret-an-interpretability-tool-for))	
  * 04/03 - **Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models** <br>([:x:](https://arxiv.org/abs/2404.02575)), ([:book:](https://browse.arxiv.org/pdf/2404.02575.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02575.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02575)), ([:house:](https://huggingface.co/papers/2404.02575)), ([HTML](https://browse.arxiv.org/html/2404.02575v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02575)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02575v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02575)), ([SS](https://api.semanticscholar.org/arXiv:2404.02575)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-models-as-compilers-simulating))	
  * 04/03 - **InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation** <br>([:x:](https://arxiv.org/abs/2404.02733)), ([:book:](https://browse.arxiv.org/pdf/2404.02733.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02733.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02733)), ([:house:](https://huggingface.co/papers/2404.02733)), ([HTML](https://browse.arxiv.org/html/2404.02733v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02733)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02733v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02733)), ([SS](https://api.semanticscholar.org/arXiv:2404.02733)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instantstyle-free-lunch-towards-style)), ([:octocat:](https://github.com/instantstyle/instantstyle)![GitHub Repo stars](https://img.shields.io/github/stars/instantstyle/instantstyle?style=social))	
  * 04/03 - **Freditor: High-Fidelity and Transferable NeRF Editing by Frequency Decomposition** <br>([:x:](https://arxiv.org/abs/2404.02514)), ([:book:](https://browse.arxiv.org/pdf/2404.02514.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02514.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02514)), ([:house:](https://huggingface.co/papers/2404.02514)), ([HTML](https://browse.arxiv.org/html/2404.02514v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02514)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02514v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02514)), ([SS](https://api.semanticscholar.org/arXiv:2404.02514)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/freditor-high-fidelity-and-transferable-nerf))	
  * 04/03 - **Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models** <br>([:x:](https://arxiv.org/abs/2404.02747)), ([:book:](https://browse.arxiv.org/pdf/2404.02747.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02747.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02747)), ([:house:](https://huggingface.co/papers/2404.02747)), ([HTML](https://browse.arxiv.org/html/2404.02747v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02747)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02747v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02747)), ([SS](https://api.semanticscholar.org/arXiv:2404.02747)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cross-attention-makes-inference-cumbersome-in)), ([:octocat:](https://github.com/haozheliu-st/t-gate)![GitHub Repo stars](https://img.shields.io/github/stars/haozheliu-st/t-gate?style=social))	
  * 04/03 - **ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline** <br>([:x:](https://arxiv.org/abs/2404.02893)), ([:book:](https://browse.arxiv.org/pdf/2404.02893.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02893.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02893)), ([:house:](https://huggingface.co/papers/2404.02893)), ([HTML](https://browse.arxiv.org/html/2404.02893v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02893)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02893v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02893)), ([SS](https://api.semanticscholar.org/arXiv:2404.02893)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatglm-math-improving-math-problem-solving)), ([:octocat:](https://github.com/thudm/chatglm-math)![GitHub Repo stars](https://img.shields.io/github/stars/thudm/chatglm-math?style=social))	
  * 04/02 - **UK & United States announce partnership on science of AI safety** <br>  ([News](https://www.gov.uk/government/news/uk-united-states-announce-partnership-on-science-of-ai-safety)), 
  * 04/02 - **Large Language Models as Planning Domain Generators** <br>([:x:](https://arxiv.org/abs/2405.06650)), ([:book:](https://browse.arxiv.org/pdf/2405.06650.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.06650.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.06650)), ([:house:](https://huggingface.co/papers/2405.06650)), ([HTML](https://browse.arxiv.org/html/2405.06650v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.06650)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.06650v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.06650)), ([SS](https://api.semanticscholar.org/arXiv:2405.06650))
  * 04/02 - **Poro 34B and the Blessing of Multilinguality** <br>([:x:](https://arxiv.org/abs/2404.01856)), ([:book:](https://browse.arxiv.org/pdf/2404.01856.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01856.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01856)), ([:house:](https://huggingface.co/papers/2404.01856)), ([HTML](https://browse.arxiv.org/html/2404.01856v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01856)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01856v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01856)), ([SS](https://api.semanticscholar.org/arXiv:2404.01856)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/poro-34b-and-the-blessing-of-multilinguality))	
  * 04/02 - **Octopus v2: On-device language model for super agent** <br>([:x:](https://arxiv.org/abs/2404.01744)), ([:book:](https://browse.arxiv.org/pdf/2404.01744.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01744.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01744)), ([:house:](https://huggingface.co/papers/2404.01744)), ([HTML](https://browse.arxiv.org/html/2404.01744v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01744)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01744v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01744)), ([SS](https://api.semanticscholar.org/arXiv:2404.01744)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/octopus-v2-on-device-language-model-for-super))	
  * 04/02 - **Mixture-of-Depths: Dynamically allocating compute in transformer-based language models** <br>([:x:](https://arxiv.org/abs/2404.02258)), ([:book:](https://browse.arxiv.org/pdf/2404.02258.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02258.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02258)), ([:house:](https://huggingface.co/papers/2404.02258)), ([HTML](https://browse.arxiv.org/html/2404.02258v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02258)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02258v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02258)), ([SS](https://api.semanticscholar.org/arXiv:2404.02258)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mixture-of-depths-dynamically-allocating))	
  * 04/02 - **Long-context LLMs Struggle with Long In-context Learning** <br>([:x:](https://arxiv.org/abs/2404.02060)), ([:book:](https://browse.arxiv.org/pdf/2404.02060.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02060.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02060)), ([:house:](https://huggingface.co/papers/2404.02060)), ([HTML](https://browse.arxiv.org/html/2404.02060v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02060)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02060v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02060)), ([SS](https://api.semanticscholar.org/arXiv:2404.02060)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/long-context-llms-struggle-with-long-in)), ([:octocat:](https://github.com/tiger-ai-lab/longiclbench)![GitHub Repo stars](https://img.shields.io/github/stars/tiger-ai-lab/longiclbench?style=social))	
  * 04/02 - **LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.01617)), ([:book:](https://browse.arxiv.org/pdf/2404.01617.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01617.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01617)), ([:house:](https://huggingface.co/papers/2404.01617)), ([HTML](https://browse.arxiv.org/html/2404.01617v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01617)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01617v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01617)), ([SS](https://api.semanticscholar.org/arXiv:2404.01617)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-abr-designing-adaptive-bitrate-algorithms))	
  * 04/02 - **Large language models could change the future of behavioral healthcare: a proposal for responsible development and evaluation** <br>([:x:](https://www.nature.com/articles/s44184-024-00056-z)) 	
  * 04/02 - **HyperCLOVA X Technical Report** <br>([:x:](https://arxiv.org/abs/2404.01954)), ([:book:](https://browse.arxiv.org/pdf/2404.01954.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01954.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01954)), ([:house:](https://huggingface.co/papers/2404.01954)), ([HTML](https://browse.arxiv.org/html/2404.01954v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01954)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01954v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01954)), ([SS](https://api.semanticscholar.org/arXiv:2404.01954)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hyperclova-x-technical-report))	
  * 04/02 - **CameraCtrl: Enabling Camera Control for Text-to-Video Generation** <br>([:x:](https://arxiv.org/abs/2404.02101)), ([:book:](https://browse.arxiv.org/pdf/2404.02101.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02101.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02101)), ([:house:](https://huggingface.co/papers/2404.02101)), ([HTML](https://browse.arxiv.org/html/2404.02101v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02101)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02101v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02101)), ([SS](https://api.semanticscholar.org/arXiv:2404.02101)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cameractrl-enabling-camera-control-for-text)), ([:octocat:](https://github.com/hehao13/cameractrl)![GitHub Repo stars](https://img.shields.io/github/stars/hehao13/cameractrl?style=social))	
  * 04/02 - **Advancing LLM Reasoning Generalists with Preference Trees** <br>([:x:](https://arxiv.org/abs/2404.02078)), ([:book:](https://browse.arxiv.org/pdf/2404.02078.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.02078.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.02078)), ([:house:](https://huggingface.co/papers/2404.02078)), ([HTML](https://browse.arxiv.org/html/2404.02078v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.02078)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.02078v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.02078)), ([SS](https://api.semanticscholar.org/arXiv:2404.02078)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/advancing-llm-reasoning-generalists-with)), ([:octocat:](https://github.com/openbmb/eurus)![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/eurus?style=social))
  * 04/01 - **Stream of Search (SoS): Learning to Search in Language** <br>([:x:](https://arxiv.org/abs/2404.03683)), ([:book:](https://browse.arxiv.org/pdf/2404.03683.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03683.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03683)), ([:house:](https://huggingface.co/papers/2404.03683)), ([HTML](https://browse.arxiv.org/html/2404.03683v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03683)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03683v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03683)), ([SS](https://api.semanticscholar.org/arXiv:2404.03683)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stream-of-search-sos-learning-to-search-in)), ([:octocat:](https://github.com/kanishkg/stream-of-search)![GitHub Repo stars](https://img.shields.io/github/stars/kanishkg/stream-of-search?style=social))   
  * 04/01 - **LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models** <br>([:x:](https://arxiv.org/abs/2404.01230)), ([:book:](https://browse.arxiv.org/pdf/2404.01230.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01230.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01230)), ([:house:](https://huggingface.co/papers/2404.01230)), ([HTML](https://browse.arxiv.org/html/2404.01230v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01230)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01230v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01230)), ([SS](https://api.semanticscholar.org/arXiv:2404.01230)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-as-a-mastermind-a-survey-of-strategic))
  * 04/01 - **The Rise and Rise of A.I.  Large Language Models (LLMs)** <br>  ([Blog](https://informationisbeautiful.net/visualizations/the-rise-of-generative-ai-large-language-models-llms-like-chatgpt/)), 	
  * 04/01 - **Streaming Dense Video Captioning** <br>([:x:](https://arxiv.org/abs/2404.01297)), ([:book:](https://browse.arxiv.org/pdf/2404.01297.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01297.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01297)), ([:house:](https://huggingface.co/papers/2404.01297)), ([HTML](https://browse.arxiv.org/html/2404.01297v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01297)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01297v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01297)), ([SS](https://api.semanticscholar.org/arXiv:2404.01297)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/streaming-dense-video-captioning)), ([:octocat:](https://github.com/google-research/scenic)![GitHub Repo stars](https://img.shields.io/github/stars/google-research/scenic?style=social))	
  * 04/01 - **Measuring Style Similarity in Diffusion Models** <br>([:x:](https://arxiv.org/abs/2404.01292)), ([:book:](https://browse.arxiv.org/pdf/2404.01292.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01292.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01292)), ([:house:](https://huggingface.co/papers/2404.01292)), ([HTML](https://browse.arxiv.org/html/2404.01292v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01292)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01292v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01292)), ([SS](https://api.semanticscholar.org/arXiv:2404.01292)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/measuring-style-similarity-in-diffusion)), ([:octocat:](https://github.com/learn2phoenix/csd)![GitHub Repo stars](https://img.shields.io/github/stars/learn2phoenix/csd?style=social))	
  * 04/01 - **Getting it Right: Improving Spatial Consistency in Text-to-Image Models** <br>([:x:](https://arxiv.org/abs/2404.01197)), ([:book:](https://browse.arxiv.org/pdf/2404.01197.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01197.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01197)), ([:house:](https://huggingface.co/papers/2404.01197)), ([HTML](https://browse.arxiv.org/html/2404.01197v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01197)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01197v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01197)), ([SS](https://api.semanticscholar.org/arXiv:2404.01197)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/getting-it-right-improving-spatial)), ([:octocat:](https://github.com/SPRIGHT-T2I/SPRIGHT)![GitHub Repo stars](https://img.shields.io/github/stars/SPRIGHT-T2I/SPRIGHT?style=social))	
  * 04/01 - **For Data-Guzzling AI Companies, the Internet Is Too Small** <br>  ([News](https://archive.is/PVGVH)), 	
  * 04/01 - **FlexiDreamer: Single Image-to-3D Generation with FlexiCubes** <br>([:x:](https://arxiv.org/abs/2404.00987)), ([:book:](https://browse.arxiv.org/pdf/2404.00987.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.00987.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.00987)), ([:house:](https://huggingface.co/papers/2404.00987)), ([HTML](https://browse.arxiv.org/html/2404.00987v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.00987)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.00987v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00987)), ([SS](https://api.semanticscholar.org/arXiv:2404.00987)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flexidreamer-single-image-to-3d-generation))	
  * 04/01 - **Evalverse: Unified and Accessible Library for Large Language Model Evaluation** <br>([:x:](https://arxiv.org/abs/2404.00943)), ([:book:](https://browse.arxiv.org/pdf/2404.00943.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.00943.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.00943)), ([:house:](https://huggingface.co/papers/2404.00943)), ([HTML](https://browse.arxiv.org/html/2404.00943v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.00943)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.00943v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00943)), ([SS](https://api.semanticscholar.org/arXiv:2404.00943)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evalverse-unified-and-accessible-library-for)), ([:octocat:](https://github.com/upstageai/evalverse)![GitHub Repo stars](https://img.shields.io/github/stars/upstageai/evalverse?style=social))	
  * 04/01 - **Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward** <br>([:x:](https://arxiv.org/abs/2404.01258)), ([:book:](https://browse.arxiv.org/pdf/2404.01258.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01258.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01258)), ([:house:](https://huggingface.co/papers/2404.01258)), ([HTML](https://browse.arxiv.org/html/2404.01258v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01258)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01258v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01258)), ([SS](https://api.semanticscholar.org/arXiv:2404.01258)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/direct-preference-optimization-of-video-large)), ([:octocat:](https://github.com/riflezhang/llava-hound-dpo)![GitHub Repo stars](https://img.shields.io/github/stars/riflezhang/llava-hound-dpo?style=social))	
  * 04/01 - **DBRX, Continual Pretraining, RewardBench, Faster Inference, and More** <br>  ([Blog](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench)), 	
  * 04/01 - **CosmicMan: A Text-to-Image Foundation Model for Humans** <br>([:x:](https://arxiv.org/abs/2404.01294)), ([:book:](https://browse.arxiv.org/pdf/2404.01294.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01294.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01294)), ([:house:](https://huggingface.co/papers/2404.01294)), ([HTML](https://browse.arxiv.org/html/2404.01294v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01294)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01294v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01294)), ([SS](https://api.semanticscholar.org/arXiv:2404.01294)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cosmicman-a-text-to-image-foundation-model))	
  * 04/01 - **Condition-Aware Neural Network for Controlled Image Generation** <br>([:x:](https://arxiv.org/abs/2404.01143)), ([:book:](https://browse.arxiv.org/pdf/2404.01143.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01143.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01143)), ([:house:](https://huggingface.co/papers/2404.01143)), ([HTML](https://browse.arxiv.org/html/2404.01143v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01143)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01143v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01143)), ([SS](https://api.semanticscholar.org/arXiv:2404.01143)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/condition-aware-neural-network-for-controlled))	
  * 04/01 - **Bigger is not Always Better: Scaling Properties of Latent Diffusion Models** <br>([:x:](https://arxiv.org/abs/2404.01367)), ([:book:](https://browse.arxiv.org/pdf/2404.01367.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01367.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01367)), ([:house:](https://huggingface.co/papers/2404.01367)), ([HTML](https://browse.arxiv.org/html/2404.01367v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01367)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01367v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01367)), ([SS](https://api.semanticscholar.org/arXiv:2404.01367)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bigger-is-not-always-better-scaling))	
  * 04/01 - **Are large language models superhuman chemists?** <br>([:x:](https://arxiv.org/abs/2404.01475)), ([:book:](https://browse.arxiv.org/pdf/2404.01475.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01475.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01475)), ([:house:](https://huggingface.co/papers/2404.01475)), ([HTML](https://browse.arxiv.org/html/2404.01475v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01475)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01475v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01475)), ([SS](https://api.semanticscholar.org/arXiv:2404.01475)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/are-large-language-models-superhuman-chemists))	
  * 03/31 - **WavLLM: Towards Robust and Adaptive Speech Large Language Model** <br>([:x:](https://arxiv.org/abs/2404.00656)), ([:book:](https://browse.arxiv.org/pdf/2404.00656.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.00656.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.00656)), ([:house:](https://huggingface.co/papers/2404.00656)), ([HTML](https://browse.arxiv.org/html/2404.00656v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.00656)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.00656v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00656)), ([SS](https://api.semanticscholar.org/arXiv:2404.00656)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wavllm-towards-robust-and-adaptive-speech))	
  * 03/31 - **Tired of Plugins? Large Language Models Can Be End-To-End Recommenders** <br>([:x:](https://arxiv.org/abs/2404.00702)), ([:book:](https://browse.arxiv.org/pdf/2404.00702.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.00702.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.00702)), ([:house:](https://huggingface.co/papers/2404.00702)), ([HTML](https://browse.arxiv.org/html/2404.00702v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.00702)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.00702v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00702)), ([SS](https://api.semanticscholar.org/arXiv:2404.00702)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tired-of-plugins-large-language-models-can-be)), ([:octocat:](https://github.com/wlzhang2020/UniLLMRec)![GitHub Repo stars](https://img.shields.io/github/stars/wlzhang2020/UniLLMRec?style=social))	
  * 03/30 - **Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods** <br>([:x:](https://arxiv.org/abs/2404.00282)), ([:book:](https://browse.arxiv.org/pdf/2404.00282.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.00282.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.00282)), ([:house:](https://huggingface.co/papers/2404.00282)), ([HTML](https://browse.arxiv.org/html/2404.00282v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.00282)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.00282v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00282)), ([SS](https://api.semanticscholar.org/arXiv:2404.00282)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/survey-on-large-language-model-enhanced))
  * 03/30 - **ST-LLM: Large Language Models Are Effective Temporal Learners** <br>([:x:](https://arxiv.org/abs/2404.00308)), ([:book:](https://browse.arxiv.org/pdf/2404.00308.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.00308.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.00308)), ([:house:](https://huggingface.co/papers/2404.00308)), ([HTML](https://browse.arxiv.org/html/2404.00308v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.00308)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.00308v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00308)), ([SS](https://api.semanticscholar.org/arXiv:2404.00308))	
  * 03/30 - **Noise-Aware Training of Layout-Aware Language Models** <br>([:x:](https://arxiv.org/abs/2404.00488)), ([:book:](https://browse.arxiv.org/pdf/2404.00488.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.00488.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.00488)), ([:house:](https://huggingface.co/papers/2404.00488)), ([HTML](https://browse.arxiv.org/html/2404.00488v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.00488)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.00488v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00488)), ([SS](https://api.semanticscholar.org/arXiv:2404.00488)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/noise-aware-training-of-layout-aware-language))	
  * 03/30 - **MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview and Text** <br>([:x:](https://arxiv.org/abs/2404.00345)), ([:book:](https://browse.arxiv.org/pdf/2404.00345.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.00345.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.00345)), ([:house:](https://huggingface.co/papers/2404.00345)), ([HTML](https://browse.arxiv.org/html/2404.00345v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.00345)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.00345v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00345)), ([SS](https://api.semanticscholar.org/arXiv:2404.00345))	
  * 03/30 - **Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order** <br>([:x:](https://arxiv.org/abs/2404.00399)), ([:book:](https://browse.arxiv.org/pdf/2404.00399.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.00399.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.00399)), ([:house:](https://huggingface.co/papers/2404.00399)), ([HTML](https://browse.arxiv.org/html/2404.00399v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.00399)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.00399v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00399)), ([SS](https://api.semanticscholar.org/arXiv:2404.00399)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aurora-m-the-first-open-source-multilingual))	
  * 03/29 - **Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models** <br>([:x:](https://arxiv.org/abs/2403.20331)), ([:book:](https://browse.arxiv.org/pdf/2403.20331.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.20331.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.20331)), ([:house:](https://huggingface.co/papers/2403.20331)), ([HTML](https://browse.arxiv.org/html/2403.20331v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.20331)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.20331v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.20331)), ([SS](https://api.semanticscholar.org/arXiv:2403.20331)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unsolvable-problem-detection-evaluating))	
  * 03/29 - **Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs** <br>([:x:](https://arxiv.org/abs/2403.20041)), ([:book:](https://browse.arxiv.org/pdf/2403.20041.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.20041.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.20041)), ([:house:](https://huggingface.co/papers/2403.20041)), ([HTML](https://browse.arxiv.org/html/2403.20041v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.20041)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.20041v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.20041)), ([SS](https://api.semanticscholar.org/arXiv:2403.20041)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/transformer-lite-high-efficiency-deployment))	
  * 03/29 - **Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces** <br>([:x:](https://arxiv.org/abs/2403.20275)), ([:book:](https://browse.arxiv.org/pdf/2403.20275.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.20275.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.20275)), ([:house:](https://huggingface.co/papers/2403.20275)), ([HTML](https://browse.arxiv.org/html/2403.20275v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.20275)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.20275v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.20275)), ([SS](https://api.semanticscholar.org/arXiv:2403.20275)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/snap-it-tap-it-splat-it-tactile-informed-3d))	
  * 03/29 - **ReALM: Reference Resolution As Language Modeling** <br>([:x:](https://arxiv.org/abs/2403.20329)), ([:book:](https://browse.arxiv.org/pdf/2403.20329.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.20329.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.20329)), ([:house:](https://huggingface.co/papers/2403.20329)), ([HTML](https://browse.arxiv.org/html/2403.20329v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.20329)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.20329v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.20329)), ([SS](https://api.semanticscholar.org/arXiv:2403.20329)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/realm-reference-resolution-as-language))	
  * 03/29 - **NVIDIA H200 GPUs Crush MLPerf’s LLM Inferencing Benchmark** <br>  ([News](https://thenewstack.io/nvidia-h200-gpus-crush-mlperfs-llm-inferencing-benchmark/)), 	
  * 03/29 - **MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection** <br>([:x:](https://arxiv.org/abs/2403.19888)), ([:book:](https://browse.arxiv.org/pdf/2403.19888.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.19888.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.19888)), ([:house:](https://huggingface.co/papers/2403.19888)), ([HTML](https://browse.arxiv.org/html/2403.19888v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.19888)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.19888v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.19888)), ([SS](https://api.semanticscholar.org/arXiv:2403.19888)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mambamixer-efficient-selective-state-space))	
  * 03/29 - **LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model** <br>([:x:](https://arxiv.org/abs/2404.01331)), ([:book:](https://browse.arxiv.org/pdf/2404.01331.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.01331.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.01331)), ([:house:](https://huggingface.co/papers/2404.01331)), ([HTML](https://browse.arxiv.org/html/2404.01331v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.01331)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.01331v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.01331)), ([SS](https://api.semanticscholar.org/arXiv:2404.01331)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llava-gemma-accelerating-multimodal))	
  * 03/29 - **InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds** <br>([:x:](https://arxiv.org/abs/2403.20309)), ([:book:](https://browse.arxiv.org/pdf/2403.20309.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.20309.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.20309)), ([:house:](https://huggingface.co/papers/2403.20309)), ([HTML](https://browse.arxiv.org/html/2403.20309v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.20309)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.20309v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.20309)), ([SS](https://api.semanticscholar.org/arXiv:2403.20309)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/instantsplat-unbounded-sparse-view-pose-free))	
  * 03/29 - **Gecko: Versatile Text Embeddings Distilled from Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.20327)), ([:book:](https://browse.arxiv.org/pdf/2403.20327.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.20327.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.20327)), ([:house:](https://huggingface.co/papers/2403.20327)), ([HTML](https://browse.arxiv.org/html/2403.20327v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.20327)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.20327v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.20327)), ([SS](https://api.semanticscholar.org/arXiv:2403.20327)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gecko-versatile-text-embeddings-distilled))	
  * 03/29 - **DiJiang: Efficient Large Language Models through Compact Kernelization** <br>([:x:](https://arxiv.org/abs/2403.19928)), ([:book:](https://browse.arxiv.org/pdf/2403.19928.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.19928.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.19928)), ([:house:](https://huggingface.co/papers/2403.19928)), ([HTML](https://browse.arxiv.org/html/2403.19928v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.19928)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.19928v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.19928)), ([SS](https://api.semanticscholar.org/arXiv:2403.19928)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dijiang-efficient-large-language-models)), ([:octocat:](https://github.com/yuchuantian/dijiang)![GitHub Repo stars](https://img.shields.io/github/stars/yuchuantian/dijiang?style=social))	
  * 03/29 - **DeepMind develops SAFE, an AI-based app that can fact-check LLMs** <br>  ([News](https://techxplore.com/news/2024-03-deepmind-safe-ai-based-app.html)), 	
  * 03/29 - **CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning** <br>([:x:](https://arxiv.org/abs/2403.19918)), ([:book:](https://browse.arxiv.org/pdf/2403.19918.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.19918.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.19918)), ([:house:](https://huggingface.co/papers/2403.19918)), ([HTML](https://browse.arxiv.org/html/2403.19918v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.19918)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.19918v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.19918)), ([SS](https://api.semanticscholar.org/arXiv:2403.19918)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ctrl-sim-reactive-and-controllable-driving))	
  * 03/29 - **Are We on the Right Way for Evaluating Large Vision-Language Models?** <br>([:x:](https://arxiv.org/abs/2403.20330)), ([:book:](https://browse.arxiv.org/pdf/2403.20330.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.20330.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.20330)), ([:house:](https://huggingface.co/papers/2403.20330)), ([HTML](https://browse.arxiv.org/html/2403.20330v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.20330)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.20330v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.20330)), ([SS](https://api.semanticscholar.org/arXiv:2403.20330)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/are-we-on-the-right-way-for-evaluating-large))	
  * 03/28 - **sDPO: Don't Use Your Data All at Once** <br>([:x:](https://arxiv.org/abs/2403.19270)), ([:book:](https://browse.arxiv.org/pdf/2403.19270.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.19270.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.19270)), ([:house:](https://huggingface.co/papers/2403.19270)), ([HTML](https://browse.arxiv.org/html/2403.19270v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.19270)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.19270v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.19270)), ([SS](https://api.semanticscholar.org/arXiv:2403.19270)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sdpo-don-t-use-your-data-all-at-once))	
  * 03/28 - **Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation** <br>([:x:](https://arxiv.org/abs/2403.19319)), ([:book:](https://browse.arxiv.org/pdf/2403.19319.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.19319.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.19319)), ([:house:](https://huggingface.co/papers/2403.19319)), ([HTML](https://browse.arxiv.org/html/2403.19319v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.19319)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.19319v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.19319)), ([SS](https://api.semanticscholar.org/arXiv:2403.19319)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mesh2nerf-direct-mesh-supervision-for-neural))	
  * 03/28 - **Localizing Paragraph Memorization in Language Models** <br>([:x:](https://arxiv.org/abs/2403.19851)), ([:book:](https://browse.arxiv.org/pdf/2403.19851.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.19851.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.19851)), ([:house:](https://huggingface.co/papers/2403.19851)), ([HTML](https://browse.arxiv.org/html/2403.19851v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.19851)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.19851v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.19851)), ([SS](https://api.semanticscholar.org/arXiv:2403.19851)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/localizing-paragraph-memorization-in-language))	
  * 03/28 - **Jamba: A Hybrid Transformer-Mamba Language Model** <br>([:x:](https://arxiv.org/abs/2403.19887)), ([:book:](https://browse.arxiv.org/pdf/2403.19887.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.19887.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.19887)), ([:house:](https://huggingface.co/papers/2403.19887)), ([HTML](https://browse.arxiv.org/html/2403.19887v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.19887)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.19887v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.19887)), ([SS](https://api.semanticscholar.org/arXiv:2403.19887)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/jamba-a-hybrid-transformer-mamba-language))	
  * 03/28 - **GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling** <br>([:x:](https://arxiv.org/abs/2403.19655)), ([:book:](https://browse.arxiv.org/pdf/2403.19655.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.19655.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.19655)), ([:house:](https://huggingface.co/papers/2403.19655)), ([HTML](https://browse.arxiv.org/html/2403.19655v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.19655)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.19655v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.19655)), ([SS](https://api.semanticscholar.org/arXiv:2403.19655)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gaussiancube-structuring-gaussian-splatting))	
  * 03/28 - **Claude 3 overtakes GPT-4 in the duel of the AI bots. Here's how to get in on the action** <br>  ([News](https://www.zdnet.com/article/claude-3-overtakes-gpt-4-in-the-duel-of-the-ai-bots-heres-how-to-get-in-on-the-action/)), 	
  * 03/28 - **Announcing Grok-1.5** <br>  ([Blog](https://x.ai/blog/grok-1.5)),  ([Demo](https://grok.x.ai/)), 	
  * 03/27 - **A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks** <br>([:x:](https://arxiv.org/abs/2403.18537)), ([:book:](https://browse.arxiv.org/pdf/2403.18537.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18537.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18537)), ([:house:](https://huggingface.co/papers/2403.18537)), ([HTML](https://browse.arxiv.org/html/2403.18537v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18537)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18537v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18537)), ([SS](https://api.semanticscholar.org/arXiv:2403.18537)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-path-towards-legal-autonomy-an))
  * 03/27 - **ViTAR: Vision Transformer with Any Resolution** <br>([:x:](https://arxiv.org/abs/2403.18361)), ([:book:](https://browse.arxiv.org/pdf/2403.18361.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18361.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18361)), ([:house:](https://huggingface.co/papers/2403.18361)), ([HTML](https://browse.arxiv.org/html/2403.18361v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18361)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18361v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18361)), ([SS](https://api.semanticscholar.org/arXiv:2403.18361)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vitar-vision-transformer-with-any-resolution))	
  * 03/27 - **Towards a World-English Language Model for On-Device Virtual Assistants** <br>([:x:](https://arxiv.org/abs/2403.18783)), ([:book:](https://browse.arxiv.org/pdf/2403.18783.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18783.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18783)), ([:house:](https://huggingface.co/papers/2403.18783)), ([HTML](https://browse.arxiv.org/html/2403.18783v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18783)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18783v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18783)), ([SS](https://api.semanticscholar.org/arXiv:2403.18783)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-a-world-english-language-model-for-on))	
  * 03/27 - **TextCraftor: Your Text Encoder Can be Image Quality Controller** <br>([:x:](https://arxiv.org/abs/2403.18978)), ([:book:](https://browse.arxiv.org/pdf/2403.18978.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18978.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18978)), ([:house:](https://huggingface.co/papers/2403.18978)), ([HTML](https://browse.arxiv.org/html/2403.18978v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18978)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18978v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18978)), ([SS](https://api.semanticscholar.org/arXiv:2403.18978)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/textcraftor-your-text-encoder-can-be-image))	
  * 03/27 - **ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion** <br>([:x:](https://arxiv.org/abs/2403.18818)), ([:book:](https://browse.arxiv.org/pdf/2403.18818.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18818.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18818)), ([:house:](https://huggingface.co/papers/2403.18818)), ([HTML](https://browse.arxiv.org/html/2403.18818v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18818)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18818v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18818)), ([SS](https://api.semanticscholar.org/arXiv:2403.18818)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/objectdrop-bootstrapping-counterfactuals-for))	
  * 03/27 - **Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models** <br>([:x:](https://arxiv.org/abs/2403.18814)), ([:book:](https://browse.arxiv.org/pdf/2403.18814.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18814.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18814)), ([:house:](https://huggingface.co/papers/2403.18814)), ([HTML](https://browse.arxiv.org/html/2403.18814v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18814)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18814v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18814)), ([SS](https://api.semanticscholar.org/arXiv:2403.18814)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mini-gemini-mining-the-potential-of-multi)), ([:octocat:](https://github.com/dvlab-research/minigemini)![GitHub Repo stars](https://img.shields.io/github/stars/dvlab-research/minigemini?style=social))	
  * 03/27 - **Long-form factuality in large language models** <br>([:x:](https://arxiv.org/abs/2403.18802)), ([:book:](https://browse.arxiv.org/pdf/2403.18802.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18802.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18802)), ([:house:](https://huggingface.co/papers/2403.18802)), ([HTML](https://browse.arxiv.org/html/2403.18802v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18802)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18802v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18802)), ([SS](https://api.semanticscholar.org/arXiv:2403.18802)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/long-form-factuality-in-large-language-models)), ([:octocat:](https://github.com/google-deepmind/long-form-factuality)![GitHub Repo stars](https://img.shields.io/github/stars/google-deepmind/long-form-factuality?style=social))	
  * 03/27 - **LITA: Language Instructed Temporal-Localization Assistant** <br>([:x:](https://arxiv.org/abs/2403.19046)), ([:book:](https://browse.arxiv.org/pdf/2403.19046.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.19046.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.19046)), ([:house:](https://huggingface.co/papers/2403.19046)), ([HTML](https://browse.arxiv.org/html/2403.19046v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.19046)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.19046v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.19046)), ([SS](https://api.semanticscholar.org/arXiv:2403.19046)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lita-language-instructed-temporal)), ([:octocat:](https://github.com/nvlabs/lita)![GitHub Repo stars](https://img.shields.io/github/stars/nvlabs/lita?style=social))	
  * 03/27 - **Garment3DGen: 3D Garment Stylization and Texture Generation** <br>([:x:](https://arxiv.org/abs/2403.18816)), ([:book:](https://browse.arxiv.org/pdf/2403.18816.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18816.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18816)), ([:house:](https://huggingface.co/papers/2403.18816)), ([HTML](https://browse.arxiv.org/html/2403.18816v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18816)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18816v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18816)), ([SS](https://api.semanticscholar.org/arXiv:2403.18816)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/garment3dgen-3d-garment-stylization-and))	
  * 03/27 - **Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction** <br>([:x:](https://arxiv.org/abs/2403.18795)), ([:book:](https://browse.arxiv.org/pdf/2403.18795.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18795.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18795)), ([:house:](https://huggingface.co/papers/2403.18795)), ([HTML](https://browse.arxiv.org/html/2403.18795v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18795)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18795v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18795)), ([SS](https://api.semanticscholar.org/arXiv:2403.18795)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gamba-marry-gaussian-splatting-with-mamba-for))	
  * 03/27 - **FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image Editing** <br>([:x:](https://arxiv.org/abs/2403.18605)), ([:book:](https://browse.arxiv.org/pdf/2403.18605.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18605.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18605)), ([:house:](https://huggingface.co/papers/2403.18605)), ([HTML](https://browse.arxiv.org/html/2403.18605v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18605)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18605v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18605)), ([SS](https://api.semanticscholar.org/arXiv:2403.18605)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flexedit-flexible-and-controllable-diffusion))	
  * 03/27 - **BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text** <br>([:x:](https://arxiv.org/abs/2403.18421)), ([:book:](https://browse.arxiv.org/pdf/2403.18421.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18421.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18421)), ([:house:](https://huggingface.co/papers/2403.18421)), ([HTML](https://browse.arxiv.org/html/2403.18421v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18421)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18421v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18421)), ([SS](https://api.semanticscholar.org/arXiv:2403.18421)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/biomedlm-a-2-7b-parameter-language-model))	
  * 03/26 - **MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution** <br>([:x:](https://arxiv.org/abs/2403.17927)), ([:book:](https://browse.arxiv.org/pdf/2403.17927.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17927.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17927)), ([:house:](https://huggingface.co/papers/2403.17927)), ([HTML](https://browse.arxiv.org/html/2403.17927v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17927)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17927v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17927)), ([SS](https://api.semanticscholar.org/arXiv:2403.17927)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/twostep-multi-agent-task-planning-using))
  * 03/26 - **The Unreasonable Ineffectiveness of the Deeper Layers** <br>([:x:](https://arxiv.org/abs/2403.17887)), ([:book:](https://browse.arxiv.org/pdf/2403.17887.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17887.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17887)), ([:house:](https://huggingface.co/papers/2403.17887)), ([HTML](https://browse.arxiv.org/html/2403.17887v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17887)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17887v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17887)), ([SS](https://api.semanticscholar.org/arXiv:2403.17887)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-unreasonable-ineffectiveness-of-the))	
  * 03/26 - **TC4D: Trajectory-Conditioned Text-to-4D Generation** <br>([:x:](https://arxiv.org/abs/2403.17920)), ([:book:](https://browse.arxiv.org/pdf/2403.17920.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17920.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17920)), ([:house:](https://huggingface.co/papers/2403.17920)), ([HTML](https://browse.arxiv.org/html/2403.17920v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17920)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17920v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17920)), ([SS](https://api.semanticscholar.org/arXiv:2403.17920)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tc4d-trajectory-conditioned-text-to-4d))	
  * 03/26 - **Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians** <br>([:x:](https://arxiv.org/abs/2403.17898)), ([:book:](https://browse.arxiv.org/pdf/2403.17898.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17898.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17898)), ([:house:](https://huggingface.co/papers/2403.17898)), ([HTML](https://browse.arxiv.org/html/2403.17898v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17898)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17898v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17898)), ([SS](https://api.semanticscholar.org/arXiv:2403.17898)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/octree-gs-towards-consistent-real-time)), ([:octocat:](https://github.com/city-super/Octree-GS)![GitHub Repo stars](https://img.shields.io/github/stars/city-super/Octree-GS?style=social))	
  * 03/26 - **Introducing DBRX: A New State-of-the-Art Open LLM** <br>  ([Blog](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)), 	
  * 03/26 - **InternLM2 Technical Report** <br>([:x:](https://arxiv.org/abs/2403.17297)), ([:book:](https://browse.arxiv.org/pdf/2403.17297.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17297.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17297)), ([:house:](https://huggingface.co/papers/2403.17297)), ([HTML](https://browse.arxiv.org/html/2403.17297v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17297)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17297v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17297)), ([SS](https://api.semanticscholar.org/arXiv:2403.17297)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/internlm2-technical-report)), ([:octocat:](https://github.com/internlm/internlm)![GitHub Repo stars](https://img.shields.io/github/stars/internlm/internlm?style=social))	
  * 03/26 - **Improving Text-to-Image Consistency via Automatic Prompt Optimization** <br>([:x:](https://arxiv.org/abs/2403.17804)), ([:book:](https://browse.arxiv.org/pdf/2403.17804.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17804.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17804)), ([:house:](https://huggingface.co/papers/2403.17804)), ([HTML](https://browse.arxiv.org/html/2403.17804v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17804)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17804v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17804)), ([SS](https://api.semanticscholar.org/arXiv:2403.17804)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/improving-text-to-image-consistency-via))	
  * 03/26 - **Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs** <br>([:x:](https://arxiv.org/abs/2403.17607)), ([:book:](https://browse.arxiv.org/pdf/2403.17607.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17607.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17607)), ([:house:](https://huggingface.co/papers/2403.17607)), ([HTML](https://browse.arxiv.org/html/2403.17607v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17607)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17607v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17607)), ([SS](https://api.semanticscholar.org/arXiv:2403.17607)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fully-fused-multi-layer-perceptrons-on-intel)), ([:octocat:](https://github.com/intel/tiny-dpcpp-nn)![GitHub Repo stars](https://img.shields.io/github/stars/intel/tiny-dpcpp-nn?style=social))	
  * 03/26 - **EgoLifter: Open-world 3D Segmentation for Egocentric Perception** <br>([:x:](https://arxiv.org/abs/2403.18118)), ([:book:](https://browse.arxiv.org/pdf/2403.18118.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.18118.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.18118)), ([:house:](https://huggingface.co/papers/2403.18118)), ([HTML](https://browse.arxiv.org/html/2403.18118v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.18118)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.18118v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.18118)), ([SS](https://api.semanticscholar.org/arXiv:2403.18118)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/egolifter-open-world-3d-segmentation-for))	
  * 03/26 - **AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation** <br>([:x:](https://arxiv.org/abs/2403.17694)), ([:book:](https://browse.arxiv.org/pdf/2403.17694.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17694.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17694)), ([:house:](https://huggingface.co/papers/2403.17694)), ([HTML](https://browse.arxiv.org/html/2403.17694v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17694)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17694v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17694)), ([SS](https://api.semanticscholar.org/arXiv:2403.17694)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/aniportrait-audio-driven-synthesis-of)), ([:octocat:](https://github.com/scutzzj/aniportrait)![GitHub Repo stars](https://img.shields.io/github/stars/scutzzj/aniportrait?style=social))	
  * 03/26 - **2D Gaussian Splatting for Geometrically Accurate Radiance Fields** <br>([:x:](https://arxiv.org/abs/2403.17888)), ([:book:](https://browse.arxiv.org/pdf/2403.17888.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17888.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17888)), ([:house:](https://huggingface.co/papers/2403.17888)), ([HTML](https://browse.arxiv.org/html/2403.17888v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17888)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17888v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17888)), ([SS](https://api.semanticscholar.org/arXiv:2403.17888)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/2d-gaussian-splatting-for-geometrically))
  * 03/25 - **Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm** <br>([:x:](https://arxiv.org/abs/2403.16446)), ([:book:](https://browse.arxiv.org/pdf/2403.16446.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.16446.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.16446)), ([:house:](https://huggingface.co/papers/2403.16446)), ([HTML](https://browse.arxiv.org/html/2403.16446v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.16446)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.16446v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.16446)), ([SS](https://api.semanticscholar.org/arXiv:2403.16446)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-automatic-evaluation-for-llms))
  * 03/25 - **RepairAgent: An Autonomous, LLM-Based Agent for Program Repair** <br>([:x:](https://arxiv.org/abs/2403.17134)), ([:book:](https://browse.arxiv.org/pdf/2403.17134.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17134.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17134)), ([:house:](https://huggingface.co/papers/2403.17134)), ([HTML](https://browse.arxiv.org/html/2403.17134v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17134)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17134v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17134)), ([SS](https://api.semanticscholar.org/arXiv:2403.17134)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/repairagent-an-autonomous-llm-based-agent-for))
  * 03/25 - **RL for Consistency Models: Faster Reward Guided Text-to-Image Generation** <br>([:x:](https://arxiv.org/abs/2404.03673)), ([:book:](https://browse.arxiv.org/pdf/2404.03673.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.03673.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.03673)), ([:house:](https://huggingface.co/papers/2404.03673)), ([HTML](https://browse.arxiv.org/html/2404.03673v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.03673)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.03673v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.03673)), ([SS](https://api.semanticscholar.org/arXiv:2404.03673)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rl-for-consistency-models-faster-reward)), ([:octocat:](https://github.com/Owen-Oertell/rlcm)![GitHub Repo stars](https://img.shields.io/github/stars/Owen-Oertell/rlcm?style=social))   
  * 03/25 - **VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation** <br>([:x:](https://arxiv.org/abs/2403.17001)), ([:book:](https://browse.arxiv.org/pdf/2403.17001.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17001.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17001)), ([:house:](https://huggingface.co/papers/2403.17001)), ([HTML](https://browse.arxiv.org/html/2403.17001v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17001)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17001v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17001)), ([SS](https://api.semanticscholar.org/arXiv:2403.17001)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vp3d-unleashing-2d-visual-prompt-for-text-to))	
  * 03/25 - **TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models** <br>([:x:](https://arxiv.org/abs/2403.17005)), ([:book:](https://browse.arxiv.org/pdf/2403.17005.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17005.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17005)), ([:house:](https://huggingface.co/papers/2403.17005)), ([HTML](https://browse.arxiv.org/html/2403.17005v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17005)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17005v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17005)), ([SS](https://api.semanticscholar.org/arXiv:2403.17005)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/trip-temporal-residual-learning-with-image))	
  * 03/25 - **SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions** <br>([:x:](https://arxiv.org/abs/2403.16627)), ([:book:](https://browse.arxiv.org/pdf/2403.16627.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.16627.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.16627)), ([:house:](https://huggingface.co/papers/2403.16627)), ([HTML](https://browse.arxiv.org/html/2403.16627v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.16627)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.16627v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.16627)), ([SS](https://api.semanticscholar.org/arXiv:2403.16627)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sdxs-real-time-one-step-latent-diffusion)), ([:octocat:](https://github.com/IDKiro/sdxs)![GitHub Repo stars](https://img.shields.io/github/stars/IDKiro/sdxs?style=social))	
  * 03/25 - **LLM Agent Operating System** <br>([:x:](https://arxiv.org/abs/2403.16971)), ([:book:](https://browse.arxiv.org/pdf/2403.16971.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.16971.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.16971)), ([:house:](https://huggingface.co/papers/2403.16971)), ([HTML](https://browse.arxiv.org/html/2403.16971v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.16971)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.16971v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.16971)), ([SS](https://api.semanticscholar.org/arXiv:2403.16971)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm-agent-operating-system)), ([:octocat:](https://github.com/agiresearch/aios)![GitHub Repo stars](https://img.shields.io/github/stars/agiresearch/aios?style=social))	
  * 03/25 - **FlashFace: Human Image Personalization with High-fidelity Identity Preservation** <br>([:x:](https://arxiv.org/abs/2403.17008)), ([:book:](https://browse.arxiv.org/pdf/2403.17008.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17008.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17008)), ([:house:](https://huggingface.co/papers/2403.17008)), ([HTML](https://browse.arxiv.org/html/2403.17008v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17008)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17008v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17008)), ([SS](https://api.semanticscholar.org/arXiv:2403.17008)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/flashface-human-image-personalization-with))	
  * 03/25 - **DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion** <br>([:x:](https://arxiv.org/abs/2403.17237)), ([:book:](https://browse.arxiv.org/pdf/2403.17237.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.17237.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.17237)), ([:house:](https://huggingface.co/papers/2403.17237)), ([HTML](https://browse.arxiv.org/html/2403.17237v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.17237)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.17237v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17237)), ([SS](https://api.semanticscholar.org/arXiv:2403.17237)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreampolisher-towards-high-quality-text-to-3d)), ([:octocat:](https://github.com/yuanze-lin/DreamPolisher)![GitHub Repo stars](https://img.shields.io/github/stars/yuanze-lin/DreamPolisher?style=social))	
  * 03/25 - **Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation** <br>([:x:](https://arxiv.org/abs/2403.16990)), ([:book:](https://browse.arxiv.org/pdf/2403.16990.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.16990.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.16990)), ([:house:](https://huggingface.co/papers/2403.16990)), ([HTML](https://browse.arxiv.org/html/2403.16990v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.16990)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.16990v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.16990)), ([SS](https://api.semanticscholar.org/arXiv:2403.16990)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/be-yourself-bounded-attention-for-multi))	
  * 03/23 - **When LLM-based Code Generation Meets the Software Development Process** <br>([:x:](https://arxiv.org/abs/2403.15852)), ([:book:](https://browse.arxiv.org/pdf/2403.15852.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15852.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15852)), ([:house:](https://huggingface.co/papers/2403.15852)), ([HTML](https://browse.arxiv.org/html/2403.15852v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15852)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15852v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15852)), ([SS](https://api.semanticscholar.org/arXiv:2403.15852)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/when-llm-based-code-generation-meets-the))
  * 03/22 - **ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars** <br>([:x:](https://arxiv.org/abs/2403.15383)), ([:book:](https://browse.arxiv.org/pdf/2403.15383.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15383.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15383)), ([:house:](https://huggingface.co/papers/2403.15383)), ([HTML](https://browse.arxiv.org/html/2403.15383v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15383)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15383v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15383)), ([SS](https://api.semanticscholar.org/arXiv:2403.15383)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/themestation-generating-theme-aware-3d-assets))	
  * 03/22 - **SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series** <br>([:x:](https://arxiv.org/abs/2403.15360)), ([:book:](https://browse.arxiv.org/pdf/2403.15360.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15360.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15360)), ([:house:](https://huggingface.co/papers/2403.15360)), ([HTML](https://browse.arxiv.org/html/2403.15360v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15360)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15360v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15360)), ([SS](https://api.semanticscholar.org/arXiv:2403.15360)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/simba-simplified-mamba-based-architecture-for)), ([:octocat:](https://github.com/badripatro/simba)![GitHub Repo stars](https://img.shields.io/github/stars/badripatro/simba?style=social))	
  * 03/22 - **LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement** <br>([:x:](https://arxiv.org/abs/2403.15042)), ([:book:](https://browse.arxiv.org/pdf/2403.15042.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15042.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15042)), ([:house:](https://huggingface.co/papers/2403.15042)), ([HTML](https://browse.arxiv.org/html/2403.15042v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15042)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15042v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15042)), ([SS](https://api.semanticscholar.org/arXiv:2403.15042)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llm2llm-boosting-llms-with-novel-iterative)), ([:octocat:](https://github.com/squeezeailab/llm2llm)![GitHub Repo stars](https://img.shields.io/github/stars/squeezeailab/llm2llm?style=social))	
  * 03/22 - **LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis** <br>([:x:](https://arxiv.org/abs/2403.15385)), ([:book:](https://browse.arxiv.org/pdf/2403.15385.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15385.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15385)), ([:house:](https://huggingface.co/papers/2403.15385)), ([HTML](https://browse.arxiv.org/html/2403.15385v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15385)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15385v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15385)), ([SS](https://api.semanticscholar.org/arXiv:2403.15385)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/latte3d-large-scale-amortized-text-to))	
  * 03/22 - **InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding** <br>([:x:](https://arxiv.org/abs/2403.15377)), ([:book:](https://browse.arxiv.org/pdf/2403.15377.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15377.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15377)), ([:house:](https://huggingface.co/papers/2403.15377)), ([HTML](https://browse.arxiv.org/html/2403.15377v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15377)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15377v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15377)), ([SS](https://api.semanticscholar.org/arXiv:2403.15377)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/internvideo2-scaling-video-foundation-models)), ([:octocat:](https://github.com/opengvlab/internvideo2)![GitHub Repo stars](https://img.shields.io/github/stars/opengvlab/internvideo2?style=social))	
  * 03/22 - **FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions** <br>([:x:](https://arxiv.org/abs/2403.15246)), ([:book:](https://browse.arxiv.org/pdf/2403.15246.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15246.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15246)), ([:house:](https://huggingface.co/papers/2403.15246)), ([HTML](https://browse.arxiv.org/html/2403.15246v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15246)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15246v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15246)), ([SS](https://api.semanticscholar.org/arXiv:2403.15246)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/followir-evaluating-and-teaching-information)), ([:octocat:](https://github.com/hiyouga/llama-factory)![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/llama-factory?style=social))	
  * 03/22 - **DragAPart: Learning a Part-Level Motion Prior for Articulated Objects** <br>([:x:](https://arxiv.org/abs/2403.15382)), ([:book:](https://browse.arxiv.org/pdf/2403.15382.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15382.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15382)), ([:house:](https://huggingface.co/papers/2403.15382)), ([HTML](https://browse.arxiv.org/html/2403.15382v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15382)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15382v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15382)), ([SS](https://api.semanticscholar.org/arXiv:2403.15382)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dragapart-learning-a-part-level-motion-prior))	
  * 03/22 - **Can large language models explore in-context?** <br>([:x:](https://arxiv.org/abs/2403.15371)), ([:book:](https://browse.arxiv.org/pdf/2403.15371.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15371.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15371)), ([:house:](https://huggingface.co/papers/2403.15371)), ([HTML](https://browse.arxiv.org/html/2403.15371v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15371)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15371v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15371)), ([SS](https://api.semanticscholar.org/arXiv:2403.15371)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/can-large-language-models-explore-in-context))	
  * 03/22 - **AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.15157)), ([:book:](https://browse.arxiv.org/pdf/2403.15157.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15157.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15157)), ([:house:](https://huggingface.co/papers/2403.15157)), ([HTML](https://browse.arxiv.org/html/2403.15157v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15157)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15157v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15157)), ([SS](https://api.semanticscholar.org/arXiv:2403.15157))	
  * 03/21 - **PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning** <br>([:x:](https://arxiv.org/abs/2403.14227)), ([:book:](https://browse.arxiv.org/pdf/2403.14227.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14227.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14227)), ([:house:](https://huggingface.co/papers/2403.14227)), ([HTML](https://browse.arxiv.org/html/2403.14227v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14227)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14227v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14227)), ([SS](https://api.semanticscholar.org/arXiv:2403.14227)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/peergpt-probing-the-roles-of-llm-based-peer))
  * 03/21 - **StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN** <br>([:x:](https://arxiv.org/abs/2403.14186)), ([:book:](https://browse.arxiv.org/pdf/2403.14186.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14186.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14186)), ([:house:](https://huggingface.co/papers/2403.14186)), ([HTML](https://browse.arxiv.org/html/2403.14186v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14186)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14186v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14186)), ([SS](https://api.semanticscholar.org/arXiv:2403.14186)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stylecinegan-landscape-cinemagraph-generation)), ([:octocat:](https://github.com/jeolpyeoni/StyleCineGAN)![GitHub Repo stars](https://img.shields.io/github/stars/jeolpyeoni/StyleCineGAN?style=social))	
  * 03/21 - **StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text** <br>([:x:](https://arxiv.org/abs/2403.14773)), ([:book:](https://browse.arxiv.org/pdf/2403.14773.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14773.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14773)), ([:house:](https://huggingface.co/papers/2403.14773)), ([HTML](https://browse.arxiv.org/html/2403.14773v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14773)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14773v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14773)), ([SS](https://api.semanticscholar.org/arXiv:2403.14773)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and)), ([:octocat:](https://github.com/picsart-ai-research/streamingt2v)![GitHub Repo stars](https://img.shields.io/github/stars/picsart-ai-research/streamingt2v?style=social))	
  * 03/21 - **ReNoise: Real Image Inversion Through Iterative Noising** <br>([:x:](https://arxiv.org/abs/2403.14602)), ([:book:](https://browse.arxiv.org/pdf/2403.14602.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14602.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14602)), ([:house:](https://huggingface.co/papers/2403.14602)), ([HTML](https://browse.arxiv.org/html/2403.14602v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14602)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14602v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14602)), ([SS](https://api.semanticscholar.org/arXiv:2403.14602)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/renoise-real-image-inversion-through))	
  * 03/21 - **Recourse for reclamation: Chatting with generative language models** <br>([:x:](https://arxiv.org/abs/2403.14467)), ([:book:](https://browse.arxiv.org/pdf/2403.14467.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14467.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14467)), ([:house:](https://huggingface.co/papers/2403.14467)), ([HTML](https://browse.arxiv.org/html/2403.14467v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14467)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14467v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14467)), ([SS](https://api.semanticscholar.org/arXiv:2403.14467)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/recourse-for-reclamation-chatting-with))	
  * 03/21 - **RakutenAI-7B: Extending Large Language Models for Japanese** <br>([:x:](https://arxiv.org/abs/2403.15484)), ([:book:](https://browse.arxiv.org/pdf/2403.15484.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15484.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15484)), ([:house:](https://huggingface.co/papers/2403.15484)), ([HTML](https://browse.arxiv.org/html/2403.15484v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15484)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15484v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15484)), ([SS](https://api.semanticscholar.org/arXiv:2403.15484)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rakutenai-7b-extending-large-language-models))	
  * 03/21 - **MyVLM: Personalizing VLMs for User-Specific Queries** <br>([:x:](https://arxiv.org/abs/2403.14599)), ([:book:](https://browse.arxiv.org/pdf/2403.14599.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14599.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14599)), ([:house:](https://huggingface.co/papers/2403.14599)), ([HTML](https://browse.arxiv.org/html/2403.14599v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14599)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14599v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14599)), ([SS](https://api.semanticscholar.org/arXiv:2403.14599)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/myvlm-personalizing-vlms-for-user-specific))	
  * 03/21 - **MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?** <br>([:x:](https://arxiv.org/abs/2403.14624)), ([:book:](https://browse.arxiv.org/pdf/2403.14624.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14624.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14624)), ([:house:](https://huggingface.co/papers/2403.14624)), ([HTML](https://browse.arxiv.org/html/2403.14624v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14624)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14624v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14624)), ([SS](https://api.semanticscholar.org/arXiv:2403.14624)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mathverse-does-your-multi-modal-llm-truly-see))	
  * 03/21 - **GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation** <br>([:x:](https://arxiv.org/abs/2403.14621)), ([:book:](https://browse.arxiv.org/pdf/2403.14621.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14621.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14621)), ([:house:](https://huggingface.co/papers/2403.14621)), ([HTML](https://browse.arxiv.org/html/2403.14621v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14621)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14621v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14621)), ([SS](https://api.semanticscholar.org/arXiv:2403.14621)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/grm-large-gaussian-reconstruction-model-for)), ([:octocat:](https://github.com/justimyhxu/grm)![GitHub Repo stars](https://img.shields.io/github/stars/justimyhxu/grm?style=social))	
  * 03/21 - **General Assembly adopts landmark resolution on artificial intelligence** <br>  ([News](https://news.un.org/en/story/2024/03/1147831)), 	
  * 03/21 - **Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering** <br>([:x:](https://arxiv.org/abs/2403.14554)), ([:book:](https://browse.arxiv.org/pdf/2403.14554.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14554.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14554)), ([:house:](https://huggingface.co/papers/2403.14554)), ([HTML](https://browse.arxiv.org/html/2403.14554v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14554)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14554v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14554)), ([SS](https://api.semanticscholar.org/arXiv:2403.14554)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gaussian-frosting-editable-complex-radiance))	
  * 03/21 - **Explorative Inbetweening of Time and Space** <br>([:x:](https://arxiv.org/abs/2403.14611)), ([:book:](https://browse.arxiv.org/pdf/2403.14611.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14611.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14611)), ([:house:](https://huggingface.co/papers/2403.14611)), ([HTML](https://browse.arxiv.org/html/2403.14611v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14611)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14611v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14611)), ([SS](https://api.semanticscholar.org/arXiv:2403.14611)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/explorative-inbetweening-of-time-and-space))	
  * 03/21 - **Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition** <br>([:x:](https://arxiv.org/abs/2403.14148)), ([:book:](https://browse.arxiv.org/pdf/2403.14148.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14148.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14148)), ([:house:](https://huggingface.co/papers/2403.14148)), ([HTML](https://browse.arxiv.org/html/2403.14148v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14148)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14148v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14148)), ([SS](https://api.semanticscholar.org/arXiv:2403.14148)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficient-video-diffusion-models-via-content))	
  * 03/21 - **DreamReward: Text-to-3D Generation with Human Preference** <br>([:x:](https://arxiv.org/abs/2403.14613)), ([:book:](https://browse.arxiv.org/pdf/2403.14613.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14613.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14613)), ([:house:](https://huggingface.co/papers/2403.14613)), ([HTML](https://browse.arxiv.org/html/2403.14613v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14613)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14613v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14613)), ([SS](https://api.semanticscholar.org/arXiv:2403.14613)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dreamreward-text-to-3d-generation-with-human))	
  * 03/21 - **Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference** <br>([:x:](https://arxiv.org/abs/2403.14520)), ([:book:](https://browse.arxiv.org/pdf/2403.14520.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14520.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14520)), ([:house:](https://huggingface.co/papers/2403.14520)), ([HTML](https://browse.arxiv.org/html/2403.14520v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14520)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14520v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14520)), ([SS](https://api.semanticscholar.org/arXiv:2403.14520)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cobra-extending-mamba-to-multi-modal-large)), ([:octocat:](https://github.com/h-zhao1997/cobra)![GitHub Repo stars](https://img.shields.io/github/stars/h-zhao1997/cobra?style=social))	
  * 03/21 - **Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance** <br>([:x:](https://arxiv.org/abs/2403.14781)), ([:book:](https://browse.arxiv.org/pdf/2403.14781.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14781.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14781)), ([:house:](https://huggingface.co/papers/2403.14781)), ([HTML](https://browse.arxiv.org/html/2403.14781v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14781)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14781v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14781)), ([SS](https://api.semanticscholar.org/arXiv:2403.14781)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/champ-controllable-and-consistent-human-image)), ([:octocat:](https://github.com/fudan-generative-vision/champ)![GitHub Repo stars](https://img.shields.io/github/stars/fudan-generative-vision/champ?style=social))	
  * 03/21 - **AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks** <br>([:x:](https://arxiv.org/abs/2403.14468)), ([:book:](https://browse.arxiv.org/pdf/2403.14468.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14468.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14468)), ([:house:](https://huggingface.co/papers/2403.14468)), ([HTML](https://browse.arxiv.org/html/2403.14468v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14468)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14468v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14468)), ([SS](https://api.semanticscholar.org/arXiv:2403.14468)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/anyv2v-a-plug-and-play-framework-for-any))	
  * 03/20 - **Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal** <br>([:x:](https://arxiv.org/abs/2403.13309)), ([:book:](https://browse.arxiv.org/pdf/2403.13309.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13309.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13309)), ([:house:](https://huggingface.co/papers/2403.13309)), ([HTML](https://browse.arxiv.org/html/2403.13309v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13309)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13309v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13309)), ([SS](https://api.semanticscholar.org/arXiv:2403.13309)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mapping-llm-security-landscapes-a))
  * 03/20 - **ZigMa: Zigzag Mamba Diffusion Model** <br>([:x:](https://arxiv.org/abs/2403.13802)), ([:book:](https://browse.arxiv.org/pdf/2403.13802.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13802.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13802)), ([:house:](https://huggingface.co/papers/2403.13802)), ([HTML](https://browse.arxiv.org/html/2403.13802v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13802)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13802v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13802)), ([SS](https://api.semanticscholar.org/arXiv:2403.13802)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/zigma-zigzag-mamba-diffusion-model)), ([:octocat:](https://github.com/CompVis/zigma)![GitHub Repo stars](https://img.shields.io/github/stars/CompVis/zigma?style=social))	
  * 03/20 - **VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis** <br>([:x:](https://arxiv.org/abs/2403.13501)), ([:book:](https://browse.arxiv.org/pdf/2403.13501.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13501.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13501)), ([:house:](https://huggingface.co/papers/2403.13501)), ([HTML](https://browse.arxiv.org/html/2403.13501v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13501)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13501v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13501)), ([SS](https://api.semanticscholar.org/arXiv:2403.13501)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vstar-generative-temporal-nursing-for-longer))	
  * 03/20 - **RewardBench: Evaluating Reward Models for Language Modeling** <br>([:x:](https://arxiv.org/abs/2403.13787)), ([:book:](https://browse.arxiv.org/pdf/2403.13787.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13787.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13787)), ([:house:](https://huggingface.co/papers/2403.13787)), ([HTML](https://browse.arxiv.org/html/2403.13787v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13787)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13787v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13787)), ([SS](https://api.semanticscholar.org/arXiv:2403.13787)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rewardbench-evaluating-reward-models-for)), ([:octocat:](https://github.com/allenai/reward-bench)![GitHub Repo stars](https://img.shields.io/github/stars/allenai/reward-bench?style=social))	
  * 03/20 - **Reverse Training to Nurse the Reversal Curse** <br>([:x:](https://arxiv.org/abs/2403.13799)), ([:book:](https://browse.arxiv.org/pdf/2403.13799.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13799.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13799)), ([:house:](https://huggingface.co/papers/2403.13799)), ([HTML](https://browse.arxiv.org/html/2403.13799v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13799)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13799v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13799)), ([SS](https://api.semanticscholar.org/arXiv:2403.13799)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/reverse-training-to-nurse-the-reversal-curse))	
  * 03/20 - **RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS** <br>([:x:](https://arxiv.org/abs/2403.13806)), ([:book:](https://browse.arxiv.org/pdf/2403.13806.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13806.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13806)), ([:house:](https://huggingface.co/papers/2403.13806)), ([HTML](https://browse.arxiv.org/html/2403.13806v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13806)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13806v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13806)), ([SS](https://api.semanticscholar.org/arXiv:2403.13806)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/radsplat-radiance-field-informed-gaussian))	
  * 03/20 - **Mora: Enabling Generalist Video Generation via A Multi-Agent Framework** <br>([:x:](https://arxiv.org/abs/2403.13248)), ([:book:](https://browse.arxiv.org/pdf/2403.13248.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13248.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13248)), ([:house:](https://huggingface.co/papers/2403.13248)), ([HTML](https://browse.arxiv.org/html/2403.13248v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13248)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13248v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13248)), ([SS](https://api.semanticscholar.org/arXiv:2403.13248)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via)), ([:octocat:](https://github.com/lichao-sun/mora)![GitHub Repo stars](https://img.shields.io/github/stars/lichao-sun/mora?style=social))	
  * 03/20 - **LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models** <br>([:x:](https://arxiv.org/abs/2403.13372)), ([:book:](https://browse.arxiv.org/pdf/2403.13372.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13372.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13372)), ([:house:](https://huggingface.co/papers/2403.13372)), ([HTML](https://browse.arxiv.org/html/2403.13372v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13372)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13372v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13372)), ([SS](https://api.semanticscholar.org/arXiv:2403.13372)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via)), ([:octocat:](https://github.com/hiyouga/llama-factory)![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/llama-factory?style=social))	
  * 03/20 - **IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models** <br>([:x:](https://arxiv.org/abs/2403.13535)), ([:book:](https://browse.arxiv.org/pdf/2403.13535.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13535.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13535)), ([:house:](https://huggingface.co/papers/2403.13535)), ([HTML](https://browse.arxiv.org/html/2403.13535v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13535)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13535v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13535)), ([SS](https://api.semanticscholar.org/arXiv:2403.13535)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/idadapter-learning-mixed-features-for-tuning))	
  * 03/20 - **HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.13447)), ([:book:](https://browse.arxiv.org/pdf/2403.13447.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13447.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13447)), ([:house:](https://huggingface.co/papers/2403.13447)), ([HTML](https://browse.arxiv.org/html/2403.13447v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13447)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13447v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13447)), ([SS](https://api.semanticscholar.org/arXiv:2403.13447)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/hyperllava-dynamic-visual-and-language-expert)), ([:octocat:](https://github.com/dcdmllm/hyperllava)![GitHub Repo stars](https://img.shields.io/github/stars/dcdmllm/hyperllava?style=social))	
  * 03/20 - **Evaluating Frontier Models for Dangerous Capabilities** <br>([:x:](https://arxiv.org/abs/2403.13793)), ([:book:](https://browse.arxiv.org/pdf/2403.13793.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13793.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13793)), ([:house:](https://huggingface.co/papers/2403.13793)), ([HTML](https://browse.arxiv.org/html/2403.13793v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13793)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13793v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13793)), ([SS](https://api.semanticscholar.org/arXiv:2403.13793)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-frontier-models-for-dangerous))	
  * 03/20 - **DepthFM: Fast Monocular Depth Estimation with Flow Matching** <br>([:x:](https://arxiv.org/abs/2403.13788)), ([:book:](https://browse.arxiv.org/pdf/2403.13788.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13788.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13788)), ([:house:](https://huggingface.co/papers/2403.13788)), ([HTML](https://browse.arxiv.org/html/2403.13788v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13788)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13788v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13788)), ([SS](https://api.semanticscholar.org/arXiv:2403.13788)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/depthfm-fast-monocular-depth-estimation-with))	
  * 03/20 - **Compress3D: a Compressed Latent Space for 3D Generation from a Single Image** <br>([:x:](https://arxiv.org/abs/2403.13524)), ([:book:](https://browse.arxiv.org/pdf/2403.13524.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13524.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13524)), ([:house:](https://huggingface.co/papers/2403.13524)), ([HTML](https://browse.arxiv.org/html/2403.13524v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13524)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13524v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13524)), ([SS](https://api.semanticscholar.org/arXiv:2403.13524)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/compress3d-a-compressed-latent-space-for-3d))	
  * 03/20 - **Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation** <br>([:x:](https://arxiv.org/abs/2403.13745)), ([:book:](https://browse.arxiv.org/pdf/2403.13745.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13745.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13745)), ([:house:](https://huggingface.co/papers/2403.13745)), ([HTML](https://browse.arxiv.org/html/2403.13745v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13745)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13745v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13745)), ([SS](https://api.semanticscholar.org/arXiv:2403.13745)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/be-your-outpainter-mastering-video)), ([:octocat:](https://github.com/g-u-n/be-your-outpainter)![GitHub Repo stars](https://img.shields.io/github/stars/g-u-n/be-your-outpainter?style=social))	
  * 03/19 - **When Do We Not Need Larger Vision Models?** <br>([:x:](https://arxiv.org/abs/2403.13043)), ([:book:](https://browse.arxiv.org/pdf/2403.13043.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13043.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13043)), ([:house:](https://huggingface.co/papers/2403.13043)), ([HTML](https://browse.arxiv.org/html/2403.13043v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13043)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13043v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13043)), ([SS](https://api.semanticscholar.org/arXiv:2403.13043)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/when-do-we-not-need-larger-vision-models)), ([:octocat:](https://github.com/bfshi/scaling_on_scales)![GitHub Repo stars](https://img.shields.io/github/stars/bfshi/scaling_on_scales?style=social))	
  * 03/19 - **Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers** <br>([:x:](https://arxiv.org/abs/2403.12943)), ([:book:](https://browse.arxiv.org/pdf/2403.12943.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12943.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12943)), ([:house:](https://huggingface.co/papers/2403.12943)), ([HTML](https://browse.arxiv.org/html/2403.12943v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12943)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12943v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12943)), ([SS](https://api.semanticscholar.org/arXiv:2403.12943)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vid2robot-end-to-end-video-conditioned-policy))	
  * 03/19 - **Towards a general-purpose foundation model for computational pathology** <br>([:x:](https://www.nature.com/articles/s41591-024-02857-3)) 	
  * 03/19 - **TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation** <br>([:x:](https://arxiv.org/abs/2403.12906)), ([:book:](https://browse.arxiv.org/pdf/2403.12906.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12906.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12906)), ([:house:](https://huggingface.co/papers/2403.12906)), ([HTML](https://browse.arxiv.org/html/2403.12906v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12906)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12906v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12906)), ([SS](https://api.semanticscholar.org/arXiv:2403.12906)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/texdreamer-towards-zero-shot-high-fidelity-3d))	
  * 03/19 - **SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model** <br>([:x:](https://arxiv.org/abs/2403.13064)), ([:book:](https://browse.arxiv.org/pdf/2403.13064.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13064.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13064)), ([:house:](https://huggingface.co/papers/2403.13064)), ([HTML](https://browse.arxiv.org/html/2403.13064v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13064)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13064v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13064)), ([SS](https://api.semanticscholar.org/arXiv:2403.13064)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scenescript-reconstructing-scenes-with-an))	
  * 03/19 - **mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding** <br>([:x:](https://arxiv.org/abs/2403.12895)), ([:book:](https://browse.arxiv.org/pdf/2403.12895.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12895.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12895)), ([:house:](https://huggingface.co/papers/2403.12895)), ([HTML](https://browse.arxiv.org/html/2403.12895v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12895)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12895v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12895)), ([SS](https://api.semanticscholar.org/arXiv:2403.12895)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mplug-docowl-1-5-unified-structure-learning)), ([:octocat:](https://github.com/x-plug/mplug-docowl)![GitHub Repo stars](https://img.shields.io/github/stars/x-plug/mplug-docowl?style=social))	
  * 03/19 - **Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos** <br>([:x:](https://arxiv.org/abs/2403.13044)), ([:book:](https://browse.arxiv.org/pdf/2403.13044.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13044.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13044)), ([:house:](https://huggingface.co/papers/2403.13044)), ([HTML](https://browse.arxiv.org/html/2403.13044v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13044)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13044v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13044)), ([SS](https://api.semanticscholar.org/arXiv:2403.13044)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/magic-fixup-streamlining-photo-editing-by))	
  * 03/19 - **LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression** <br>([:x:](https://arxiv.org/abs/2403.12968)), ([:book:](https://browse.arxiv.org/pdf/2403.12968.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12968.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12968)), ([:house:](https://huggingface.co/papers/2403.12968)), ([HTML](https://browse.arxiv.org/html/2403.12968v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12968)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12968v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12968)), ([SS](https://api.semanticscholar.org/arXiv:2403.12968)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llmlingua-2-data-distillation-for-efficient)), ([:octocat:](https://github.com/microsoft/LLMLingua)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/LLMLingua?style=social))	
  * 03/19 - **GVGEN: Text-to-3D Generation with Volumetric Representation** <br>([:x:](https://arxiv.org/abs/2403.12957)), ([:book:](https://browse.arxiv.org/pdf/2403.12957.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12957.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12957)), ([:house:](https://huggingface.co/papers/2403.12957)), ([HTML](https://browse.arxiv.org/html/2403.12957v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12957)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12957v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12957)), ([SS](https://api.semanticscholar.org/arXiv:2403.12957)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gvgen-text-to-3d-generation-with-volumetric))	
  * 03/19 - **GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation** <br>([:x:](https://arxiv.org/abs/2403.12365)), ([:book:](https://browse.arxiv.org/pdf/2403.12365.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12365.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12365)), ([:house:](https://huggingface.co/papers/2403.12365)), ([HTML](https://browse.arxiv.org/html/2403.12365v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12365)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12365v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12365)), ([SS](https://api.semanticscholar.org/arXiv:2403.12365)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gaussianflow-splatting-gaussian-dynamics-for))	
  * 03/19 - **FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation** <br>([:x:](https://arxiv.org/abs/2403.12962)), ([:book:](https://browse.arxiv.org/pdf/2403.12962.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12962.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12962)), ([:house:](https://huggingface.co/papers/2403.12962)), ([HTML](https://browse.arxiv.org/html/2403.12962v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12962)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12962v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12962)), ([SS](https://api.semanticscholar.org/arXiv:2403.12962)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fresco-spatial-temporal-correspondence-for)), ([:octocat:](https://github.com/williamyang1991/fresco)![GitHub Repo stars](https://img.shields.io/github/stars/williamyang1991/fresco?style=social))	
  * 03/19 - **FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis** <br>([:x:](https://arxiv.org/abs/2403.12963)), ([:book:](https://browse.arxiv.org/pdf/2403.12963.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12963.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12963)), ([:house:](https://huggingface.co/papers/2403.12963)), ([HTML](https://browse.arxiv.org/html/2403.12963v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12963)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12963v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12963)), ([SS](https://api.semanticscholar.org/arXiv:2403.12963)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fouriscale-a-frequency-perspective-on)), ([:octocat:](https://github.com/leonhlj/fouriscale)![GitHub Repo stars](https://img.shields.io/github/stars/leonhlj/fouriscale?style=social))	
  * 03/19 - **Evolutionary Optimization of Model Merging Recipes** <br>([:x:](https://arxiv.org/abs/2403.13187)), ([:book:](https://browse.arxiv.org/pdf/2403.13187.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.13187.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.13187)), ([:house:](https://huggingface.co/papers/2403.13187)), ([HTML](https://browse.arxiv.org/html/2403.13187v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.13187)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.13187v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.13187)), ([SS](https://api.semanticscholar.org/arXiv:2403.13187)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evolutionary-optimization-of-model-merging)), ([:octocat:](https://github.com/ sakanaai/evolutionary-model-merge)![GitHub Repo stars](https://img.shields.io/github/stars/ sakanaai/evolutionary-model-merge?style=social))	
  * 03/19 - **ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance** <br>([:x:](https://arxiv.org/abs/2403.12409)), ([:book:](https://browse.arxiv.org/pdf/2403.12409.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12409.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12409)), ([:house:](https://huggingface.co/papers/2403.12409)), ([HTML](https://browse.arxiv.org/html/2403.12409v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12409)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12409v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12409)), ([SS](https://api.semanticscholar.org/arXiv:2403.12409)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/comboverse-compositional-3d-assets-creation))	
  * 03/19 - **Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs** <br>([:x:](https://arxiv.org/abs/2403.12596)), ([:book:](https://browse.arxiv.org/pdf/2403.12596.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12596.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12596)), ([:house:](https://huggingface.co/papers/2403.12596)), ([HTML](https://browse.arxiv.org/html/2403.12596v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12596)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12596v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12596)), ([SS](https://api.semanticscholar.org/arXiv:2403.12596)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chart-based-reasoning-transferring))	
  * 03/19 - **Apple's MM1: A multimodal large language model capable of interpreting both images and text data** <br>  ([News](https://techxplore.com/news/2024-03-apple-mm1-multimodal-llm-capable.html)), 	
  * 03/19 - **AnimateDiff-Lightning: Cross-Model Diffusion Distillation** <br>([:x:](https://arxiv.org/abs/2403.12706)), ([:book:](https://browse.arxiv.org/pdf/2403.12706.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12706.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12706)), ([:house:](https://huggingface.co/papers/2403.12706)), ([HTML](https://browse.arxiv.org/html/2403.12706v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12706)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12706v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12706)), ([SS](https://api.semanticscholar.org/arXiv:2403.12706)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/animatediff-lightning-cross-model-diffusion))	
  * 03/19 - **Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.12881)), ([:book:](https://browse.arxiv.org/pdf/2403.12881.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12881.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12881)), ([:house:](https://huggingface.co/papers/2403.12881)), ([HTML](https://browse.arxiv.org/html/2403.12881v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12881)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12881v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12881)), ([SS](https://api.semanticscholar.org/arXiv:2403.12881)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agent-flan-designing-data-and-methods-of)), ([:octocat:](https://github.com/internlm/agent-flan)![GitHub Repo stars](https://img.shields.io/github/stars/internlm/agent-flan?style=social))	
  * 03/19 - **A visual-language foundation model for computational pathology** <br>([:x:](https://www.nature.com/articles/s41591-024-02856-4)) , ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-a-visual-language-foundation-model))	
  * 03/19 - **Characteristic AI Agents via Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.12368)), ([:book:](https://browse.arxiv.org/pdf/2403.12368.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12368.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12368)), ([:house:](https://huggingface.co/papers/2403.12368)), ([HTML](https://browse.arxiv.org/html/2403.12368v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12368)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12368v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12368)), ([SS](https://api.semanticscholar.org/arXiv:2403.12368)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/characteristic-ai-agents-via-large-language)), ([:octocat:](https://github.com/nuaa-nlp/character100 )![GitHub Repo stars](https://img.shields.io/github/stars/nuaa-nlp/character100 ?style=social))
  * 03/18 - **How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments** <br>([:x:](https://arxiv.org/abs/2403.11807)), ([:book:](https://browse.arxiv.org/pdf/2403.11807.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.11807.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.11807)), ([:house:](https://huggingface.co/papers/2403.11807)), ([HTML](https://browse.arxiv.org/html/2403.11807v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.11807)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.11807v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.11807)), ([SS](https://api.semanticscholar.org/arXiv:2403.11807)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-far-are-we-on-the-decision-making-of-llms)), ([:octocat:](https://github.com/cuhk-arise/gamabench)![GitHub Repo stars](https://img.shields.io/github/stars/cuhk-arise/gamabench?style=social))
  * 03/18 - **VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding** <br>([:x:](https://arxiv.org/abs/2403.11481)), ([:book:](https://browse.arxiv.org/pdf/2403.11481.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.11481.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.11481)), ([:house:](https://huggingface.co/papers/2403.11481)), ([HTML](https://browse.arxiv.org/html/2403.11481v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.11481)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.11481v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.11481)), ([SS](https://api.semanticscholar.org/arXiv:2403.11481)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videoagent-a-memory-augmented-multimodal))	
  * 03/18 - **VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models** <br>([:x:](https://arxiv.org/abs/2403.12034)), ([:book:](https://browse.arxiv.org/pdf/2403.12034.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12034.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12034)), ([:house:](https://huggingface.co/papers/2403.12034)), ([HTML](https://browse.arxiv.org/html/2403.12034v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12034)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12034v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12034)), ([SS](https://api.semanticscholar.org/arXiv:2403.12034)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vfusion3d-learning-scalable-3d-generative))	
  * 03/18 - **TnT-LLM: Text Mining at Scale with Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.12173)), ([:book:](https://browse.arxiv.org/pdf/2403.12173.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12173.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12173)), ([:house:](https://huggingface.co/papers/2403.12173)), ([HTML](https://browse.arxiv.org/html/2403.12173v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12173)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12173v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12173)), ([SS](https://api.semanticscholar.org/arXiv:2403.12173)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tnt-llm-text-mining-at-scale-with-large))	
  * 03/18 - **SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion** <br>([:x:](https://arxiv.org/abs/2403.12008)), ([:book:](https://browse.arxiv.org/pdf/2403.12008.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12008.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12008)), ([:house:](https://huggingface.co/papers/2403.12008)), ([HTML](https://browse.arxiv.org/html/2403.12008v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12008)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12008v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12008)), ([SS](https://api.semanticscholar.org/arXiv:2403.12008)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sv3d-novel-multi-view-synthesis-and-3d))	
  * 03/18 - **ROUTERBENCH: A Benchmark for Multi-LLM Routing System** <br>([:x:](https://arxiv.org/abs/2403.12031)), ([:book:](https://browse.arxiv.org/pdf/2403.12031.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12031.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12031)), ([:house:](https://huggingface.co/papers/2403.12031)), ([HTML](https://browse.arxiv.org/html/2403.12031v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12031)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12031v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12031)), ([SS](https://api.semanticscholar.org/arXiv:2403.12031)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/routerbench-a-benchmark-for-multi-llm-routing)), ([SS](withmartian/routerbench))	
  * 03/18 - **Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs** <br>([:x:](https://arxiv.org/abs/2403.11755)), ([:book:](https://browse.arxiv.org/pdf/2403.11755.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.11755.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.11755)), ([:house:](https://huggingface.co/papers/2403.11755)), ([HTML](https://browse.arxiv.org/html/2403.11755v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.11755)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.11755v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.11755)), ([SS](https://api.semanticscholar.org/arXiv:2403.11755)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/meta-prompting-for-automating-zero-shot)), ([:octocat:](https://github.com/jmiemirza/meta-prompting)![GitHub Repo stars](https://img.shields.io/github/stars/jmiemirza/meta-prompting?style=social))	
  * 03/18 - **LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation** <br>([:x:](https://arxiv.org/abs/2403.12019)), ([:book:](https://browse.arxiv.org/pdf/2403.12019.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12019.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12019)), ([:house:](https://huggingface.co/papers/2403.12019)), ([HTML](https://browse.arxiv.org/html/2403.12019v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12019)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12019v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12019)), ([SS](https://api.semanticscholar.org/arXiv:2403.12019)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ln3diff-scalable-latent-neural-fields))	
  * 03/18 - **LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images** <br>([:x:](https://arxiv.org/abs/2403.11703)), ([:book:](https://browse.arxiv.org/pdf/2403.11703.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.11703.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.11703)), ([:house:](https://huggingface.co/papers/2403.11703)), ([HTML](https://browse.arxiv.org/html/2403.11703v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.11703)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.11703v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.11703)), ([SS](https://api.semanticscholar.org/arXiv:2403.11703)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio)), ([:octocat:](https://github.com/thunlp/llava-uhd)![GitHub Repo stars](https://img.shields.io/github/stars/thunlp/llava-uhd?style=social))	
  * 03/18 - **Larimar: Large Language Models with Episodic Memory Control** <br>([:x:](https://arxiv.org/abs/2403.11901)), ([:book:](https://browse.arxiv.org/pdf/2403.11901.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.11901.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.11901)), ([:house:](https://huggingface.co/papers/2403.11901)), ([HTML](https://browse.arxiv.org/html/2403.11901v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.11901)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.11901v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.11901)), ([SS](https://api.semanticscholar.org/arXiv:2403.11901)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/larimar-large-language-models-with-episodic))	
  * 03/18 - **Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm** <br>([:x:](https://arxiv.org/abs/2403.11781)), ([:book:](https://browse.arxiv.org/pdf/2403.11781.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.11781.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.11781)), ([:house:](https://huggingface.co/papers/2403.11781)), ([HTML](https://browse.arxiv.org/html/2403.11781v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.11781)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.11781v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.11781)), ([SS](https://api.semanticscholar.org/arXiv:2403.11781)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/infinite-id-identity-preserved))	
  * 03/18 - **GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture** <br>([:x:](https://arxiv.org/abs/2403.11858)), ([:book:](https://browse.arxiv.org/pdf/2403.11858.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.11858.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.11858)), ([:house:](https://huggingface.co/papers/2403.11858)), ([HTML](https://browse.arxiv.org/html/2403.11858v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.11858)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.11858v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.11858)), ([SS](https://api.semanticscholar.org/arXiv:2403.11858)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gpt-4-as-evaluator-evaluating-large-language))	
  * 03/18 - **Generic 3D Diffusion Adapter Using Controlled Multi-View Editing** <br>([:x:](https://arxiv.org/abs/2403.12032)), ([:book:](https://browse.arxiv.org/pdf/2403.12032.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12032.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12032)), ([:house:](https://huggingface.co/papers/2403.12032)), ([HTML](https://browse.arxiv.org/html/2403.12032v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12032)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12032v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12032)), ([SS](https://api.semanticscholar.org/arXiv:2403.12032)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/generic-3d-diffusion-adapter-using-controlled)), ([:octocat:](https://github.com/Lakonik/MVEdit)![GitHub Repo stars](https://img.shields.io/github/stars/Lakonik/MVEdit?style=social))	
  * 03/18 - **From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models** <br>([:x:](https://arxiv.org/abs/2403.12027)), ([:book:](https://browse.arxiv.org/pdf/2403.12027.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12027.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12027)), ([:house:](https://huggingface.co/papers/2403.12027)), ([HTML](https://browse.arxiv.org/html/2403.12027v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12027)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12027v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12027)), ([SS](https://api.semanticscholar.org/arXiv:2403.12027)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/from-pixels-to-insights-a-survey-on-automatic)), ([:octocat:](https://github.com/khuangaf/awesome-chart-understanding)![GitHub Repo stars](https://img.shields.io/github/stars/khuangaf/awesome-chart-understanding?style=social))	
  * 03/18 - **Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation** <br>([:x:](https://arxiv.org/abs/2403.12015)), ([:book:](https://browse.arxiv.org/pdf/2403.12015.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.12015.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.12015)), ([:house:](https://huggingface.co/papers/2403.12015)), ([HTML](https://browse.arxiv.org/html/2403.12015v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.12015)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.12015v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.12015)), ([SS](https://api.semanticscholar.org/arXiv:2403.12015)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fast-high-resolution-image-synthesis-with))	
  * 03/18 - **Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression** <br>([:x:](https://arxiv.org/abs/2403.15447)), ([:book:](https://browse.arxiv.org/pdf/2403.15447.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.15447.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.15447)), ([:house:](https://huggingface.co/papers/2403.15447)), ([HTML](https://browse.arxiv.org/html/2403.15447v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.15447)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.15447v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.15447)), ([SS](https://api.semanticscholar.org/arXiv:2403.15447)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/decoding-compressed-trust-scrutinizing-the))	
  * 03/18 - **Compiler generated feedback for Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.14714)), ([:book:](https://browse.arxiv.org/pdf/2403.14714.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.14714.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.14714)), ([:house:](https://huggingface.co/papers/2403.14714)), ([HTML](https://browse.arxiv.org/html/2403.14714v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.14714)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.14714v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.14714)), ([SS](https://api.semanticscholar.org/arXiv:2403.14714)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/compiler-generated-feedback-for-large))	
  * 03/17 - **PhD: A Prompted Visual Hallucination Evaluation Dataset** <br>([:x:](https://arxiv.org/abs/2403.11116)), ([:book:](https://browse.arxiv.org/pdf/2403.11116.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.11116.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.11116)), ([:house:](https://huggingface.co/papers/2403.11116)), ([HTML](https://browse.arxiv.org/html/2403.11116v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.11116)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.11116v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.11116)), ([SS](https://api.semanticscholar.org/arXiv:2403.11116)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/phd-a-prompted-visual-hallucination))	
  * 03/17 - **MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data** <br>([:x:](https://arxiv.org/abs/2403.11207)), ([:book:](https://browse.arxiv.org/pdf/2403.11207.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.11207.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.11207)), ([:house:](https://huggingface.co/papers/2403.11207)), ([HTML](https://browse.arxiv.org/html/2403.11207v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.11207)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.11207v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.11207)), ([SS](https://api.semanticscholar.org/arXiv:2403.11207)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mindeye2-shared-subject-models-enable-fmri-to)), ([:octocat:](https://github.com/medarc-ai/mindeyev2)![GitHub Repo stars](https://img.shields.io/github/stars/medarc-ai/mindeyev2?style=social))	
  * 03/16 - **VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis** <br>([:x:](https://arxiv.org/abs/2403.10823)), ([:book:](https://browse.arxiv.org/pdf/2403.10823.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10823.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10823)), ([:house:](https://huggingface.co/papers/2403.10823)), ([HTML](https://browse.arxiv.org/html/2403.10823v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10823)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10823v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10823)), ([SS](https://api.semanticscholar.org/arXiv:2403.10823)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/visionclip-an-med-aigc-based-ethical-language))	
  * 03/16 - **Do Large Language Models understand Medical Codes?** <br>([:x:](https://arxiv.org/abs/2403.10822)), ([:book:](https://browse.arxiv.org/pdf/2403.10822.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10822.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10822)), ([:house:](https://huggingface.co/papers/2403.10822)), ([HTML](https://browse.arxiv.org/html/2403.10822v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10822)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10822v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10822)), ([SS](https://api.semanticscholar.org/arXiv:2403.10822)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/do-large-language-models-understand-medical))	
  * 03/15 - **VideoAgent: Long-form Video Understanding with Large Language Model as Agent** <br>([:x:](https://arxiv.org/abs/2403.10517)), ([:book:](https://browse.arxiv.org/pdf/2403.10517.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10517.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10517)), ([:house:](https://huggingface.co/papers/2403.10517)), ([HTML](https://browse.arxiv.org/html/2403.10517v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10517)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10517v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10517)), ([SS](https://api.semanticscholar.org/arXiv:2403.10517)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videoagent-long-form-video-understanding-with))	
  * 03/15 - **Uni-SMART: Universal Science Multimodal Analysis and Research Transformer** <br>([:x:](https://arxiv.org/abs/2403.10301)), ([:book:](https://browse.arxiv.org/pdf/2403.10301.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10301.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10301)), ([:house:](https://huggingface.co/papers/2403.10301)), ([HTML](https://browse.arxiv.org/html/2403.10301v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10301)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10301v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10301)), ([SS](https://api.semanticscholar.org/arXiv:2403.10301)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/uni-smart-universal-science-multimodal))	
  * 03/15 - **Trusting the Search: Unraveling Human Trust in Health Information from Google and ChatGPT** <br>([:x:](https://arxiv.org/abs/2403.09987)), ([:book:](https://browse.arxiv.org/pdf/2403.09987.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09987.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09987)), ([:house:](https://huggingface.co/papers/2403.09987)), ([HTML](https://browse.arxiv.org/html/2403.09987v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09987)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09987v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09987)), ([SS](https://api.semanticscholar.org/arXiv:2403.09987))	
  * 03/15 - **RAFT: Adapting Language Model to Domain Specific RAG** <br>([:x:](https://arxiv.org/abs/2403.10131)), ([:book:](https://browse.arxiv.org/pdf/2403.10131.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10131.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10131)), ([:house:](https://huggingface.co/papers/2403.10131)), ([HTML](https://browse.arxiv.org/html/2403.10131v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10131)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10131v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10131)), ([SS](https://api.semanticscholar.org/arXiv:2403.10131)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/raft-adapting-language-model-to-domain)), ([:octocat:](https://github.com/ShishirPatil/gorilla)![GitHub Repo stars](https://img.shields.io/github/stars/ShishirPatil/gorilla?style=social))	
  * 03/15 - **RAFT: Adapting Language Model to Domain Specific RAG** <br>([:x:](https://arxiv.org/abs/2403.10131)), ([:book:](https://browse.arxiv.org/pdf/2403.10131.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10131.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10131)), ([:house:](https://huggingface.co/papers/2403.10131)), ([HTML](https://browse.arxiv.org/html/2403.10131v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10131)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10131v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10131)), ([SS](https://api.semanticscholar.org/arXiv:2403.10131)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/raft-adapting-language-model-to-domain))	
  * 03/15 - **PERL: Parameter Efficient Reinforcement Learning from Human Feedback** <br>([:x:](https://arxiv.org/abs/2403.10704)), ([:book:](https://browse.arxiv.org/pdf/2403.10704.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10704.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10704)), ([:house:](https://huggingface.co/papers/2403.10704)), ([HTML](https://browse.arxiv.org/html/2403.10704v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10704)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10704v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10704)), ([SS](https://api.semanticscholar.org/arXiv:2403.10704)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/perl-parameter-efficient-reinforcement))	
  * 03/15 - **NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices** <br>([:x:](https://arxiv.org/abs/2403.10425)), ([:book:](https://browse.arxiv.org/pdf/2403.10425.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10425.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10425)), ([:house:](https://huggingface.co/papers/2403.10425)), ([HTML](https://browse.arxiv.org/html/2403.10425v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10425)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10425v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10425)), ([SS](https://api.semanticscholar.org/arXiv:2403.10425)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/neuflow-real-time-high-accuracy-optical-flow)), ([:octocat:](https://github.com/neufieldrobotics/neuflow)![GitHub Repo stars](https://img.shields.io/github/stars/neufieldrobotics/neuflow?style=social))	
  * 03/15 - **MusicHiFi: Fast High-Fidelity Stereo Vocoding** <br>([:x:](https://arxiv.org/abs/2403.10493)), ([:book:](https://browse.arxiv.org/pdf/2403.10493.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10493.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10493)), ([:house:](https://huggingface.co/papers/2403.10493)), ([HTML](https://browse.arxiv.org/html/2403.10493v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10493)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10493v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10493)), ([SS](https://api.semanticscholar.org/arXiv:2403.10493))	
  * 03/15 - **LightIt: Illumination Modeling and Control for Diffusion Models** <br>([:x:](https://arxiv.org/abs/2403.10615)), ([:book:](https://browse.arxiv.org/pdf/2403.10615.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10615.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10615)), ([:house:](https://huggingface.co/papers/2403.10615)), ([HTML](https://browse.arxiv.org/html/2403.10615v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10615)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10615v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10615)), ([SS](https://api.semanticscholar.org/arXiv:2403.10615)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/lightit-illumination-modeling-and-control-for))	
  * 03/15 - **Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding** <br>([:x:](https://arxiv.org/abs/2403.10395)), ([:book:](https://browse.arxiv.org/pdf/2403.10395.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10395.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10395)), ([:house:](https://huggingface.co/papers/2403.10395)), ([HTML](https://browse.arxiv.org/html/2403.10395v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10395)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10395v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10395)), ([SS](https://api.semanticscholar.org/arXiv:2403.10395)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/isotropic3d-image-to-3d-generation-based-on-a)), ([:octocat:](https://github.com/pkunliu/isotropic3d)![GitHub Repo stars](https://img.shields.io/github/stars/pkunliu/isotropic3d?style=social))	
  * 03/15 - **FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model** <br>([:x:](https://arxiv.org/abs/2403.10242)), ([:book:](https://browse.arxiv.org/pdf/2403.10242.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10242.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10242)), ([:house:](https://huggingface.co/papers/2403.10242)), ([HTML](https://browse.arxiv.org/html/2403.10242v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10242)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10242v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10242)), ([SS](https://api.semanticscholar.org/arXiv:2403.10242)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fdgaussian-fast-gaussian-splatting-from))	
  * 03/15 - **Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning** <br>([:x:](https://arxiv.org/abs/2403.10107)), ([:book:](https://browse.arxiv.org/pdf/2403.10107.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10107.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10107)), ([:house:](https://huggingface.co/papers/2403.10107)), ([HTML](https://browse.arxiv.org/html/2403.10107v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10107)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10107v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10107)), ([SS](https://api.semanticscholar.org/arXiv:2403.10107)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-human-centered-dynamic-scene))	
  * 03/15 - **EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba** <br>([:x:](https://arxiv.org/abs/2403.09977)), ([:book:](https://browse.arxiv.org/pdf/2403.09977.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09977.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09977)), ([:house:](https://huggingface.co/papers/2403.09977)), ([HTML](https://browse.arxiv.org/html/2403.09977v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09977)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09977v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09977)), ([SS](https://api.semanticscholar.org/arXiv:2403.09977)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/efficientvmamba-atrous-selective-scan-for))	
  * 03/15 - **DiPaCo: Distributed Path Composition** <br>([:x:](https://arxiv.org/abs/2403.10616)), ([:book:](https://browse.arxiv.org/pdf/2403.10616.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.10616.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.10616)), ([:house:](https://huggingface.co/papers/2403.10616)), ([HTML](https://browse.arxiv.org/html/2403.10616v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.10616)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.10616v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.10616)), ([SS](https://api.semanticscholar.org/arXiv:2403.10616)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dipaco-distributed-path-composition))	
  * 03/15 - **Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting** <br>([:x:](https://arxiv.org/abs/2403.09981)), ([:book:](https://browse.arxiv.org/pdf/2403.09981.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09981.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09981)), ([:house:](https://huggingface.co/papers/2403.09981)), ([HTML](https://browse.arxiv.org/html/2403.09981v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09981)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09981v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09981)), ([SS](https://api.semanticscholar.org/arXiv:2403.09981)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/controllable-text-to-3d-generation-via))	
  * 03/14 - **WavCraft: Audio Editing and Generation with Natural Language Prompts** <br>([:x:](https://arxiv.org/abs/2403.09527)), ([:book:](https://browse.arxiv.org/pdf/2403.09527.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09527.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09527)), ([:house:](https://huggingface.co/papers/2403.09527)), ([HTML](https://browse.arxiv.org/html/2403.09527v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09527)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09527v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09527)), ([SS](https://api.semanticscholar.org/arXiv:2403.09527)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wavcraft-audio-editing-and-generation-with)), ([:octocat:](https://github.com/jinhualiang/wavcraft)![GitHub Repo stars](https://img.shields.io/github/stars/jinhualiang/wavcraft?style=social))	
  * 03/14 - **VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding** <br>([:x:](https://arxiv.org/abs/2403.09530)), ([:book:](https://browse.arxiv.org/pdf/2403.09530.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09530.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09530)), ([:house:](https://huggingface.co/papers/2403.09530)), ([HTML](https://browse.arxiv.org/html/2403.09530v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09530)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09530v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09530)), ([SS](https://api.semanticscholar.org/arXiv:2403.09530)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/visiongpt-3d-a-generalized-multimodal-agent)), ([:octocat:](https://github.com/superagi/veagle)![GitHub Repo stars](https://img.shields.io/github/stars/superagi/veagle?style=social))	
  * 03/14 - **Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding** <br>([:x:](https://arxiv.org/abs/2403.09626)), ([:book:](https://browse.arxiv.org/pdf/2403.09626.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09626.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09626)), ([:house:](https://huggingface.co/papers/2403.09626)), ([HTML](https://browse.arxiv.org/html/2403.09626v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09626)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09626v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09626)), ([SS](https://api.semanticscholar.org/arXiv:2403.09626)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video-mamba-suite-state-space-model-as-a)), ([:octocat:](https://github.com/opengvlab/video-mamba-suite)![GitHub Repo stars](https://img.shields.io/github/stars/opengvlab/video-mamba-suite?style=social))	
  * 03/14 - **Video Editing via Factorized Diffusion Distillation** <br>([:x:](https://arxiv.org/abs/2403.09334)), ([:book:](https://browse.arxiv.org/pdf/2403.09334.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09334.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09334)), ([:house:](https://huggingface.co/papers/2403.09334)), ([HTML](https://browse.arxiv.org/html/2403.09334v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09334)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09334v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09334)), ([SS](https://api.semanticscholar.org/arXiv:2403.09334)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video-editing-via-factorized-diffusion))	
  * 03/14 - **Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset** <br>([:x:](https://arxiv.org/abs/2403.09029)), ([:book:](https://browse.arxiv.org/pdf/2403.09029.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09029.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09029)), ([:house:](https://huggingface.co/papers/2403.09029)), ([HTML](https://browse.arxiv.org/html/2403.09029v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09029)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09029v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09029)), ([SS](https://api.semanticscholar.org/arXiv:2403.09029)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/unlocking-the-conversion-of-web-screenshots))	
  * 03/14 - **StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control** <br>([:x:](https://arxiv.org/abs/2403.09055)), ([:book:](https://browse.arxiv.org/pdf/2403.09055.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09055.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09055)), ([:house:](https://huggingface.co/papers/2403.09055)), ([HTML](https://browse.arxiv.org/html/2403.09055v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09055)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09055v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09055)), ([SS](https://api.semanticscholar.org/arXiv:2403.09055)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/streammultidiffusion-real-time-interactive)), ([:octocat:](https://github.com/ironjr/streammultidiffusion)![GitHub Repo stars](https://img.shields.io/github/stars/ironjr/streammultidiffusion?style=social))	
  * 03/14 - **Scaling Instructable Agents Across Many Simulated Worlds** <br>  ([twitter](https://twitter.com/fablesimulation/status/1767988371828388027)),  ([Blog](https://gonzoml.substack.com/p/deepmind-sima-scaling-instructable)), 	
  * 03/14 - **Recurrent Drafter for Fast Speculative Decoding in Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.09919)), ([:book:](https://browse.arxiv.org/pdf/2403.09919.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09919.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09919)), ([:house:](https://huggingface.co/papers/2403.09919)), ([HTML](https://browse.arxiv.org/html/2403.09919v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09919)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09919v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09919)), ([SS](https://api.semanticscholar.org/arXiv:2403.09919)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/recurrent-drafter-for-fast-speculative))	
  * 03/14 - **Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking** <br>([:x:](https://arxiv.org/abs/2403.09629)), ([:book:](https://browse.arxiv.org/pdf/2403.09629.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09629.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09629)), ([:house:](https://huggingface.co/papers/2403.09629)), ([HTML](https://browse.arxiv.org/html/2403.09629v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09629)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09629v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09629)), ([SS](https://api.semanticscholar.org/arXiv:2403.09629)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/quiet-star-language-models-can-teach)), ([:octocat:](https://github.com/ezelikman/quiet-star)![GitHub Repo stars](https://img.shields.io/github/stars/ezelikman/quiet-star?style=social))	
  * 03/14 - **MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training** <br>([:x:](https://arxiv.org/abs/2403.09611)), ([:book:](https://browse.arxiv.org/pdf/2403.09611.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09611.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09611)), ([:house:](https://huggingface.co/papers/2403.09611)), ([HTML](https://browse.arxiv.org/html/2403.09611v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09611)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09611v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09611)), ([SS](https://api.semanticscholar.org/arXiv:2403.09611)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mm1-methods-analysis-insights-from-multimodal))	
  * 03/14 - **LocalMamba: Visual State Space Model with Windowed Selective Scan** <br>([:x:](https://arxiv.org/abs/2403.09338)), ([:book:](https://browse.arxiv.org/pdf/2403.09338.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09338.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09338)), ([:house:](https://huggingface.co/papers/2403.09338)), ([HTML](https://browse.arxiv.org/html/2403.09338v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09338)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09338v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09338)), ([SS](https://api.semanticscholar.org/arXiv:2403.09338)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/localmamba-visual-state-space-model-with)), ([:octocat:](https://github.com/hunto/localmamba)![GitHub Repo stars](https://img.shields.io/github/stars/hunto/localmamba?style=social))	
  * 03/14 - **Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey** <br>([:x:](https://arxiv.org/abs/2403.09606)), ([:book:](https://browse.arxiv.org/pdf/2403.09606.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09606.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09606)), ([:house:](https://huggingface.co/papers/2403.09606)), ([HTML](https://browse.arxiv.org/html/2403.09606v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09606)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09606v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09606)), ([SS](https://api.semanticscholar.org/arXiv:2403.09606)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-and-causal-inference-in))	
  * 03/14 - **Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention** <br>([:x:](https://arxiv.org/abs/2403.09795)), ([:book:](https://browse.arxiv.org/pdf/2403.09795.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09795.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09795)), ([:house:](https://huggingface.co/papers/2403.09795)), ([HTML](https://browse.arxiv.org/html/2403.09795v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09795)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09795v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09795)), ([SS](https://api.semanticscholar.org/arXiv:2403.09795)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/helpful-or-harmful-exploring-the-efficacy-of))	
  * 03/14 - **Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring** <br>([:x:](https://arxiv.org/abs/2403.09333)), ([:book:](https://browse.arxiv.org/pdf/2403.09333.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09333.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09333)), ([:house:](https://huggingface.co/papers/2403.09333)), ([HTML](https://browse.arxiv.org/html/2403.09333v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09333)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09333v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09333)), ([SS](https://api.semanticscholar.org/arXiv:2403.09333)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/griffon-v2-advancing-multimodal-perception)), ([:octocat:](https://github.com/jefferyzhan/griffon)![GitHub Repo stars](https://img.shields.io/github/stars/jefferyzhan/griffon?style=social))	
  * 03/14 - **GPT on a Quantum Computer** <br>([:x:](https://arxiv.org/abs/2403.09418)), ([:book:](https://browse.arxiv.org/pdf/2403.09418.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09418.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09418)), ([:house:](https://huggingface.co/papers/2403.09418)), ([HTML](https://browse.arxiv.org/html/2403.09418v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09418)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09418v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09418)), ([SS](https://api.semanticscholar.org/arXiv:2403.09418))	
  * 03/14 - **Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering** <br>([:x:](https://arxiv.org/abs/2403.09622)), ([:book:](https://browse.arxiv.org/pdf/2403.09622.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09622.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09622)), ([:house:](https://huggingface.co/papers/2403.09622)), ([HTML](https://browse.arxiv.org/html/2403.09622v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09622)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09622v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09622)), ([SS](https://api.semanticscholar.org/arXiv:2403.09622)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/glyph-byt5-a-customized-text-encoder-for))	
  * 03/14 - **GiT: Towards Generalist Vision Transformer through Universal Language Interface** <br>([:x:](https://arxiv.org/abs/2403.09394)), ([:book:](https://browse.arxiv.org/pdf/2403.09394.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09394.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09394)), ([:house:](https://huggingface.co/papers/2403.09394)), ([HTML](https://browse.arxiv.org/html/2403.09394v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09394)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09394v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09394)), ([SS](https://api.semanticscholar.org/arXiv:2403.09394)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/git-towards-generalist-vision-transformer)), ([:octocat:](https://github.com/haiyang-w/git)![GitHub Repo stars](https://img.shields.io/github/stars/haiyang-w/git?style=social))	
  * 03/14 - **Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector** <br>([:x:](https://arxiv.org/abs/2403.09125)), ([:book:](https://browse.arxiv.org/pdf/2403.09125.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09125.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09125)), ([:house:](https://huggingface.co/papers/2403.09125)), ([HTML](https://browse.arxiv.org/html/2403.09125v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09125)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09125v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09125)), ([SS](https://api.semanticscholar.org/arXiv:2403.09125)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-the-capabilities-and-limitations-of))	
  * 03/14 - **BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences** <br>([:x:](https://arxiv.org/abs/2403.09347)), ([:book:](https://browse.arxiv.org/pdf/2403.09347.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09347.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09347)), ([:house:](https://huggingface.co/papers/2403.09347)), ([HTML](https://browse.arxiv.org/html/2403.09347v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09347)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09347v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09347)), ([SS](https://api.semanticscholar.org/arXiv:2403.09347)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/burstattention-an-efficient-distributed)), ([:octocat:](https://github.com/MayDomine/Burst-Attention)![GitHub Repo stars](https://img.shields.io/github/stars/MayDomine/Burst-Attention?style=social))	
  * 03/14 - **3D-VLA: A 3D Vision-Language-Action Generative World Model** <br>([:x:](https://arxiv.org/abs/2403.09631)), ([:book:](https://browse.arxiv.org/pdf/2403.09631.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09631.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09631)), ([:house:](https://huggingface.co/papers/2403.09631)), ([HTML](https://browse.arxiv.org/html/2403.09631v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09631)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09631v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09631)), ([SS](https://api.semanticscholar.org/arXiv:2403.09631)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3d-vla-a-3d-vision-language-action-generative))
  * 03/13 - **Scaling Instructable Agents Across Many Simulated Worlds** <br>([:x:](https://arxiv.org/abs/2404.10179)), ([:book:](https://browse.arxiv.org/pdf/2404.10179.pdf)), ([:paperclip:](https://arxiv.org/pdf/2404.10179.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2404.10179)), ([:house:](https://huggingface.co/papers/2404.10179)), ([HTML](https://browse.arxiv.org/html/2404.10179v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2404.10179)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2404.10179v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2404.10179)), ([SS](https://api.semanticscholar.org/arXiv:2404.10179))  
  * 03/13 - **VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis** <br>([:x:](https://arxiv.org/abs/2403.08764)), ([:book:](https://browse.arxiv.org/pdf/2403.08764.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08764.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08764)), ([:house:](https://huggingface.co/papers/2403.08764)), ([HTML](https://browse.arxiv.org/html/2403.08764v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08764)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08764v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08764)), ([SS](https://api.semanticscholar.org/arXiv:2403.08764)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vlogger-multimodal-diffusion-for-embodied))	
  * 03/13 - **The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions** <br>([:x:](https://arxiv.org/abs/2403.09743)), ([:book:](https://browse.arxiv.org/pdf/2403.09743.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.09743.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.09743)), ([:house:](https://huggingface.co/papers/2403.09743)), ([HTML](https://browse.arxiv.org/html/2403.09743v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.09743)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.09743v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.09743)), ([SS](https://api.semanticscholar.org/arXiv:2403.09743)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-human-factor-in-detecting-errors-of-large))	
  * 03/13 - **SOTOPIA-π: Interactive Learning of Socially Intelligent Language Agents** <br>([:x:](https://arxiv.org/abs/2403.08715)), ([:book:](https://browse.arxiv.org/pdf/2403.08715.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08715.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08715)), ([:house:](https://huggingface.co/papers/2403.08715)), ([HTML](https://browse.arxiv.org/html/2403.08715v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08715)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08715v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08715)), ([SS](https://api.semanticscholar.org/arXiv:2403.08715)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sotopia-p-interactive-learning-of-socially)), ([:octocat:](https://github.com/sotopia-lab/sotopia-pi)![GitHub Repo stars](https://img.shields.io/github/stars/sotopia-lab/sotopia-pi?style=social))	
  * 03/13 - **Simple and Scalable Strategies to Continually Pre-train Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.08763)), ([:book:](https://browse.arxiv.org/pdf/2403.08763.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08763.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08763)), ([:house:](https://huggingface.co/papers/2403.08763)), ([HTML](https://browse.arxiv.org/html/2403.08763v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08763)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08763v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08763)), ([SS](https://api.semanticscholar.org/arXiv:2403.08763)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/simple-and-scalable-strategies-to-continually)), ([:octocat:](https://github.com/eleutherai/gpt-neox)![GitHub Repo stars](https://img.shields.io/github/stars/eleutherai/gpt-neox?style=social))	
  * 03/13 - **Scaling Up Dynamic Human-Scene Interaction Modeling** <br>([:x:](https://arxiv.org/abs/2403.08629)), ([:book:](https://browse.arxiv.org/pdf/2403.08629.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08629.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08629)), ([:house:](https://huggingface.co/papers/2403.08629)), ([HTML](https://browse.arxiv.org/html/2403.08629v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08629)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08629v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08629)), ([SS](https://api.semanticscholar.org/arXiv:2403.08629)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaling-up-dynamic-human-scene-interaction))	
  * 03/13 - **Language-based game theory in the age of artificial intelligence** <br>([:x:](https://arxiv.org/abs/2403.08944)), ([:book:](https://browse.arxiv.org/pdf/2403.08944.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08944.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08944)), ([:house:](https://huggingface.co/papers/2403.08944)), ([HTML](https://browse.arxiv.org/html/2403.08944v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08944)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08944v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08944)), ([SS](https://api.semanticscholar.org/arXiv:2403.08944)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-based-game-theory-in-the-age-of))	
  * 03/13 - **Language models scale reliably with over-training and on downstream tasks** <br>([:x:](https://arxiv.org/abs/2403.08540)), ([:book:](https://browse.arxiv.org/pdf/2403.08540.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08540.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08540)), ([:house:](https://huggingface.co/papers/2403.08540)), ([HTML](https://browse.arxiv.org/html/2403.08540v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08540)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08540v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08540)), ([SS](https://api.semanticscholar.org/arXiv:2403.08540)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/language-models-scale-reliably-with-over)), ([:octocat:](https://github.com/mlfoundations/scaling)![GitHub Repo stars](https://img.shields.io/github/stars/mlfoundations/scaling?style=social))	
  * 03/13 - **Knowledge Conflicts for LLMs: A Survey** <br>([:x:](https://arxiv.org/abs/2403.08319)), ([:book:](https://browse.arxiv.org/pdf/2403.08319.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08319.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08319)), ([:house:](https://huggingface.co/papers/2403.08319)), ([HTML](https://browse.arxiv.org/html/2403.08319v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08319)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08319v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08319)), ([SS](https://api.semanticscholar.org/arXiv:2403.08319)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/knowledge-conflicts-for-llms-a-survey))	
  * 03/13 - **Gemma: Open Models Based on Gemini Research and Technology** <br>([:x:](https://arxiv.org/abs/2403.08295)), ([:book:](https://browse.arxiv.org/pdf/2403.08295.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08295.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08295)), ([:house:](https://huggingface.co/papers/2403.08295)), ([HTML](https://browse.arxiv.org/html/2403.08295v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08295)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08295v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08295)), ([SS](https://api.semanticscholar.org/arXiv:2403.08295)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gemma-open-models-based-on-gemini-research))	
  * 03/13 - **GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting** <br>([:x:](https://arxiv.org/abs/2403.08551)), ([:book:](https://browse.arxiv.org/pdf/2403.08551.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08551.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08551)), ([:house:](https://huggingface.co/papers/2403.08551)), ([HTML](https://browse.arxiv.org/html/2403.08551v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08551)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08551v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08551)), ([SS](https://api.semanticscholar.org/arXiv:2403.08551)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gaussianimage-1000-fps-image-representation))	
  * 03/13 - **Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts** <br>([:x:](https://arxiv.org/abs/2403.08268)), ([:book:](https://browse.arxiv.org/pdf/2403.08268.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08268.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08268)), ([:house:](https://huggingface.co/papers/2403.08268)), ([HTML](https://browse.arxiv.org/html/2403.08268v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08268)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08268v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08268)), ([SS](https://api.semanticscholar.org/arXiv:2403.08268)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/follow-your-click-open-domain-regional-image)), ([:octocat:](https://github.com/mayuelala/followyourclick)![GitHub Repo stars](https://img.shields.io/github/stars/mayuelala/followyourclick?style=social))	
  * 03/13 - **Cultural evolution in populations of Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.08882)), ([:book:](https://browse.arxiv.org/pdf/2403.08882.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08882.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08882)), ([:house:](https://huggingface.co/papers/2403.08882)), ([HTML](https://browse.arxiv.org/html/2403.08882v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08882)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08882v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08882)), ([SS](https://api.semanticscholar.org/arXiv:2403.08882)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cultural-evolution-in-populations-of-large)), ([:octocat:](https://github.com/jeremyperez2/llm-culture)![GitHub Repo stars](https://img.shields.io/github/stars/jeremyperez2/llm-culture?style=social))	
  * 03/13 - **Bugs in Large Language Models Generated Code: An Empirical Study** <br>([:x:](https://arxiv.org/abs/2403.08937)), ([:book:](https://browse.arxiv.org/pdf/2403.08937.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.08937.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.08937)), ([:house:](https://huggingface.co/papers/2403.08937)), ([HTML](https://browse.arxiv.org/html/2403.08937v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.08937)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.08937v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.08937)), ([SS](https://api.semanticscholar.org/arXiv:2403.08937)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/bugs-in-large-language-models-generated-code)), ([:octocat:](https://github.com/flowss/bugsinllms)![GitHub Repo stars](https://img.shields.io/github/stars/flowss/bugsinllms?style=social))	
  * 03/12 - **Synth^2: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings** <br>([:x:](https://arxiv.org/abs/2403.07750)), ([:book:](https://browse.arxiv.org/pdf/2403.07750.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.07750.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.07750)), ([:house:](https://huggingface.co/papers/2403.07750)), ([HTML](https://browse.arxiv.org/html/2403.07750v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.07750)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.07750v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.07750)), ([SS](https://api.semanticscholar.org/arXiv:2403.07750)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/synth-2-boosting-visual-language-models-with))	
  * 03/12 - **Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM** <br>([:x:](https://arxiv.org/abs/2403.07487)), ([:book:](https://browse.arxiv.org/pdf/2403.07487.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.07487.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.07487)), ([:house:](https://huggingface.co/papers/2403.07487)), ([HTML](https://browse.arxiv.org/html/2403.07487v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.07487)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.07487v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.07487)), ([SS](https://api.semanticscholar.org/arXiv:2403.07487)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/motion-mamba-efficient-and-long-sequence))	
  * 03/12 - **MoAI: Mixture of All Intelligence for Large Language and Vision Models** <br>([:x:](https://arxiv.org/abs/2403.07508)), ([:book:](https://browse.arxiv.org/pdf/2403.07508.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.07508.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.07508)), ([:house:](https://huggingface.co/papers/2403.07508)), ([HTML](https://browse.arxiv.org/html/2403.07508v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.07508)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.07508v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.07508)), ([SS](https://api.semanticscholar.org/arXiv:2403.07508)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/moai-mixture-of-all-intelligence-for-large))	
  * 03/12 - **Learning Generalizable Feature Fields for Mobile Manipulation** <br>([:x:](https://arxiv.org/abs/2403.07563)), ([:book:](https://browse.arxiv.org/pdf/2403.07563.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.07563.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.07563)), ([:house:](https://huggingface.co/papers/2403.07563)), ([HTML](https://browse.arxiv.org/html/2403.07563v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.07563)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.07563v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.07563)), ([SS](https://api.semanticscholar.org/arXiv:2403.07563)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-generalizable-feature-fields-for))	
  * 03/12 - **DragAnything: Motion Control for Anything using Entity Representation** <br>([:x:](https://arxiv.org/abs/2403.07420)), ([:book:](https://browse.arxiv.org/pdf/2403.07420.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.07420.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.07420)), ([:house:](https://huggingface.co/papers/2403.07420)), ([HTML](https://browse.arxiv.org/html/2403.07420v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.07420)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.07420v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.07420)), ([SS](https://api.semanticscholar.org/arXiv:2403.07420)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/draganything-motion-control-for-anything)), ([:octocat:](https://github.com/showlab/draganything)![GitHub Repo stars](https://img.shields.io/github/stars/showlab/draganything?style=social))	
  * 03/12 - **Chronos: Learning the Language of Time Series** <br>([:x:](https://arxiv.org/abs/2403.07815)), ([:book:](https://browse.arxiv.org/pdf/2403.07815.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.07815.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.07815)), ([:house:](https://huggingface.co/papers/2403.07815)), ([HTML](https://browse.arxiv.org/html/2403.07815v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.07815)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.07815v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.07815)), ([SS](https://api.semanticscholar.org/arXiv:2403.07815)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chronos-learning-the-language-of-time-series)), ([:octocat:](https://github.com/amazon-science/chronos-forecasting)![GitHub Repo stars](https://img.shields.io/github/stars/amazon-science/chronos-forecasting?style=social))	
  * 03/12 - **Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM** <br>([:x:](https://arxiv.org/abs/2403.07816)), ([:book:](https://browse.arxiv.org/pdf/2403.07816.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.07816.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.07816)), ([:house:](https://huggingface.co/papers/2403.07816)), ([HTML](https://browse.arxiv.org/html/2403.07816v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.07816)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.07816v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.07816)), ([SS](https://api.semanticscholar.org/arXiv:2403.07816)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/branch-train-mix-mixing-expert-llms-into-a)), ([:octocat:](https://github.com/arshadshk/mergekitMOE)![GitHub Repo stars](https://img.shields.io/github/stars/arshadshk/mergekitMOE?style=social))	
  * 03/11 - **Transparent AI Disclosure Obligations: Who, What, When, Where, Why, How** <br>([:x:](https://arxiv.org/abs/2403.06823)), ([:book:](https://browse.arxiv.org/pdf/2403.06823.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06823.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06823)), ([:house:](https://huggingface.co/papers/2403.06823)), ([HTML](https://browse.arxiv.org/html/2403.06823v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06823)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06823v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06823)), ([SS](https://api.semanticscholar.org/arXiv:2403.06823))	
  * 03/11 - **HILL: A Hallucination Identifier for Large Language Models** <br>([:x:](https://arxiv.org/abs/2403.06710)), ([:book:](https://browse.arxiv.org/pdf/2403.06710.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06710.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06710)), ([:house:](https://huggingface.co/papers/2403.06710)), ([HTML](https://browse.arxiv.org/html/2403.06710v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06710)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06710v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06710)), ([SS](https://api.semanticscholar.org/arXiv:2403.06710))	
  * 03/11 - **FAX: Scalable and Differentiable Federated Primitives in JAX** <br>([:x:](https://arxiv.org/abs/2403.07128)), ([:book:](https://browse.arxiv.org/pdf/2403.07128.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.07128.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.07128)), ([:house:](https://huggingface.co/papers/2403.07128)), ([HTML](https://browse.arxiv.org/html/2403.07128v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.07128)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.07128v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.07128)), ([SS](https://api.semanticscholar.org/arXiv:2403.07128)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fax-scalable-and-differentiable-federated)), ([:octocat:](https://github.com/google-research/google-research)![GitHub Repo stars](https://img.shields.io/github/stars/google-research/google-research?style=social))	
  * 03/11 - **FashionReGen: LLM-Empowered Fashion Report Generation** <br>([:x:](https://arxiv.org/abs/2403.06660)), ([:book:](https://browse.arxiv.org/pdf/2403.06660.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06660.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06660)), ([:house:](https://huggingface.co/papers/2403.06660)), ([HTML](https://browse.arxiv.org/html/2403.06660v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06660)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06660v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06660)), ([SS](https://api.semanticscholar.org/arXiv:2403.06660)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fashionregen-llm-empowered-fashion-report))	
  * 03/11 - **VideoMamba: State Space Model for Efficient Video Understanding** <br>([:x:](https://arxiv.org/abs/2403.06977)), ([:book:](https://browse.arxiv.org/pdf/2403.06977.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06977.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06977)), ([:house:](https://huggingface.co/papers/2403.06977)), ([HTML](https://browse.arxiv.org/html/2403.06977v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06977)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06977v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06977)), ([SS](https://api.semanticscholar.org/arXiv:2403.06977)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videomamba-state-space-model-for-efficient)), ([:octocat:](https://github.com/opengvlab/videomamba)![GitHub Repo stars](https://img.shields.io/github/stars/opengvlab/videomamba?style=social))
  * 03/11 - **V3D: Video Diffusion Models are Effective 3D Generators** <br>([:x:](https://arxiv.org/abs/2403.06738)), ([:book:](https://browse.arxiv.org/pdf/2403.06738.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06738.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06738)), ([:house:](https://huggingface.co/papers/2403.06738)), ([HTML](https://browse.arxiv.org/html/2403.06738v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06738)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06738v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06738)), ([SS](https://api.semanticscholar.org/arXiv:2403.06738)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/v3d-video-diffusion-models-are-effective-3d)), ([:octocat:](https://github.com/heheyas/v3d)![GitHub Repo stars](https://img.shields.io/github/stars/heheyas/v3d?style=social))
  * 03/11 - **Stealing Part of a Production Language Model** <br>([:x:](https://arxiv.org/abs/2403.06634)), ([:book:](https://browse.arxiv.org/pdf/2403.06634.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06634.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06634)), ([:house:](https://huggingface.co/papers/2403.06634)), ([HTML](https://browse.arxiv.org/html/2403.06634v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06634)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06634v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06634)), ([SS](https://api.semanticscholar.org/arXiv:2403.06634))
  * 03/11 - **Multistep Consistency Models** <br>([:x:](https://arxiv.org/abs/2403.06807)), ([:book:](https://browse.arxiv.org/pdf/2403.06807.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06807.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06807)), ([:house:](https://huggingface.co/papers/2403.06807)), ([HTML](https://browse.arxiv.org/html/2403.06807v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06807)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06807v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06807)), ([SS](https://api.semanticscholar.org/arXiv:2403.06807)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/consistency-models)), ([:octocat:](https://github.com/openai/consistency_models)![GitHub Repo stars](https://img.shields.io/github/stars/openai/consistency_models?style=social))
  * 03/11 - **FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation** <br>([:x:](https://arxiv.org/abs/2403.06775)), ([:book:](https://browse.arxiv.org/pdf/2403.06775.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06775.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06775)), ([:house:](https://huggingface.co/papers/2403.06775)), ([HTML](https://browse.arxiv.org/html/2403.06775v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06775)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06775v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06775)), ([SS](https://api.semanticscholar.org/arXiv:2403.06775)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/facechain-sude-building-derived-class-to))
  * 03/11 - **Chain-of-table: Evolving tables in the reasoning chain for table understanding**  ([Blog](https://blog.research.google/2024/03/chain-of-table-evolving-tables-in.html)), 
  * 03/11 - **An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models** <br>([:x:](https://arxiv.org/abs/2403.06764)), ([:book:](https://browse.arxiv.org/pdf/2403.06764.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06764.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06764)), ([:house:](https://huggingface.co/papers/2403.06764)), ([HTML](https://browse.arxiv.org/html/2403.06764v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06764)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06764v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06764)), ([SS](https://api.semanticscholar.org/arXiv:2403.06764)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-image-is-worth-1-2-tokens-after-layer-2)), ([:octocat:](https://github.com/pkunlp-icler/fastv)![GitHub Repo stars](https://img.shields.io/github/stars/pkunlp-icler/fastv?style=social))
  * 03/11 - **Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU** <br>([:x:](https://arxiv.org/abs/2403.06504)), ([:book:](https://browse.arxiv.org/pdf/2403.06504.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06504.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06504)), ([:house:](https://huggingface.co/papers/2403.06504)), ([HTML](https://browse.arxiv.org/html/2403.06504v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06504)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06504v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06504)), ([SS](https://api.semanticscholar.org/arXiv:2403.06504))
  * 03/10 - **VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models** <br>([:x:](https://arxiv.org/abs/2403.06098)), ([:book:](https://browse.arxiv.org/pdf/2403.06098.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.06098.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.06098)), ([:house:](https://huggingface.co/papers/2403.06098)), ([HTML](https://browse.arxiv.org/html/2403.06098v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.06098)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.06098v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.06098)), ([SS](https://api.semanticscholar.org/arXiv:2403.06098)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vidprom-a-million-scale-real-prompt-gallery))
  * 03/09 - **Algorithmic progress in language models** <br>([:x:](https://arxiv.org/abs/2403.05812)), ([:book:](https://browse.arxiv.org/pdf/2403.05812.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05812.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05812)), ([:house:](https://huggingface.co/papers/2403.05812)), ([HTML](https://browse.arxiv.org/html/2403.05812v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05812)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05812v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05812)), ([SS](https://api.semanticscholar.org/arXiv:2403.05812)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/algorithmic-progress-in-language-models))
  * 03/08 - **Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation** <br>([:x:](https://arxiv.org/abs/2403.05131)), ([:book:](https://browse.arxiv.org/pdf/2403.05131.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05131.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05131)), ([:house:](https://huggingface.co/papers/2403.05131)), ([HTML](https://browse.arxiv.org/html/2403.05131v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05131)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05131v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05131)), ([SS](https://api.semanticscholar.org/arXiv:2403.05131)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sora-as-an-agi-world-model-a-complete-survey))	
  * 03/08 - **On Protecting the Data Privacy of Large Language Models (LLMs): A Survey** <br>([:x:](https://arxiv.org/abs/2403.05156)), ([:book:](https://browse.arxiv.org/pdf/2403.05156.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05156.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05156)), ([:house:](https://huggingface.co/papers/2403.05156)), ([HTML](https://browse.arxiv.org/html/2403.05156v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05156)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05156v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05156)), ([SS](https://api.semanticscholar.org/arXiv:2403.05156))	
  * 03/08 - **VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models** <br>([:x:](https://arxiv.org/abs/2403.05438)), ([:book:](https://browse.arxiv.org/pdf/2403.05438.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05438.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05438)), ([:house:](https://huggingface.co/papers/2403.05438)), ([HTML](https://browse.arxiv.org/html/2403.05438v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05438)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05438v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05438)), ([SS](https://api.semanticscholar.org/arXiv:2403.05438)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/videoelevator-elevating-video-generation))
  * 03/08 - **Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks** <br>([:x:](https://arxiv.org/abs/2403.05185)), ([:book:](https://browse.arxiv.org/pdf/2403.05185.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05185.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05185)), ([:house:](https://huggingface.co/papers/2403.05185)), ([HTML](https://browse.arxiv.org/html/2403.05185v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05185)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05185v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05185)), ([SS](https://api.semanticscholar.org/arXiv:2403.05185)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/personalized-audiobook-recommendations-at))
  * 03/08 - **Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context** <br>([:x:](https://arxiv.org/abs/2403.05530)), ([:book:](https://browse.arxiv.org/pdf/2403.05530.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05530.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05530)), ([:house:](https://huggingface.co/papers/2403.05530)), ([HTML](https://browse.arxiv.org/html/2403.05530v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05530)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05530v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05530)), ([SS](https://api.semanticscholar.org/arXiv:2403.05530)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gemini-1-5-unlocking-multimodal-understanding))
  * 03/08 - **ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment** <br>([:x:](https://arxiv.org/abs/2403.05135)), ([:book:](https://browse.arxiv.org/pdf/2403.05135.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05135.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05135)), ([:house:](https://huggingface.co/papers/2403.05135)), ([HTML](https://browse.arxiv.org/html/2403.05135v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05135)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05135v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05135)), ([SS](https://api.semanticscholar.org/arXiv:2403.05135)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ella-equip-diffusion-models-with-llm-for))
  * 03/08 - **DeepSeek-VL: Towards Real-World Vision-Language Understanding** <br>([:x:](https://arxiv.org/abs/2403.05525)), ([:book:](https://browse.arxiv.org/pdf/2403.05525.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05525.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05525)), ([:house:](https://huggingface.co/papers/2403.05525)), ([HTML](https://browse.arxiv.org/html/2403.05525v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05525)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05525v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05525)), ([SS](https://api.semanticscholar.org/arXiv:2403.05525)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/deepseek-vl-towards-real-world-vision)), ([:octocat:](https://github.com/deepseek-ai/deepseek-vl)![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/deepseek-vl?style=social))
  * 03/08 - **CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model** <br>([:x:](https://arxiv.org/abs/2403.05034)), ([:book:](https://browse.arxiv.org/pdf/2403.05034.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05034.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05034)), ([:house:](https://huggingface.co/papers/2403.05034)), ([HTML](https://browse.arxiv.org/html/2403.05034v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05034)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05034v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05034)), ([SS](https://api.semanticscholar.org/arXiv:2403.05034)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/crm-single-image-to-3d-textured-mesh-with))
  * 03/08 - **CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion** <br>([:x:](https://arxiv.org/abs/2403.05121)), ([:book:](https://browse.arxiv.org/pdf/2403.05121.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.05121.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.05121)), ([:house:](https://huggingface.co/papers/2403.05121)), ([HTML](https://browse.arxiv.org/html/2403.05121v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.05121)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.05121v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05121)), ([SS](https://api.semanticscholar.org/arXiv:2403.05121)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/cogview3-finer-and-faster-text-to-image))
  * 03/08 - **Now available on Poe: Claude 3**  ([Demo](https://poe.com/Claude-3-Opus)), 
  * 03/08 - **Google - Health-specific embedding tools for dermatology and pathology**  ([Blog](https://blog.research.google/2024/03/health-specific-embedding-tools-for.html)),
  * 03/07 - **Yi: Open Foundation Models by 01.AI** <br>([:x:](https://arxiv.org/abs/2403.04652)), ([:book:](https://browse.arxiv.org/pdf/2403.04652.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04652.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04652)), ([:house:](https://huggingface.co/papers/2403.04652)), ([HTML](https://browse.arxiv.org/html/2403.04652v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04652)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04652v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04652)), ([SS](https://api.semanticscholar.org/arXiv:2403.04652)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/yi-open-foundation-models-by-01-ai)), ([:octocat:](https://github.com/01-ai/yi)![GitHub Repo stars](https://img.shields.io/github/stars/01-ai/yi?style=social))
  * 03/07 - **Teaching Large Language Models to Reason with Reinforcement Learning** <br>([:x:](https://arxiv.org/abs/2403.04642)), ([:book:](https://browse.arxiv.org/pdf/2403.04642.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04642.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04642)), ([:house:](https://huggingface.co/papers/2403.04642)), ([HTML](https://browse.arxiv.org/html/2403.04642v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04642)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04642v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04642)), ([SS](https://api.semanticscholar.org/arXiv:2403.04642)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/teaching-large-language-models-to-reason-with))
  * 03/07 - **StableDrag: Stable Dragging for Point-based Image Editing** <br>([:x:](https://arxiv.org/abs/2403.04437)), ([:book:](https://browse.arxiv.org/pdf/2403.04437.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04437.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04437)), ([:house:](https://huggingface.co/papers/2403.04437)), ([HTML](https://browse.arxiv.org/html/2403.04437v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04437)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04437v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04437)), ([SS](https://api.semanticscholar.org/arXiv:2403.04437)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stabledrag-stable-dragging-for-point-based))
  * 03/07 - **Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis** <br>([:x:](https://arxiv.org/abs/2403.04116)), ([:book:](https://browse.arxiv.org/pdf/2403.04116.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04116.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04116)), ([:house:](https://huggingface.co/papers/2403.04116)), ([HTML](https://browse.arxiv.org/html/2403.04116v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04116)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04116v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04116)), ([SS](https://api.semanticscholar.org/arXiv:2403.04116)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/radiative-gaussian-splatting-for-efficient-x)), ([:octocat:](https://github.com/caiyuanhao1998/x-gaussian)![GitHub Repo stars](https://img.shields.io/github/stars/caiyuanhao1998/x-gaussian?style=social))
  * 03/07 - **PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation** <br>([:x:](https://arxiv.org/abs/2403.04692)), ([:book:](https://browse.arxiv.org/pdf/2403.04692.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04692.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04692)), ([:house:](https://huggingface.co/papers/2403.04692)), ([HTML](https://browse.arxiv.org/html/2403.04692v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04692)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04692v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04692)), ([SS](https://api.semanticscholar.org/arXiv:2403.04692)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pixart-s-weak-to-strong-training-of-diffusion))
  * 03/07 - **Pix2Gif: Motion-Guided Diffusion for GIF Generation** <br>([:x:](https://arxiv.org/abs/2403.04634)), ([:book:](https://browse.arxiv.org/pdf/2403.04634.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04634.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04634)), ([:house:](https://huggingface.co/papers/2403.04634)), ([HTML](https://browse.arxiv.org/html/2403.04634v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04634)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04634v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04634)), ([SS](https://api.semanticscholar.org/arXiv:2403.04634)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/pix2gif-motion-guided-diffusion-for-gif))
  * 03/07 - **Meet ‘Liberated Qwen’, an uncensored LLM that strictly adheres to system prompts**  ([News](https://venturebeat.com/ai/meet-liberated-qwen-an-uncensored-llm-that-strictly-adheres-to-system-prompts/)), 
  * 03/07 - **LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error** <br>([:x:](https://arxiv.org/abs/2403.04746)), ([:book:](https://browse.arxiv.org/pdf/2403.04746.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04746.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04746)), ([:house:](https://huggingface.co/papers/2403.04746)), ([HTML](https://browse.arxiv.org/html/2403.04746v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04746)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04746v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04746)), ([SS](https://api.semanticscholar.org/arXiv:2403.04746)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llms-in-the-imaginarium-tool-learning-through)), ([:octocat:](https://github.com/microsoft/simulated-trial-and-error)![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/simulated-trial-and-error?style=social))
  * 03/07 - **KAIST develops next-generation ultra-low power LLM accelerator**  ([News](https://en.yna.co.kr/view/AEN20240306003700320)), 
  * 03/07 - **Inflection-2.5: meet the world's best personal AI**  ([News](https://inflection.ai/inflection-2-5)), 
  * 03/07 - **How Far Are We from Intelligent Visual Deductive Reasoning?** <br>([:x:](https://arxiv.org/abs/2403.04732)), ([:book:](https://browse.arxiv.org/pdf/2403.04732.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04732.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04732)), ([:house:](https://huggingface.co/papers/2403.04732)), ([HTML](https://browse.arxiv.org/html/2403.04732v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04732)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04732v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04732)), ([SS](https://api.semanticscholar.org/arXiv:2403.04732)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/how-far-are-we-from-intelligent-visual)), ([:octocat:](https://github.com/apple/ml-rpm-bench)![GitHub Repo stars](https://img.shields.io/github/stars/apple/ml-rpm-bench?style=social))
  * 03/07 - **GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection** <br>([:x:](https://arxiv.org/abs/2403.03507)), ([:book:](https://browse.arxiv.org/pdf/2403.03507.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03507.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03507)), ([:house:](https://huggingface.co/papers/2403.03507)), ([HTML](https://browse.arxiv.org/html/2403.03507v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03507)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03507v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03507)), ([SS](https://api.semanticscholar.org/arXiv:2403.03507)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/galore-memory-efficient-llm-training-by)), ([:octocat:](https://github.com/jiaweizzhao/galore)![GitHub Repo stars](https://img.shields.io/github/stars/jiaweizzhao/galore?style=social))
  * 03/07 - **Evaluating LLM models at scale**  ([Blog](https://blog.mozilla.org/en/mozilla/ai/evaluating-llm-models-at-scale/)), 
  * 03/07 - **Common 7B Language Models Already Possess Strong Math Capabilities** <br>([:x:](https://arxiv.org/abs/2403.04706)), ([:book:](https://browse.arxiv.org/pdf/2403.04706.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04706.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04706)), ([:house:](https://huggingface.co/papers/2403.04706)), ([HTML](https://browse.arxiv.org/html/2403.04706v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04706)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04706v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04706)), ([SS](https://api.semanticscholar.org/arXiv:2403.04706)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/common-7b-language-models-already-possess))
  * 03/07 - **Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference** <br>([:x:](https://arxiv.org/abs/2403.04132)), ([:book:](https://browse.arxiv.org/pdf/2403.04132.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.04132.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.04132)), ([:house:](https://huggingface.co/papers/2403.04132)), ([HTML](https://browse.arxiv.org/html/2403.04132v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.04132)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.04132v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.04132)), ([SS](https://api.semanticscholar.org/arXiv:2403.04132)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatbot-arena-an-open-platform-for-evaluating))
  * 03/06 - **Stop Regressing: Training Value Functions via Classification for Scalable Deep RL** <br>([:x:](https://arxiv.org/abs/2403.03950)), ([:book:](https://browse.arxiv.org/pdf/2403.03950.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03950.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03950)), ([:house:](https://huggingface.co/papers/2403.03950)), ([HTML](https://browse.arxiv.org/html/2403.03950v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03950)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03950v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03950)), ([SS](https://api.semanticscholar.org/arXiv:2403.03950)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/stop-regressing-training-value-functions-via))
  * 03/06 - **ShortGPT: Layers in Large Language Models are More Redundant Than You Expect** <br>([:x:](https://arxiv.org/abs/2403.03853)), ([:book:](https://browse.arxiv.org/pdf/2403.03853.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03853.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03853)), ([:house:](https://huggingface.co/papers/2403.03853)), ([HTML](https://browse.arxiv.org/html/2403.03853v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03853)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03853v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03853)), ([SS](https://api.semanticscholar.org/arXiv:2403.03853)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/shortgpt-layers-in-large-language-models-are))
  * 03/06 - **SaulLM-7B: A pioneering Large Language Model for Law** <br>([:x:](https://arxiv.org/abs/2403.03883)), ([:book:](https://browse.arxiv.org/pdf/2403.03883.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03883.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03883)), ([:house:](https://huggingface.co/papers/2403.03883)), ([HTML](https://browse.arxiv.org/html/2403.03883v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03883)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03883v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03883)), ([SS](https://api.semanticscholar.org/arXiv:2403.03883)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/saullm-7b-a-pioneering-large-language-model))
  * 03/06 - **NY hospital exec: Multimodal LLM assistants will create a “paradigm shift” in patient care**  ([News](https://venturebeat.com/ai/ny-hospital-exec-multimodal-llm-assistants-will-create-a-paradigm-shift-in-patient-care/)), 
  * 03/06 - **Learning to Decode Collaboratively with Multiple Language Models** <br>([:x:](https://arxiv.org/abs/2403.03870)), ([:book:](https://browse.arxiv.org/pdf/2403.03870.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03870.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03870)), ([:house:](https://huggingface.co/papers/2403.03870)), ([HTML](https://browse.arxiv.org/html/2403.03870v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03870)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03870v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03870)), ([SS](https://api.semanticscholar.org/arXiv:2403.03870)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/learning-to-decode-collaboratively-with)), ([:octocat:](https://github.com/clinicalml/co-llm)![GitHub Repo stars](https://img.shields.io/github/stars/clinicalml/co-llm?style=social))
  * 03/06 - **Enhancing Vision-Language Pre-training with Rich Supervisions** <br>([:x:](https://arxiv.org/abs/2403.03346)), ([:book:](https://browse.arxiv.org/pdf/2403.03346.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03346.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03346)), ([:house:](https://huggingface.co/papers/2403.03346)), ([HTML](https://browse.arxiv.org/html/2403.03346v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03346)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03346v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03346)), ([SS](https://api.semanticscholar.org/arXiv:2403.03346)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-vision-language-pre-training-with-1))
  * 03/06 - **Backtracing: Retrieving the Cause of the Query** <br>([:x:](https://arxiv.org/abs/2403.03956)), ([:book:](https://browse.arxiv.org/pdf/2403.03956.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03956.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03956)), ([:house:](https://huggingface.co/papers/2403.03956)), ([HTML](https://browse.arxiv.org/html/2403.03956v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03956)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03956v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03956)), ([SS](https://api.semanticscholar.org/arXiv:2403.03956)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/backtracing-retrieving-the-cause-of-the-query)), ([:octocat:](https://github.com/rosewang2008/backtracing)![GitHub Repo stars](https://img.shields.io/github/stars/rosewang2008/backtracing?style=social))
  * 03/06 - **AI Prompt Engineering Is Dead**  ([News](https://spectrum.ieee.org/prompt-engineering-is-dead)), 
  * 03/06 - **3D Diffusion Policy** <br>([:x:](https://arxiv.org/abs/2403.03954)), ([:book:](https://browse.arxiv.org/pdf/2403.03954.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03954.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03954)), ([:house:](https://huggingface.co/papers/2403.03954)), ([HTML](https://browse.arxiv.org/html/2403.03954v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03954)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03954v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03954)), ([SS](https://api.semanticscholar.org/arXiv:2403.03954)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3d-diffusion-policy)), ([:octocat:](https://github.com/YanjieZe/3D-Diffusion-Policy)![GitHub Repo stars](https://img.shields.io/github/stars/YanjieZe/3D-Diffusion-Policy?style=social))
  * 03/05 - **OpenAI and Elon Musk**   ([Blog](https://openai.com/blog/openai-elon-musk)), 
  * 03/05 - **Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling** <br>([:x:](https://arxiv.org/abs/2403.03234)), ([:book:](https://browse.arxiv.org/pdf/2403.03234.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03234.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03234)), ([:house:](https://huggingface.co/papers/2403.03234)), ([HTML](https://browse.arxiv.org/html/2403.03234v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03234)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03234v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03234)), ([SS](https://api.semanticscholar.org/arXiv:2403.03234)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/caduceus-bi-directional-equivariant-long))
  * 03/05 - **WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction** ([:x:](https://arxiv.org/abs/2403.02962)), ([:book:](https://browse.arxiv.org/pdf/2403.02962.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02962.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02962)), ([:house:](https://huggingface.co/papers/2403.02962)), ([HTML](https://browse.arxiv.org/html/2403.02962v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02962)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02962v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02962)), ([SS](https://api.semanticscholar.org/arXiv:2403.02962)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wikitableedit-a-benchmark-for-table-editing))
  * 03/05 - **Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research** ([:x:](https://arxiv.org/abs/2403.02558)), ([:book:](https://browse.arxiv.org/pdf/2403.02558.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02558.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02558)), ([:house:](https://huggingface.co/papers/2403.02558)), ([HTML](https://browse.arxiv.org/html/2403.02558v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02558)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02558v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02558)), ([SS](https://api.semanticscholar.org/arXiv:2403.02558)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/updating-the-minimum-information-about)), ([:octocat:](https://github.com/bmiao10/mi-claim-2024)![GitHub Repo stars](https://img.shields.io/github/stars/bmiao10/mi-claim-2024?style=social))
  * 03/05 - **Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation** ([:x:](https://arxiv.org/abs/2403.02827)), ([:book:](https://browse.arxiv.org/pdf/2403.02827.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02827.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02827)), ([:house:](https://huggingface.co/papers/2403.02827)), ([HTML](https://browse.arxiv.org/html/2403.02827v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02827)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02827v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02827)), ([SS](https://api.semanticscholar.org/arXiv:2403.02827)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/tuning-free-noise-rectification-for-high))
  * 03/05 - **The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning** ([:x:](https://arxiv.org/abs/2403.03218)), ([:book:](https://browse.arxiv.org/pdf/2403.03218.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03218.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03218)), ([:house:](https://huggingface.co/papers/2403.03218)), ([HTML](https://browse.arxiv.org/html/2403.03218v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03218)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03218v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03218)), ([SS](https://api.semanticscholar.org/arXiv:2403.03218)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-wmdp-benchmark-measuring-and-reducing))
  * 03/05 - **Scaling Rectified Flow Transformers for High-Resolution Image Synthesis** ([:x:](https://arxiv.org/abs/2403.03206)), ([:book:](https://browse.arxiv.org/pdf/2403.03206.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03206.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03206)), ([:house:](https://huggingface.co/papers/2403.03206)), ([HTML](https://browse.arxiv.org/html/2403.03206v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03206)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03206v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03206)), ([SS](https://api.semanticscholar.org/arXiv:2403.03206)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/scaling-rectified-flow-transformers-for-high))
  * 03/05 - **RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches** ([:x:](https://arxiv.org/abs/2403.02709)), ([:book:](https://browse.arxiv.org/pdf/2403.02709.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02709.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02709)), ([:house:](https://huggingface.co/papers/2403.02709)), ([HTML](https://browse.arxiv.org/html/2403.02709v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02709)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02709v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02709)), ([SS](https://api.semanticscholar.org/arXiv:2403.02709))
  * 03/05 - **Revisiting Meta-evaluation for Grammatical Error Correction** ([:x:](https://arxiv.org/abs/2403.02674)), ([:book:](https://browse.arxiv.org/pdf/2403.02674.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02674.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02674)), ([:house:](https://huggingface.co/papers/2403.02674)), ([HTML](https://browse.arxiv.org/html/2403.02674v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02674)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02674v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02674)), ([SS](https://api.semanticscholar.org/arXiv:2403.02674)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/revisiting-meta-evaluation-for-grammatical))
  * 03/05 - **NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models** ([:x:](https://arxiv.org/abs/2403.03100)), ([:book:](https://browse.arxiv.org/pdf/2403.03100.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03100.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03100)), ([:house:](https://huggingface.co/papers/2403.03100)), ([HTML](https://browse.arxiv.org/html/2403.03100v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03100)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03100v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03100)), ([SS](https://api.semanticscholar.org/arXiv:2403.03100)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/naturalspeech-3-zero-shot-speech-synthesis))
  * 03/05 - **Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use** ([:x:](https://arxiv.org/abs/2403.02626)), ([:book:](https://browse.arxiv.org/pdf/2403.02626.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02626.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02626)), ([:house:](https://huggingface.co/papers/2403.02626)), ([HTML](https://browse.arxiv.org/html/2403.02626v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02626)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02626v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02626)), ([SS](https://api.semanticscholar.org/arXiv:2403.02626)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/modeling-collaborator-enabling-subjective))
  * 03/05 - **MathScale: Scaling Instruction Tuning for Mathematical Reasoning** ([:x:](https://arxiv.org/abs/2403.02884)), ([:book:](https://browse.arxiv.org/pdf/2403.02884.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02884.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02884)), ([:house:](https://huggingface.co/papers/2403.02884)), ([HTML](https://browse.arxiv.org/html/2403.02884v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02884)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02884v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02884)), ([SS](https://api.semanticscholar.org/arXiv:2403.02884))
  * 03/05 - **KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents** ([:x:](https://arxiv.org/abs/2403.03101)), ([:book:](https://browse.arxiv.org/pdf/2403.03101.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03101.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03101)), ([:house:](https://huggingface.co/papers/2403.03101)), ([HTML](https://browse.arxiv.org/html/2403.03101v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03101)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03101v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03101)), ([SS](https://api.semanticscholar.org/arXiv:2403.03101)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/knowagent-knowledge-augmented-planning-for)), ([:octocat:](https://github.com/zjunlp/knowagent)![GitHub Repo stars](https://img.shields.io/github/stars/zjunlp/knowagent?style=social))
  * 03/05 - **Interactive Continual Learning: Fast and Slow Thinking** ([:x:](https://arxiv.org/abs/2403.02628)), ([:book:](https://browse.arxiv.org/pdf/2403.02628.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02628.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02628)), ([:house:](https://huggingface.co/papers/2403.02628)), ([HTML](https://browse.arxiv.org/html/2403.02628v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02628)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02628v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02628)), ([SS](https://api.semanticscholar.org/arXiv:2403.02628)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/interactive-continual-learning-fast-and-slow))
  * 03/05 - **InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents** ([:x:](https://arxiv.org/abs/2403.02691)), ([:book:](https://browse.arxiv.org/pdf/2403.02691.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02691.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02691)), ([:house:](https://huggingface.co/papers/2403.02691)), ([HTML](https://browse.arxiv.org/html/2403.02691v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02691)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02691v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02691)), ([SS](https://api.semanticscholar.org/arXiv:2403.02691)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/injecagent-benchmarking-indirect-prompt))
  * 03/05 - **In Search of Truth: An Interrogation Approach to Hallucination Detection** ([:x:](https://arxiv.org/abs/2403.02889)), ([:book:](https://browse.arxiv.org/pdf/2403.02889.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02889.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02889)), ([:house:](https://huggingface.co/papers/2403.02889)), ([HTML](https://browse.arxiv.org/html/2403.02889v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02889)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02889v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02889)), ([SS](https://api.semanticscholar.org/arXiv:2403.02889)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/in-search-of-truth-an-interrogation-approach))
  * 03/05 - **ImgTrojan: Jailbreaking Vision-Language Models with ONE Image** ([:x:](https://arxiv.org/abs/2403.02910)), ([:book:](https://browse.arxiv.org/pdf/2403.02910.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02910.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02910)), ([:house:](https://huggingface.co/papers/2403.02910)), ([HTML](https://browse.arxiv.org/html/2403.02910v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02910)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02910v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02910)), ([SS](https://api.semanticscholar.org/arXiv:2403.02910)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/imgtrojan-jailbreaking-vision-language-models))
  * 03/05 - **Generative Software Engineering** ([:x:](https://arxiv.org/abs/2403.02583)), ([:book:](https://browse.arxiv.org/pdf/2403.02583.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02583.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02583)), ([:house:](https://huggingface.co/papers/2403.02583)), ([HTML](https://browse.arxiv.org/html/2403.02583v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02583)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02583v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02583)), ([SS](https://api.semanticscholar.org/arXiv:2403.02583))
  * 03/05 - **Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters** ([:x:](https://arxiv.org/abs/2403.02677)), ([:book:](https://browse.arxiv.org/pdf/2403.02677.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02677.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02677)), ([:house:](https://huggingface.co/papers/2403.02677)), ([HTML](https://browse.arxiv.org/html/2403.02677v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02677)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02677v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02677)), ([SS](https://api.semanticscholar.org/arXiv:2403.02677)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/finetuned-multimodal-language-models-are-high))
  * 03/05 - **Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models** ([:x:](https://arxiv.org/abs/2403.03003)), ([:book:](https://browse.arxiv.org/pdf/2403.03003.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03003.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03003)), ([:house:](https://huggingface.co/papers/2403.03003)), ([HTML](https://browse.arxiv.org/html/2403.03003v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03003)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03003v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03003)), ([SS](https://api.semanticscholar.org/arXiv:2403.03003)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/feast-your-eyes-mixture-of-resolution))
  * 03/05 - **Exploring the Limitations of Large Language Models in Compositional Relation Reasoning** ([:x:](https://arxiv.org/abs/2403.02615)), ([:book:](https://browse.arxiv.org/pdf/2403.02615.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02615.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02615)), ([:house:](https://huggingface.co/papers/2403.02615)), ([HTML](https://browse.arxiv.org/html/2403.02615v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02615)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02615v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02615)), ([SS](https://api.semanticscholar.org/arXiv:2403.02615)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/exploring-the-limitations-of-large-language))
  * 03/05 - **EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs** ([:x:](https://arxiv.org/abs/2403.02775)), ([:book:](https://browse.arxiv.org/pdf/2403.02775.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02775.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02775)), ([:house:](https://huggingface.co/papers/2403.02775)), ([HTML](https://browse.arxiv.org/html/2403.02775v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02775)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02775v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02775)), ([SS](https://api.semanticscholar.org/arXiv:2403.02775)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/easyquant-an-efficient-data-free-quantization))
  * 03/05 - **Design2Code: How Far Are We From Automating Front-End Engineering?** ([:x:](https://arxiv.org/abs/2403.03163)), ([:book:](https://browse.arxiv.org/pdf/2403.03163.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03163.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03163)), ([:house:](https://huggingface.co/papers/2403.03163)), ([HTML](https://browse.arxiv.org/html/2403.03163v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03163)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03163v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03163)), ([SS](https://api.semanticscholar.org/arXiv:2403.03163)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/design2code-how-far-are-we-from-automating))
  * 03/05 - **ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities** ([:x:](https://arxiv.org/abs/2403.02965)), ([:book:](https://browse.arxiv.org/pdf/2403.02965.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02965.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02965)), ([:house:](https://huggingface.co/papers/2403.02965)), ([HTML](https://browse.arxiv.org/html/2403.02965v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02965)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02965v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02965)), ([SS](https://api.semanticscholar.org/arXiv:2403.02965)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatgpt-and-biometrics-an-assessment-of-face))
  * 03/05 - **ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary** ([:x:](https://arxiv.org/abs/2403.02574)), ([:book:](https://browse.arxiv.org/pdf/2403.02574.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02574.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02574)), ([:house:](https://huggingface.co/papers/2403.02574)), ([HTML](https://browse.arxiv.org/html/2403.02574v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02574)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02574v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02574)), ([SS](https://api.semanticscholar.org/arXiv:2403.02574)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatcite-llm-agent-with-human-workflow))
  * 03/05 - **Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation** ([:x:](https://arxiv.org/abs/2403.02951)), ([:book:](https://browse.arxiv.org/pdf/2403.02951.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02951.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02951)), ([:house:](https://huggingface.co/papers/2403.02951)), ([HTML](https://browse.arxiv.org/html/2403.02951v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02951)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02951v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02951)), ([SS](https://api.semanticscholar.org/arXiv:2403.02951)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/benchmarking-the-text-to-sql-capability-of))
  * 03/05 - **An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers** ([:x:](https://arxiv.org/abs/2403.02839)), ([:book:](https://browse.arxiv.org/pdf/2403.02839.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02839.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02839)), ([:house:](https://huggingface.co/papers/2403.02839)), ([HTML](https://browse.arxiv.org/html/2403.02839v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02839)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02839v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02839)), ([SS](https://api.semanticscholar.org/arXiv:2403.02839)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/an-empirical-study-of-llm-as-a-judge-for-llm))
  * 3/5 - OpenAI - ChatGPT can now read responses to you.   ([twitter](https://twitter.com/OpenAI/status/1764712432939995549), 
  * 03/04 - **The Claude 3 Model Family: Opus, Sonnet, Haiku** <br>([:x:](https://www.anthropic.com/news/claude-3-family))  ([twitter](https://twitter.com/AnthropicAI/status/1764653830468428150)), , ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-claude-3-model-family-opus-sonnet-haiku))
  * 03/04 - **Wukong: Towards a Scaling Law for Large-Scale Recommendation** ([:x:](https://arxiv.org/abs/2403.02545)), ([:book:](https://browse.arxiv.org/pdf/2403.02545.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02545.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02545)), ([:house:](https://huggingface.co/papers/2403.02545)), ([HTML](https://browse.arxiv.org/html/2403.02545v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02545)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02545v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02545)), ([SS](https://api.semanticscholar.org/arXiv:2403.02545)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/wukong-towards-a-scaling-law-for-large-scale))
  * 03/04 - **Large language models surpass human experts in predicting neuroscience results** <br>([:x:](https://arxiv.org/abs/2403.03230)), ([:book:](https://browse.arxiv.org/pdf/2403.03230.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.03230.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.03230)), ([:house:](https://huggingface.co/papers/2403.03230)), ([HTML](https://browse.arxiv.org/html/2403.03230v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.03230)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.03230v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.03230)), ([SS](https://api.semanticscholar.org/arXiv:2403.03230)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-surpass-human-experts))	
  * 03/04 - **NoteLLM: A Retrievable Large Language Model for Note Recommendation** ([:x:](https://arxiv.org/abs/2403.01744)), ([:book:](https://browse.arxiv.org/pdf/2403.01744.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.01744.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.01744)), ([:house:](https://huggingface.co/papers/2403.01744)), ([HTML](https://browse.arxiv.org/html/2403.01744v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.01744)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.01744v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.01744)), ([SS](https://api.semanticscholar.org/arXiv:2403.01744)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/notellm-a-retrievable-large-language-model))
  * 03/04 - **MagicClay: Sculpting Meshes With Generative Neural Fields** ([:x:](https://arxiv.org/abs/2403.02460)), ([:book:](https://browse.arxiv.org/pdf/2403.02460.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02460.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02460)), ([:house:](https://huggingface.co/papers/2403.02460)), ([HTML](https://browse.arxiv.org/html/2403.02460v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02460)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02460v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02460)), ([SS](https://api.semanticscholar.org/arXiv:2403.02460))
  * 03/04 - **Enhancing LLM Safety via Constrained Direct Preference Optimization** ([:x:](https://arxiv.org/abs/2403.02475)), ([:book:](https://browse.arxiv.org/pdf/2403.02475.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02475.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02475)), ([:house:](https://huggingface.co/papers/2403.02475)), ([HTML](https://browse.arxiv.org/html/2403.02475v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02475)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02475v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02475)), ([SS](https://api.semanticscholar.org/arXiv:2403.02475)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-llm-safety-via-constrained-direct))
  * 03/04 - **DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation** ([:x:](https://arxiv.org/abs/2403.02528)), ([:book:](https://browse.arxiv.org/pdf/2403.02528.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02528.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02528)), ([:house:](https://huggingface.co/papers/2403.02528)), ([HTML](https://browse.arxiv.org/html/2403.02528v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02528)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02528v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02528)), ([SS](https://api.semanticscholar.org/arXiv:2403.02528)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/daco-towards-application-driven-and)), ([:octocat:](https://github.com/shirley-wu/daco)![GitHub Repo stars](https://img.shields.io/github/stars/shirley-wu/daco?style=social))
  * 03/04 - **CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation towards Automatic Compliance Checking** ([:x:](https://arxiv.org/abs/2403.02231)), ([:book:](https://browse.arxiv.org/pdf/2403.02231.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02231.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02231)), ([:house:](https://huggingface.co/papers/2403.02231)), ([HTML](https://browse.arxiv.org/html/2403.02231v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02231)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02231v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02231)), ([SS](https://api.semanticscholar.org/arXiv:2403.02231)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/code-accord-a-corpus-of-building-regulatory)), ([:octocat:](https://github.com/accord-project/code-accord)![GitHub Repo stars](https://img.shields.io/github/stars/accord-project/code-accord?style=social))
  * 03/04 - **Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF** ([:x:](https://arxiv.org/abs/2403.02513)), ([:book:](https://browse.arxiv.org/pdf/2403.02513.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02513.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02513)), ([:house:](https://huggingface.co/papers/2403.02513)), ([HTML](https://browse.arxiv.org/html/2403.02513v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02513)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02513v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02513)), ([SS](https://api.semanticscholar.org/arXiv:2403.02513)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/balancing-enhancement-harmlessness-and))
  * 03/04 - **adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds** ([:x:](https://arxiv.org/abs/2403.02370)), ([:book:](https://browse.arxiv.org/pdf/2403.02370.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02370.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02370)), ([:house:](https://huggingface.co/papers/2403.02370)), ([HTML](https://browse.arxiv.org/html/2403.02370v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.02370)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02370v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02370)), ([SS](https://api.semanticscholar.org/arXiv:2403.02370)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/adaptmllm-fine-tuning-multilingual-language)), ([:octocat:](https://github.com/adaptnmt/adaptmllm)![GitHub Repo stars](https://img.shields.io/github/stars/adaptnmt/adaptmllm?style=social))
  * 3/4 - ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models ([:x:](https://arxiv.org/abs/2403.01807)), ([:book:](https://browse.arxiv.org/pdf/2403.01807.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.01807.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.01807)), ([:house:](https://huggingface.co/papers/2403.01807)), ([HTML](https://browse.arxiv.org/html/2403.01807v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.01807v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.01807)), ([SS](https://api.semanticscholar.org/arXiv:2403.01807)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/viewdiff-3d-consistent-image-generation-with)), ([:octocat:](https://github.com/facebookresearch/viewdiff)![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/viewdiff?style=social))
  * 3/4 - TripoSR: Fast 3D Object Reconstruction from a Single Image ([:x:](https://arxiv.org/abs/2403.02151)), ([:book:](https://browse.arxiv.org/pdf/2403.02151.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02151.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02151)), ([:house:](https://huggingface.co/papers/2403.02151)), ([HTML](https://browse.arxiv.org/html/2403.02151v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02151v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02151)), ([SS](https://api.semanticscholar.org/arXiv:2403.02151)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/triposr-fast-3d-object-reconstruction-from-a)), ([:octocat:](https://github.com/vast-ai-research/triposr)![GitHub Repo stars](https://img.shields.io/github/stars/vast-ai-research/triposr?style=social))
  * 3/4 - RT-H: Action Hierarchies Using Language ([:x:](https://arxiv.org/abs/2403.01823)), ([:book:](https://browse.arxiv.org/pdf/2403.01823.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.01823.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.01823)), ([:house:](https://huggingface.co/papers/2403.01823)), ([HTML](https://browse.arxiv.org/html/2403.01823v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.01823v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.01823)), ([SS](https://api.semanticscholar.org/arXiv:2403.01823)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rt-h-action-hierarchies-using-language))
  * 3/4 - ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models ([:x:](https://arxiv.org/abs/2403.02084)), ([:book:](https://browse.arxiv.org/pdf/2403.02084.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.02084.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.02084)), ([:house:](https://huggingface.co/papers/2403.02084)), ([HTML](https://browse.arxiv.org/html/2403.02084v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.02084v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.02084)), ([SS](https://api.semanticscholar.org/arXiv:2403.02084)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/resadapter-domain-consistent-resolution))
  * 3/4 - OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on ([:x:](https://arxiv.org/abs/2403.01779)), ([:book:](https://browse.arxiv.org/pdf/2403.01779.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.01779.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.01779)), ([:house:](https://huggingface.co/papers/2403.01779)), ([HTML](https://browse.arxiv.org/html/2403.01779v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.01779v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.01779)), ([SS](https://api.semanticscholar.org/arXiv:2403.01779)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ootdiffusion-outfitting-fusion-based-latent)), ([:octocat:](https://github.com/levihsu/ootdiffusion)![GitHub Repo stars](https://img.shields.io/github/stars/levihsu/ootdiffusion?style=social))
  * 3/4 - Build AI for a Better Future   ([twitter](https://twitter.com/RonConway/status/1764652519324778966)),  ([News](https://openletter.svangel.com/)), 
  * 3/4 - AtomoVideo: High Fidelity Image-to-Video Generation ([:x:](https://arxiv.org/abs/2403.01800)), ([:book:](https://browse.arxiv.org/pdf/2403.01800.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.01800.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.01800)), ([:house:](https://huggingface.co/papers/2403.01800)), ([HTML](https://browse.arxiv.org/html/2403.01800v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.01800v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.01800)), ([SS](https://api.semanticscholar.org/arXiv:2403.01800)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/atomovideo-high-fidelity-image-to-video))
  * 03/03 - **Research Papers in February 2024: A LoRA Successor, Small Finetuned LLMs Vs Generalist LLMs, and Transparent LLM Research**   ([Blog](https://magazine.sebastianraschka.com/p/research-papers-in-february-2024)), 
  * 3/3 - Nvidia CEO Jensen Huang says AI could pass most human tests in 5 years   ([News](https://www.foxbusiness.com/technology/nvidia-ceo-jensen-huang-says-ai-could-pass-most-human-tests-5-years)
  * 3/3 - MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies ([:x:](https://arxiv.org/abs/2403.01422)), ([:book:](https://browse.arxiv.org/pdf/2403.01422.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.01422.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.01422)), ([:house:](https://huggingface.co/papers/2403.01422)), ([HTML](https://browse.arxiv.org/html/2403.01422v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.01422v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.01422)), ([SS](https://api.semanticscholar.org/arXiv:2403.01422)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/moviellm-enhancing-long-video-understanding))
  * 3/3 - InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding ([:x:](https://arxiv.org/abs/2403.01487)), ([:book:](https://browse.arxiv.org/pdf/2403.01487.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.01487.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.01487)), ([:house:](https://huggingface.co/papers/2403.01487)), ([HTML](https://browse.arxiv.org/html/2403.01487v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.01487v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.01487)), ([SS](https://api.semanticscholar.org/arXiv:2403.01487)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/infimm-hd-a-leap-forward-in-high-resolution))
  * 3/3 - Could this be bigger than OpenAI? Microsoft invests billions in French startup — Mistral AI is a multilingual maestro that's almost as good as ChatGPT 4   ([News](https://www.techradar.com/pro/could-this-be-bigger-than-openai-microsoft-invests-billions-in-french-startup-mistral-ai-is-a-multilingual-maestro-thats-almost-as-good-as-chatgpt-4)), 
  * 3/3 - 3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos ([:x:](https://arxiv.org/abs/2403.01444)), ([:book:](https://browse.arxiv.org/pdf/2403.01444.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.01444.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.01444)), ([:house:](https://huggingface.co/papers/2403.01444)), ([HTML](https://browse.arxiv.org/html/2403.01444v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.01444v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.01444)), ([SS](https://api.semanticscholar.org/arXiv:2403.01444)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/3dgstream-on-the-fly-training-of-3d-gaussians))
  * 3/2 - Nvidia CEO says AI could pass human tests in five years   ([News](https://www.reuters.com/technology/nvidia-ceo-says-ai-could-pass-human-tests-five-years-2024-03-01/)
  * 3/1 - Elon Musk sues OpenAI and CEO Sam Altman over contract breach   ([News](https://www.cnbc.com/2024/03/01/elon-musk-sues-openai-and-ceo-sam-altman-over-contract-breach.html))
  * 3.1 - AtP*: An efficient and scalable method for localizing LLM behaviour to components ([:x:](https://arxiv.org/abs/2403.00745)), ([:book:](https://browse.arxiv.org/pdf/2403.00745.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.00745.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.00745)), ([:house:](https://huggingface.co/papers/2403.00745)), ([HTML](https://browse.arxiv.org/html/2403.00745v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.00745v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.00745)), ([SS](https://api.semanticscholar.org/arXiv:2403.00745))
  * 3.1 - VisionLLaMA: A Unified LLaMA Interface for Vision Tasks ([:x:](https://arxiv.org/abs/2403.00522)), ([:book:](https://browse.arxiv.org/pdf/2403.00522.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.00522.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.00522)), ([:house:](https://huggingface.co/papers/2403.00522)), ([HTML](https://browse.arxiv.org/html/2403.00522v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.00522v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.00522)), ([SS](https://api.semanticscholar.org/arXiv:2403.00522))
  * 3.1 - Learning and Leveraging World Models in Visual Representation Learning ([:x:](https://arxiv.org/abs/2403.00504)), ([:book:](https://browse.arxiv.org/pdf/2403.00504.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.00504.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.00504)), ([:house:](https://huggingface.co/papers/2403.00504)), ([HTML](https://browse.arxiv.org/html/2403.00504v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.00504v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.00504)), ([SS](https://api.semanticscholar.org/arXiv:2403.00504))
  * 3.1 - RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization ([:x:](https://arxiv.org/abs/2403.00483)), ([:book:](https://browse.arxiv.org/pdf/2403.00483.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.00483.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.00483)), ([:house:](https://huggingface.co/papers/2403.00483)), ([HTML](https://browse.arxiv.org/html/2403.00483v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.00483v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.00483)), ([SS](https://api.semanticscholar.org/arXiv:2403.00483))
  * 3.1 - Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models ([:x:](https://arxiv.org/abs/2403.00231)), ([:book:](https://browse.arxiv.org/pdf/2403.00231.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.00231.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.00231)), ([:house:](https://huggingface.co/papers/2403.00231)), ([HTML](https://browse.arxiv.org/html/2403.00231v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.00231v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.00231)), ([SS](https://api.semanticscholar.org/arXiv:2403.00231))
  * 3.1 - Resonance RoPE: Improving Context Length Generalization of Large Language Models ([:x:](https://arxiv.org/abs/2403.00071)), ([:book:](https://browse.arxiv.org/pdf/2403.00071.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.00071.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.00071)), ([:house:](https://huggingface.co/papers/2403.00071)), ([HTML](https://browse.arxiv.org/html/2403.00071v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.00071v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.00071)), ([SS](https://api.semanticscholar.org/arXiv:2403.00071)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/resonance-rope-improving-context-length)), ([:octocat:](https://github.com/sheryc/resonance_rope)![GitHub Repo stars](https://img.shields.io/github/stars/sheryc/resonance_rope?style=social))
  * 02/29 - **OHTA: One-shot Hand Avatar via Data-driven Implicit Priors** <br>([:x:](https://arxiv.org/abs/2402.18969)), ([:book:](https://browse.arxiv.org/pdf/2402.18969.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18969.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18969)), ([:house:](https://huggingface.co/papers/2402.18969)), ([HTML](https://browse.arxiv.org/html/2402.18969v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2402.18969)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18969v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18969)), ([SS](https://api.semanticscholar.org/arXiv:2402.18969)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/ohta-one-shot-hand-avatar-via-data-driven))
  * 02/29 - **Retrieval-Augmented Generation for AI-Generated Content: A Survey** ([:x:](https://arxiv.org/abs/2402.19473)), ([:book:](https://browse.arxiv.org/pdf/2402.19473.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.19473.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.19473)), ([:house:](https://huggingface.co/papers/2402.19473)), ([HTML](https://browse.arxiv.org/html/2402.19473v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2402.19473)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.19473v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.19473)), ([SS](https://api.semanticscholar.org/arXiv:2402.19473)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/retrieval-augmented-generation-for-ai)), ([:octocat:](https://github.com/hymie122/rag-survey)![GitHub Repo stars](https://img.shields.io/github/stars/hymie122/rag-survey?style=social))
  * 2.29 - DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models ([:x:](https://arxiv.org/abs/2402.19481)), ([:book:](https://browse.arxiv.org/pdf/2402.19481.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.19481.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.19481)), ([:house:](https://huggingface.co/papers/2402.19481)), ([HTML](https://browse.arxiv.org/html/2402.19481v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.19481v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.19481)), ([SS](https://api.semanticscholar.org/arXiv:2402.19481)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/distrifusion-distributed-parallel-inference))
  * 2.29 - Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers ([:x:](https://arxiv.org/abs/2402.19479)), ([:book:](https://browse.arxiv.org/pdf/2402.19479.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.19479.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.19479)), ([:house:](https://huggingface.co/papers/2402.19479)), ([HTML](https://browse.arxiv.org/html/2402.19479v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.19479v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.19479)), ([SS](https://api.semanticscholar.org/arXiv:2402.19479)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/panda-70m-captioning-70m-videos-with-multiple))
  * 2.29 - Humanoid Locomotion as Next Token Prediction ([:x:](https://arxiv.org/abs/2402.19469)), ([:book:](https://browse.arxiv.org/pdf/2402.19469.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.19469.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.19469)), ([:house:](https://huggingface.co/papers/2402.19469)), ([HTML](https://browse.arxiv.org/html/2402.19469v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.19469v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.19469)), ([SS](https://api.semanticscholar.org/arXiv:2402.19469)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/humanoid-locomotion-as-next-token-prediction))
  * 2.29 - Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models ([:x:](https://arxiv.org/abs/2402.19427)), ([:book:](https://browse.arxiv.org/pdf/2402.19427.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.19427.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.19427)), ([:house:](https://huggingface.co/papers/2402.19427)), ([HTML](https://browse.arxiv.org/html/2402.19427v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.19427v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.19427)), ([SS](https://api.semanticscholar.org/arXiv:2402.19427)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/griffin-mixing-gated-linear-recurrences-with))
  * 2.29 - StarCoder 2 and The Stack v2: The Next Generation ([:x:](https://arxiv.org/abs/2402.19173)), ([:book:](https://browse.arxiv.org/pdf/2402.19173.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.19173.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.19173)), ([:house:](https://huggingface.co/papers/2402.19173)), ([HTML](https://browse.arxiv.org/html/2402.19173v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.19173v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.19173)), ([SS](https://api.semanticscholar.org/arXiv:2402.19173)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/starcoder-2-and-the-stack-v2-the-next))
  * 2.29 - Trajectory Consistency Distillation ([:x:](https://arxiv.org/abs/2402.19159)), ([:book:](https://browse.arxiv.org/pdf/2402.19159.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.19159.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.19159)), ([:house:](https://huggingface.co/papers/2402.19159)), ([HTML](https://browse.arxiv.org/html/2402.19159v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.19159v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.19159)), ([SS](https://api.semanticscholar.org/arXiv:2402.19159)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/trajectory-consistency-distillation)), ([:octocat:](https://github.com/jabir-zheng/TCD)![GitHub Repo stars](https://img.shields.io/github/stars/jabir-zheng/TCD?style=social))
  * 2.29 - Beyond Language Models: Byte Models are Digital World Simulators ([:x:](https://arxiv.org/abs/2402.19155)), ([:book:](https://browse.arxiv.org/pdf/2402.19155.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.19155.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.19155)), ([:house:](https://huggingface.co/papers/2402.19155)), ([HTML](https://browse.arxiv.org/html/2402.19155v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.19155v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.19155)), ([SS](https://api.semanticscholar.org/arXiv:2402.19155)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/beyond-language-models-byte-models-are))
  * 2.29 - Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models ([:x:](https://arxiv.org/abs/2402.18945)), ([:book:](https://browse.arxiv.org/pdf/2402.18945.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18945.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18945)), ([:house:](https://huggingface.co/papers/2402.18945)), ([HTML](https://browse.arxiv.org/html/2402.18945v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18945v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18945)), ([SS](https://api.semanticscholar.org/arXiv:2402.18945)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/syntactic-ghost-an-imperceptible-general))
  * 2.29 - ViewFusion: Towards Multi-View Consistency via Interpolated Denoising ([:x:](https://arxiv.org/abs/2402.18842)), ([:book:](https://browse.arxiv.org/pdf/2402.18842.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18842.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18842)), ([:house:](https://huggingface.co/papers/2402.18842)), ([HTML](https://browse.arxiv.org/html/2402.18842v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18842v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18842)), ([SS](https://api.semanticscholar.org/arXiv:2402.18842)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/viewfusion-towards-multi-view-consistency-via)), ([:octocat:](https://github.com/Wi-sc/ViewFusion)![GitHub Repo stars](https://img.shields.io/github/stars/Wi-sc/ViewFusion?style=social))
  * 2.29 - MOSAIC: A Modular System for Assistive and Interactive Cooking ([:x:](https://arxiv.org/abs/2402.18796)), ([:book:](https://browse.arxiv.org/pdf/2402.18796.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18796.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18796)), ([:house:](https://huggingface.co/papers/2402.18796)), ([HTML](https://browse.arxiv.org/html/2402.18796v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18796v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18796)), ([SS](https://api.semanticscholar.org/arXiv:2402.18796))
  * 02/28 - **Automatic Creative Selection with Cross-Modal Matching** <br>([:x:](https://arxiv.org/abs/2405.00029)), ([:book:](https://browse.arxiv.org/pdf/2405.00029.pdf)), ([:paperclip:](https://arxiv.org/pdf/2405.00029.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2405.00029)), ([:house:](https://huggingface.co/papers/2405.00029)), ([HTML](https://browse.arxiv.org/html/2405.00029v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2405.00029)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2405.00029v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2405.00029)), ([SS](https://api.semanticscholar.org/arXiv:2405.00029))
  * 2.28 - Priority Sampling of Large Language Models for Compilers ([:x:](https://arxiv.org/abs/2402.18734)), ([:book:](https://browse.arxiv.org/pdf/2402.18734.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18734.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18734)), ([:house:](https://huggingface.co/papers/2402.18734)), ([HTML](https://browse.arxiv.org/html/2402.18734v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18734v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18734)), ([SS](https://api.semanticscholar.org/arXiv:2402.18734)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/priority-sampling-of-large-language-models))
  * 2.28 - Simple linear attention language models balance the recall-throughput tradeoff ([:x:](https://arxiv.org/abs/2402.18668)), ([:book:](https://browse.arxiv.org/pdf/2402.18668.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18668.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18668)), ([:house:](https://huggingface.co/papers/2402.18668)), ([HTML](https://browse.arxiv.org/html/2402.18668v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18668v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18668)), ([SS](https://api.semanticscholar.org/arXiv:2402.18668)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/simple-linear-attention-language-models)), ([:octocat:](https://github.com/hazyresearch/based)![GitHub Repo stars](https://img.shields.io/github/stars/hazyresearch/based?style=social))
  * 2.28 - Approaching Human-Level Forecasting with Language Models ([:x:](https://arxiv.org/abs/2402.18563)), ([:book:](https://browse.arxiv.org/pdf/2402.18563.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18563.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18563)), ([:house:](https://huggingface.co/papers/2402.18563)), ([HTML](https://browse.arxiv.org/html/2402.18563v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18563v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18563)), ([SS](https://api.semanticscholar.org/arXiv:2402.18563)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/approaching-human-level-forecasting-with))
  * 2.28 - Datasets for Large Language Models: A Comprehensive Survey ([:x:](https://arxiv.org/abs/2402.18041)), ([:book:](https://browse.arxiv.org/pdf/2402.18041.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18041.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18041)), ([:house:](https://huggingface.co/papers/2402.18041)), ([HTML](https://browse.arxiv.org/html/2402.18041v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18041v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18041)), ([SS](https://api.semanticscholar.org/arXiv:2402.18041)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/datasets-for-large-language-models-a)), ([:octocat:](https://github.com/lmmlzn/awesome-llms-datasets)![GitHub Repo stars](https://img.shields.io/github/stars/lmmlzn/awesome-llms-datasets?style=social))
  * 2.28 - A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems ([:x:](https://arxiv.org/abs/2402.18013)), ([:book:](https://browse.arxiv.org/pdf/2402.18013.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18013.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18013)), ([:house:](https://huggingface.co/papers/2402.18013)), ([HTML](https://browse.arxiv.org/html/2402.18013v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18013v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18013)), ([SS](https://api.semanticscholar.org/arXiv:2402.18013)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/a-survey-on-recent-advances-in-llm-based))
  * 02/27 - **A High Level Guide to LLM Evaluation Metrics**   ([Blog](https://towardsdatascience.com/a-high-level-guide-to-llm-evaluation-metrics-fbecd08f725c)), 
  * 2/27 - Users Say Microsoft's AI Has Alternate Personality as Godlike AGI That Demands to Be Worshipped   ([News](https://futurism.com/microsoft-copilot-alter-egos))
  * 2/27 - Google DeepMind CEO on AGI, OpenAI and Beyond – MWC 2024   ([News](https://aibusiness.com/nlp/google-deepmind-ceo-on-agi-openai-and-beyond-mwc-2024))
  * 2.27 - Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding -- A Survey ([:x:](https://arxiv.org/abs/2402.17944)), ([:book:](https://browse.arxiv.org/pdf/2402.17944.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17944.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17944)), ([:house:](https://huggingface.co/papers/2402.17944)), ([HTML](https://browse.arxiv.org/html/2402.17944v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17944v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17944)), ([SS](https://api.semanticscholar.org/arXiv:2402.17944)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/large-language-models-on-tabular-data-a))
  * 2.27 - The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits ([:x:](https://arxiv.org/abs/2402.17764)), ([:book:](https://browse.arxiv.org/pdf/2402.17764.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17764.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17764)), ([:house:](https://huggingface.co/papers/2402.17764)), ([HTML](https://browse.arxiv.org/html/2402.17764v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17764v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17764)), ([SS](https://api.semanticscholar.org/arXiv:2402.17764)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/the-era-of-1-bit-llms-all-large-language)), ([:octocat:](https://github.com/Entropy-xcy/bitnet158)![GitHub Repo stars](https://img.shields.io/github/stars/Entropy-xcy/bitnet158?style=social))
  * 2.27 - Towards Optimal Learning of Language Models ([:x:](https://arxiv.org/abs/2402.17759)), ([:book:](https://browse.arxiv.org/pdf/2402.17759.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17759.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17759)), ([:house:](https://huggingface.co/papers/2402.17759)), ([HTML](https://browse.arxiv.org/html/2402.17759v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17759v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17759)), ([SS](https://api.semanticscholar.org/arXiv:2402.17759)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-optimal-learning-of-language-models))
  * 2.27 - Evaluating Very Long-Term Conversational Memory of LLM Agents ([:x:](https://arxiv.org/abs/2402.17753)), ([:book:](https://browse.arxiv.org/pdf/2402.17753.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17753.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17753)), ([:house:](https://huggingface.co/papers/2402.17753)), ([HTML](https://browse.arxiv.org/html/2402.17753v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17753v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17753)), ([SS](https://api.semanticscholar.org/arXiv:2402.17753)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/evaluating-very-long-term-conversational))
  * 2.27 - Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners ([:x:](https://arxiv.org/abs/2402.17723)), ([:book:](https://browse.arxiv.org/pdf/2402.17723.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17723.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17723)), ([:house:](https://huggingface.co/papers/2402.17723)), ([HTML](https://browse.arxiv.org/html/2402.17723v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17723v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17723)), ([SS](https://api.semanticscholar.org/arXiv:2402.17723)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/seeing-and-hearing-open-domain-visual-audio))
  * 2.27 - OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web ([:x:](https://arxiv.org/abs/2402.17553)), ([:book:](https://browse.arxiv.org/pdf/2402.17553.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17553.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17553)), ([:house:](https://huggingface.co/papers/2402.17553)), ([HTML](https://browse.arxiv.org/html/2402.17553v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17553v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17553)), ([SS](https://api.semanticscholar.org/arXiv:2402.17553)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/omniact-a-dataset-and-benchmark-for-enabling))
  * 2.27 - EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions ([:x:](https://arxiv.org/abs/2402.17485)), ([:book:](https://browse.arxiv.org/pdf/2402.17485.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17485.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17485)), ([:house:](https://huggingface.co/papers/2402.17485)), ([HTML](https://browse.arxiv.org/html/2402.17485v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17485v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17485)), ([SS](https://api.semanticscholar.org/arXiv:2402.17485)), ([:eight_spoked_asterisk:](https://paperswithcode.com/search?q_meta=&q_type=&q=EMO%3A%20Emote%20Portrait%20Alive%20--%20Generating%20Expressive%20Portrait%20Videos%20with%20Audio2Video%20Diffusion%20Model%20under%20Weak%20Conditions))
  * 2.27 - EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions ([:x:](https://arxiv.org/abs/2402.17485)), ([:book:](https://browse.arxiv.org/pdf/2402.17485.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17485.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17485)), ([:house:](https://huggingface.co/papers/2402.17485)), ([HTML](https://browse.arxiv.org/html/2402.17485v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17485v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17485)), ([SS](https://api.semanticscholar.org/arXiv:2402.17485)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/emo-emote-portrait-alive-generating))
  * 2.27 - Training-Free Long-Context Scaling of Large Language Models ([:x:](https://arxiv.org/abs/2402.17463)), ([:book:](https://browse.arxiv.org/pdf/2402.17463.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17463.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17463)), ([:house:](https://huggingface.co/papers/2402.17463)), ([HTML](https://browse.arxiv.org/html/2402.17463v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17463v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17463)), ([SS](https://api.semanticscholar.org/arXiv:2402.17463)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/training-free-long-context-scaling-of-large)), ([:octocat:](https://github.com/hkunlp/chunkllama)![GitHub Repo stars](https://img.shields.io/github/stars/hkunlp/chunkllama?style=social))
  * 2.27 - VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction ([:x:](https://arxiv.org/abs/2402.17427)), ([:book:](https://browse.arxiv.org/pdf/2402.17427.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17427.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17427)), ([:house:](https://huggingface.co/papers/2402.17427)), ([HTML](https://browse.arxiv.org/html/2402.17427v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17427v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17427)), ([SS](https://api.semanticscholar.org/arXiv:2402.17427)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vastgaussian-vast-3d-gaussians-for-large))
  * 2.27 - DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model ([:x:](https://arxiv.org/abs/2402.17412)), ([:book:](https://browse.arxiv.org/pdf/2402.17412.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17412.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17412)), ([:house:](https://huggingface.co/papers/2402.17412)), ([HTML](https://browse.arxiv.org/html/2402.17412v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17412v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17412)), ([SS](https://api.semanticscholar.org/arXiv:2402.17412)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/diffusekrona-a-parameter-efficient-fine))
  * 2.27 - Sora Generates Videos with Stunning Geometrical Consistency ([:x:](https://arxiv.org/abs/2402.17403)), ([:book:](https://browse.arxiv.org/pdf/2402.17403.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17403.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17403)), ([:house:](https://huggingface.co/papers/2402.17403)), ([HTML](https://browse.arxiv.org/html/2402.17403v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17403v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17403)), ([SS](https://api.semanticscholar.org/arXiv:2402.17403)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sora-generates-videos-with-stunning))
  * 2.27 - Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation ([:x:](https://arxiv.org/abs/2402.17245)), ([:book:](https://browse.arxiv.org/pdf/2402.17245.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17245.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17245)), ([:house:](https://huggingface.co/papers/2402.17245)), ([HTML](https://browse.arxiv.org/html/2402.17245v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17245v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17245)), ([SS](https://api.semanticscholar.org/arXiv:2402.17245)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/playground-v2-5-three-insights-towards))
  * 2.27 - When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method ([:x:](https://arxiv.org/abs/2402.17193)), ([:book:](https://browse.arxiv.org/pdf/2402.17193.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17193.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17193)), ([:house:](https://huggingface.co/papers/2402.17193)), ([HTML](https://browse.arxiv.org/html/2402.17193v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17193v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17193)), ([SS](https://api.semanticscholar.org/arXiv:2402.17193)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/when-scaling-meets-llm-finetuning-the-effect))
  * 2.27 - Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models ([:x:](https://arxiv.org/abs/2402.17177)), ([:book:](https://browse.arxiv.org/pdf/2402.17177.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17177.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17177)), ([:house:](https://huggingface.co/papers/2402.17177)), ([HTML](https://browse.arxiv.org/html/2402.17177v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17177v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17177)), ([SS](https://api.semanticscholar.org/arXiv:2402.17177)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/sora-a-review-on-background-technology)), ([:octocat:](https://github.com/lichao-sun/sorareview)![GitHub Repo stars](https://img.shields.io/github/stars/lichao-sun/sorareview?style=social))
  * 2.27 - Video as the New Language for Real-World Decision Making ([:x:](https://arxiv.org/abs/2402.17139)), ([:book:](https://browse.arxiv.org/pdf/2402.17139.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.17139.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.17139)), ([:house:](https://huggingface.co/papers/2402.17139)), ([HTML](https://browse.arxiv.org/html/2402.17139v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.17139v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17139)), ([SS](https://api.semanticscholar.org/arXiv:2402.17139)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/video-as-the-new-language-for-real-world))
  * 02/27 - **On the Societal Impact of Open Foundation Models** <br>([:x:](https://arxiv.org/abs/2403.07918)), ([:book:](https://browse.arxiv.org/pdf/2403.07918.pdf)), ([:paperclip:](https://arxiv.org/pdf/2403.07918.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.07918)), ([:house:](https://huggingface.co/papers/2403.07918)), ([HTML](https://browse.arxiv.org/html/2403.07918v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2403.07918)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.07918v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.07918)), ([SS](https://api.semanticscholar.org/arXiv:2403.07918)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/on-the-societal-impact-of-open-foundation))
  * 02/26 - **Set the Clock: Temporal Alignment of Pretrained Language Models** <br>([:x:](https://arxiv.org/abs/2402.16797)), ([:book:](https://browse.arxiv.org/pdf/2402.16797.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16797.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16797)), ([:house:](https://huggingface.co/papers/2402.16797)), ([HTML](https://browse.arxiv.org/html/2402.16797v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2402.16797)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16797v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16797)), ([SS](https://api.semanticscholar.org/arXiv:2402.16797)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/set-the-clock-temporal-alignment-of)), ([:octocat:](https://github.com/yizhongw/llm-temporal-alignment)![GitHub Repo stars](https://img.shields.io/github/stars/yizhongw/llm-temporal-alignment?style=social))	
  * 2/26 - DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models ([:x:](https://arxiv.org/abs/2403.00818)), ([:book:](https://browse.arxiv.org/pdf/2403.00818.pdf))([:paperclip:](https://arxiv.org/pdf/2403.00818.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2403.00818)), ([:house:](https://huggingface.co/papers/2403.00818)), ([HTML](https://browse.arxiv.org/html/2403.00818v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2403.00818v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2403.00818)), ([SS](https://api.semanticscholar.org/arXiv:2403.00818)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/densemamba-state-space-models-with-dense)), ([:octocat:](https://github.com/wailordhe/densessm)![GitHub Repo stars](https://img.shields.io/github/stars/wailordhe/densessm?style=social))
  * 02/26 - **Mistral Large is our flagship model, with top-tier reasoning capacities**   ([News](https://mistral.ai/news/mistral-large/))																												
  * 2.26 - Disentangled 3D Scene Generation with Layout Learning ([:x:](https://arxiv.org/abs/2402.16936)), ([:book:](https://browse.arxiv.org/pdf/2402.16936.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16936.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16936)), ([:house:](https://huggingface.co/papers/2402.16936)), ([HTML](https://browse.arxiv.org/html/2402.16936v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16936v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16936)), ([SS](https://api.semanticscholar.org/arXiv:2402.16936)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/disentangled-3d-scene-generation-with-layout))
  * 2.26 - Multi-LoRA Composition for Image Generation ([:x:](https://arxiv.org/abs/2402.16843)), ([:book:](https://browse.arxiv.org/pdf/2402.16843.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16843.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16843)), ([:house:](https://huggingface.co/papers/2402.16843)), ([HTML](https://browse.arxiv.org/html/2402.16843v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16843v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16843)), ([SS](https://api.semanticscholar.org/arXiv:2402.16843)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/multi-lora-composition-for-image-generation))
  * 2.26 - MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT ([:x:](https://arxiv.org/abs/2402.16840)), ([:book:](https://browse.arxiv.org/pdf/2402.16840.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16840.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16840)), ([:house:](https://huggingface.co/papers/2402.16840)), ([HTML](https://browse.arxiv.org/html/2402.16840v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16840v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16840)), ([SS](https://api.semanticscholar.org/arXiv:2402.16840)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mobillama-towards-accurate-and-lightweight)), ([:octocat:](https://github.com/mbzuai-oryx/mobillama)![GitHub Repo stars](https://img.shields.io/github/stars/mbzuai-oryx/mobillama?style=social))
  * 2.26 - Do Large Language Models Latently Perform Multi-Hop Reasoning? ([:x:](https://arxiv.org/abs/2402.16837)), ([:book:](https://browse.arxiv.org/pdf/2402.16837.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16837.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16837)), ([:house:](https://huggingface.co/papers/2402.16837)), ([HTML](https://browse.arxiv.org/html/2402.16837v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16837v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16837)), ([SS](https://api.semanticscholar.org/arXiv:2402.16837)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/do-large-language-models-latently-perform))
  * 2.26 - Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts ([:x:](https://arxiv.org/abs/2402.16822)), ([:book:](https://browse.arxiv.org/pdf/2402.16822.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16822.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16822)), ([:house:](https://huggingface.co/papers/2402.16822)), ([HTML](https://browse.arxiv.org/html/2402.16822v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16822v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16822)), ([SS](https://api.semanticscholar.org/arXiv:2402.16822)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/rainbow-teaming-open-ended-generation-of))
  * 2.26 - Nemotron-4 15B Technical Report ([:x:](https://arxiv.org/abs/2402.16819)), ([:book:](https://browse.arxiv.org/pdf/2402.16819.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16819.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16819)), ([:house:](https://huggingface.co/papers/2402.16819)), ([HTML](https://browse.arxiv.org/html/2402.16819v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16819v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16819)), ([SS](https://api.semanticscholar.org/arXiv:2402.16819)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/nemotron-4-15b-technical-report))
  * 2.26 - StructLM: Towards Building Generalist Models for Structured Knowledge Grounding ([:x:](https://arxiv.org/abs/2402.16671)), ([:book:](https://browse.arxiv.org/pdf/2402.16671.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16671.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16671)), ([:house:](https://huggingface.co/papers/2402.16671)), ([HTML](https://browse.arxiv.org/html/2402.16671v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16671v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16671)), ([SS](https://api.semanticscholar.org/arXiv:2402.16671)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/structlm-towards-building-generalist-models))
  * 2.26 - Towards Open-ended Visual Quality Comparison ([:x:](https://arxiv.org/abs/2402.16641)), ([:book:](https://browse.arxiv.org/pdf/2402.16641.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16641.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16641)), ([:house:](https://huggingface.co/papers/2402.16641)), ([HTML](https://browse.arxiv.org/html/2402.16641v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16641v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16641)), ([SS](https://api.semanticscholar.org/arXiv:2402.16641)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/towards-open-ended-visual-quality-comparison))
  * 2.25 - ChatMusician: Understanding and Generating Music Intrinsically with LLM ([:x:](https://arxiv.org/abs/2402.16153)), ([:book:](https://browse.arxiv.org/pdf/2402.16153.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16153.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16153)), ([:house:](https://huggingface.co/papers/2402.16153)), ([HTML](https://browse.arxiv.org/html/2402.16153v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16153v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16153)), ([SS](https://api.semanticscholar.org/arXiv:2402.16153)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chatmusician-understanding-and-generating)), ([:octocat:](https://github.com/hf-lin/ChatMusician)![GitHub Repo stars](https://img.shields.io/github/stars/hf-lin/ChatMusician?style=social))
  * 2.25 - FuseChat: Knowledge Fusion of Chat Models ([:x:](https://arxiv.org/abs/2402.16107)), ([:book:](https://browse.arxiv.org/pdf/2402.16107.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.16107.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.16107)), ([:house:](https://huggingface.co/papers/2402.16107)), ([HTML](https://browse.arxiv.org/html/2402.16107v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.16107v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.16107)), ([SS](https://api.semanticscholar.org/arXiv:2402.16107)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/fusechat-knowledge-fusion-of-chat-models)), ([:octocat:](https://github.com/fanqiwan/fusellm)![GitHub Repo stars](https://img.shields.io/github/stars/fanqiwan/fusellm?style=social))
  * 02/24 - **Divide-or-Conquer? Which Part Should You Distill Your LLM?** <br>([:x:](https://arxiv.org/abs/2402.15000)), ([:book:](https://browse.arxiv.org/pdf/2402.15000.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15000.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15000)), ([:house:](https://huggingface.co/papers/2402.15000)), ([HTML](https://browse.arxiv.org/html/2402.15000v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2402.15000)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15000v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15000)), ([SS](https://api.semanticscholar.org/arXiv:2402.15000)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/divide-or-conquer-which-part-should-you))
  * 02/24 - **Perplexity.ai Revamps Google SEO Model For LLM Era**   ([News](https://spectrum.ieee.org/perplexity-ai))
  * 02/24 - **Data Interpreter: An LLM Agent For Data Science** <br>([:x:](https://arxiv.org/abs/2402.18679)), ([:book:](https://browse.arxiv.org/pdf/2402.18679.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.18679.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.18679)), ([:house:](https://huggingface.co/papers/2402.18679)), ([HTML](https://browse.arxiv.org/html/2402.18679v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2402.18679)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.18679v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.18679)), ([SS](https://api.semanticscholar.org/arXiv:2402.18679)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/data-interpreter-an-llm-agent-for-data)), ([:octocat:](https://github.com/geekan/metagpt)![GitHub Repo stars](https://img.shields.io/github/stars/geekan/metagpt?style=social))
  * 2.24 - Empowering Large Language Model Agents through Action Learning ([:x:](https://arxiv.org/abs/2402.15809)), ([:book:](https://browse.arxiv.org/pdf/2402.15809.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15809.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15809)), ([:house:](https://huggingface.co/papers/2402.15809)), ([HTML](https://browse.arxiv.org/html/2402.15809v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15809v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15809)), ([SS](https://api.semanticscholar.org/arXiv:2402.15809)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/empowering-large-language-model-agents))
  * 2.23 - MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs ([:x:](https://arxiv.org/abs/2402.15627)), ([:book:](https://browse.arxiv.org/pdf/2402.15627.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15627.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15627)), ([:house:](https://huggingface.co/papers/2402.15627)), ([HTML](https://browse.arxiv.org/html/2402.15627v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15627v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15627)), ([SS](https://api.semanticscholar.org/arXiv:2402.15627)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/megascale-scaling-large-language-model))
  * 2.23 - Seamless Human Motion Composition with Blended Positional Encodings ([:x:](https://arxiv.org/abs/2402.15509)), ([:book:](https://browse.arxiv.org/pdf/2402.15509.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15509.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15509)), ([:house:](https://huggingface.co/papers/2402.15509)), ([HTML](https://browse.arxiv.org/html/2402.15509v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15509v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15509)), ([SS](https://api.semanticscholar.org/arXiv:2402.15509)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/seamless-human-motion-composition-with)), ([:octocat:](https://github.com/BarqueroGerman/FlowMDM)![GitHub Repo stars](https://img.shields.io/github/stars/BarqueroGerman/FlowMDM?style=social))
  * 2.23 - AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning ([:x:](https://arxiv.org/abs/2402.15506)), ([:book:](https://browse.arxiv.org/pdf/2402.15506.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15506.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15506)), ([:house:](https://huggingface.co/papers/2402.15506)), ([HTML](https://browse.arxiv.org/html/2402.15506v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15506v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15506)), ([SS](https://api.semanticscholar.org/arXiv:2402.15506)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/agentohana-design-unified-data-and-training))
  * 2.23 - Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition ([:x:](https://arxiv.org/abs/2402.15504)), ([:book:](https://browse.arxiv.org/pdf/2402.15504.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15504.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15504)), ([:house:](https://huggingface.co/papers/2402.15504)), ([HTML](https://browse.arxiv.org/html/2402.15504v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15504v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15504)), ([SS](https://api.semanticscholar.org/arXiv:2402.15504)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gen4gen-generative-data-pipeline-for)), ([:octocat:](https://github.com/louisYen/Gen4Gen)![GitHub Repo stars](https://img.shields.io/github/stars/louisYen/Gen4Gen?style=social))
  * 2.23 - API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs ([:x:](https://arxiv.org/abs/2402.15491)), ([:book:](https://browse.arxiv.org/pdf/2402.15491.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15491.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15491)), ([:house:](https://huggingface.co/papers/2402.15491)), ([HTML](https://browse.arxiv.org/html/2402.15491v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15491v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15491)), ([SS](https://api.semanticscholar.org/arXiv:2402.15491)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/api-blend-a-comprehensive-corpora-for))
  * 2.23 - Genie: Generative Interactive Environments ([:x:](https://arxiv.org/abs/2402.15391)), ([:book:](https://browse.arxiv.org/pdf/2402.15391.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15391.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15391)), ([:house:](https://huggingface.co/papers/2402.15391)), ([HTML](https://browse.arxiv.org/html/2402.15391v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15391v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15391)), ([SS](https://api.semanticscholar.org/arXiv:2402.15391)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/genie-generative-interactive-environments))
  * 2.23 - GPTVQ: The Blessing of Dimensionality for LLM Quantization ([:x:](https://arxiv.org/abs/2402.15319)), ([:book:](https://browse.arxiv.org/pdf/2402.15319.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15319.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15319)), ([:house:](https://huggingface.co/papers/2402.15319)), ([HTML](https://browse.arxiv.org/html/2402.15319v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15319v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15319)), ([SS](https://api.semanticscholar.org/arXiv:2402.15319)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/gptvq-the-blessing-of-dimensionality-for-llm))
  * 2.23 - ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition ([:x:](https://arxiv.org/abs/2402.15220)), ([:book:](https://browse.arxiv.org/pdf/2402.15220.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15220.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15220)), ([:house:](https://huggingface.co/papers/2402.15220)), ([HTML](https://browse.arxiv.org/html/2402.15220v1)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.15220v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15220)), ([SS](https://api.semanticscholar.org/arXiv:2402.15220)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/chunkattention-efficient-self-attention-with))
  * 2.22 - CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models ([:x:](https://arxiv.org/abs/2402.15021)), ([:book:](https://browse.arxiv.org/pdf/2402.15021.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15021.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15021)), ([:house:](https://huggingface.co/papers/2402.15021)), ([HTML](https://browse.arxiv.org/html/2402.15021v1)), ([AS](https://www.summarizepaper.com/en/arxiv-id/2402.15021v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15021)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/clove-encoding-compositional-language-in)), ([:octocat:](https://github.com/netflix/clove)![GitHub Repo stars](https://img.shields.io/github/stars/netflix/clove?style=social))
  * 02/22 - **Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models** ([:x:](https://arxiv.org/abs/2402.14207)), ([:book:](https://browse.arxiv.org/pdf/2402.14207.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14207.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14207)), ([:house:](https://huggingface.co/papers/2402.14207)), ([HTML](https://browse.arxiv.org/html/2402.14207v1)), ([SL](https://arxiv-sanity-lite.com/?rank=pid&pid=2402.14207)), ([SP](https://www.summarizepaper.com/en/arxiv-id/2402.14207v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.14207)), ([SS](https://api.semanticscholar.org/arXiv:2402.14207))
  * 2.22 - Divide-or-Conquer? Which Part Should You Distill Your LLM? ([:x:](https://arxiv.org/abs/2402.15000)), ([:book:](https://browse.arxiv.org/pdf/2402.15000.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.15000.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.15000)), ([:house:](https://huggingface.co/papers/2402.15000)), ([HTML](https://browse.arxiv.org/html/2402.15000v1)), ([AS](https://www.summarizepaper.com/en/arxiv-id/2402.15000v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15000)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/divide-or-conquer-which-part-should-you))
  * 2.22 - MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases ([:x:](https://arxiv.org/abs/2402.14905)), ([:book:](https://browse.arxiv.org/pdf/2402.14905.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14905.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14905)), ([:house:](https://huggingface.co/papers/2402.14905)), ([HTML](https://browse.arxiv.org/html/2402.14905v1)), ([AS](https://www.summarizepaper.com/en/arxiv-id/2402.14905v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.14905)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/mobilellm-optimizing-sub-billion-parameter))
  * 2.22 - Watermarking Makes Language Models Radioactive ([:x:](https://arxiv.org/abs/2402.14904)), ([:book:](https://browse.arxiv.org/pdf/2402.14904.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14904.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14904)), ([:house:](https://huggingface.co/papers/2402.14904)), ([HTML](https://browse.arxiv.org/html/2402.14904v1)), ([AS](https://www.summarizepaper.com/en/arxiv-id/2402.14904v1/)), ([GS](https://scholar.google.com/scholar_lookup?arxiv_id=2402.14904)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/watermarking-makes-language-models))
  * 2.22 - AutoPrompt - prompt optimization framework  ([:octocat:](https://github.com/Eladlev/AutoPrompt)![GitHub Repo stars](https://img.shields.io/github/stars/Eladlev/AutoPrompt?style=social))
  * 2.22 - Announcing Stable Diffusion 3 ([tweet](https://twitter.com/StabilityAI/status/1760656767237656820)), ([blog](https://stability.ai/news/stable-diffusion-3?utm_source=twitter&utm_medium=website&utm_campaign=blog))
  * 2.22 - DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models ([:x:](https://arxiv.org/abs/2402.14767)), ([:book:](https://browse.arxiv.org/pdf/2402.14767.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14767.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14767)), ([:house:](https://huggingface.co/papers/2402.14767)), ([HTML](https://browse.arxiv.org/html/2402.14767v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/dualfocus-integrating-macro-and-micro)) , ([:octocat:](https://github.com/InternLM/InternLM-XComposer)![GitHub Repo stars](https://img.shields.io/github/stars/InternLM/InternLM-XComposer?style=social))
  * 2.22 - RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation ([:x:](https://arxiv.org/abs/2402.14623)), ([:book:](https://browse.arxiv.org/pdf/2402.14623.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14623.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14623)), ([:house:](https://huggingface.co/papers/2402.14623)), ([HTML](https://browse.arxiv.org/html/2402.14623v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/roboscript-code-generation-for-free-form)) 
  * 2.22 - LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey ([:x:](https://arxiv.org/abs/2402.14558)), ([:book:](https://browse.arxiv.org/pdf/2402.14558.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14558.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14558)), ([:house:](https://huggingface.co/papers/2402.14558)), ([HTML](https://browse.arxiv.org/html/2402.14558v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/llms-with-industrial-lens-deciphering-the)) 
  * 2.22 - Vision-Language Navigation with Embodied Intelligence: A Survey ([:x:](https://arxiv.org/abs/2402.14304)), ([:book:](https://browse.arxiv.org/pdf/2402.14304.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14304.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14304)), ([:house:](https://huggingface.co/papers/2402.14304)), ([HTML](https://browse.arxiv.org/html/2402.14304v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/vision-language-navigation-with-embodied)) 
  * 2.22 - Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models ([:x:](https://arxiv.org/abs/2402.14245)), ([:book:](https://browse.arxiv.org/pdf/2402.14245.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14245.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14245)), ([:house:](https://huggingface.co/papers/2402.14245)), ([HTML](https://browse.arxiv.org/html/2402.14245v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/enhancing-robotic-manipulation-with-ai)) 
  * 2.22 - Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization ([:x:](https://arxiv.org/abs/2402.14182)), ([:book:](https://browse.arxiv.org/pdf/2402.14182.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14182.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14182)), ([:house:](https://huggingface.co/papers/2402.14182)), ([HTML](https://browse.arxiv.org/html/2402.14182v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/do-machines-and-humans-focus-on-similar-code)) 
  * 2.22 - PALO: A Polyglot Large Multimodal Model for 5B People ([:x:](https://arxiv.org/abs/2402.14818)), ([:book:](https://browse.arxiv.org/pdf/2402.14818.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.14818.pdf)),  ([:orange_book:](https://www.arxiv-vanity.com/papers/2402.14818)), ([:house:](https://huggingface.co/papers/2402.14818)), ([HTML](https://browse.arxiv.org/html/2402.14818v1)), ([:eight_spoked_asterisk:](https://paperswithcode.com/paper/palo-a-polyglot-large-multimodal-model-for-5b)) , ([:octocat:](https://github.com/mbzuai-oryx/palo)![GitHub Repo stars](https://img.shields.io/github/stars/mbzuai-oryx/palo?style=social))
  * 2.22 - GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion ([:x:](https://arxiv.org/abs/2402.14810)), ([:book:](https://browse.arxiv.org/pdf/2402.14810.pdf)), ([:paperclip:](https://arxiv.org/pdf/2402.148